{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Claims - Keras LSTM + Embedding + Dropout\n",
    "\n",
    "In this post we will see if we can build some classifiers to predict a first level patent classification from the claim text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using USPTO data, where I believe the claims are classified according to the IPC. To keep things simple we will use the first letter of the IPC (top level category).  \n",
    "\n",
    "The list of top level categories can be found here: https://rs.espacenet.com/help?locale=en_EP&method=handleHelpTopic&topic=ipc:\n",
    "* A Human Necessities\n",
    "* B Performing Operations; Transporting\n",
    "* C Chemistry; Metallurgy\n",
    "* D Textiles; Paper\n",
    "* E Fixed Constructions\n",
    "* F Mechanical Engineering; Lighting; Heating; Weapons; Blasting Engines or Pumps\n",
    "* G Physics\n",
    "* H Electricity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Getting Our Data\n",
    "\n",
    "See the previous notebook for our data preparation.  \n",
    "\n",
    "Here we will load the data as tokenised and converted to integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "filename = \"X_Y_data.pkl\"\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        print(\"Loading data\")\n",
    "        X_data, Y_data, count, word_dictionary, reverse_word_dictionary, class_dictionary, reverse_class_dictionary = pickle.load(f)\n",
    "else:\n",
    "    print(\"Run the previous notebook for data preparation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 7, 7, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 9,\n",
       " 3,\n",
       " 511,\n",
       " 13,\n",
       " 3700,\n",
       " 6638,\n",
       " 30,\n",
       " 17590,\n",
       " 23,\n",
       " 5524,\n",
       " 20,\n",
       " 15,\n",
       " 18,\n",
       " 11,\n",
       " 1333,\n",
       " 370,\n",
       " 43,\n",
       " 15,\n",
       " 3,\n",
       " 370,\n",
       " 78,\n",
       " 5,\n",
       " 7,\n",
       " 36,\n",
       " 11,\n",
       " 3700,\n",
       " 91,\n",
       " 138,\n",
       " 32,\n",
       " 11,\n",
       " 91,\n",
       " 13,\n",
       " 3,\n",
       " 4916,\n",
       " 428,\n",
       " 4,\n",
       " 11,\n",
       " 1333,\n",
       " 12,\n",
       " 11,\n",
       " 7720,\n",
       " 19,\n",
       " 66,\n",
       " 6,\n",
       " 19,\n",
       " 1532,\n",
       " 7,\n",
       " 272,\n",
       " 11,\n",
       " 3700,\n",
       " 1333,\n",
       " 41,\n",
       " 5,\n",
       " 410,\n",
       " 12,\n",
       " 3,\n",
       " 4916,\n",
       " 683,\n",
       " 5,\n",
       " 12,\n",
       " 141,\n",
       " 3,\n",
       " 1171,\n",
       " 31,\n",
       " 11,\n",
       " 1333,\n",
       " 41,\n",
       " 13,\n",
       " 38,\n",
       " 3700,\n",
       " 800,\n",
       " 5,\n",
       " 15,\n",
       " 11,\n",
       " 3700,\n",
       " 4301,\n",
       " 6,\n",
       " 3,\n",
       " 478,\n",
       " 5058,\n",
       " 4,\n",
       " 10,\n",
       " 7720,\n",
       " 5,\n",
       " 94,\n",
       " 406,\n",
       " 1333,\n",
       " 399,\n",
       " 476,\n",
       " 17,\n",
       " 219,\n",
       " 4833,\n",
       " 84,\n",
       " 3,\n",
       " 540,\n",
       " 3700,\n",
       " 800,\n",
       " 8,\n",
       " 10,\n",
       " 1333,\n",
       " 370,\n",
       " 43,\n",
       " 15,\n",
       " 3,\n",
       " 41,\n",
       " 78,\n",
       " 5,\n",
       " 178,\n",
       " 36,\n",
       " 3,\n",
       " 7674,\n",
       " 41,\n",
       " 19,\n",
       " 7,\n",
       " 53,\n",
       " 811,\n",
       " 7,\n",
       " 10,\n",
       " 511,\n",
       " 5,\n",
       " 36,\n",
       " 3928,\n",
       " 4833,\n",
       " 4,\n",
       " 4121,\n",
       " 12984,\n",
       " 12,\n",
       " 22497,\n",
       " 3018,\n",
       " 7,\n",
       " 10,\n",
       " 1333,\n",
       " 41,\n",
       " 8,\n",
       " 11,\n",
       " 705,\n",
       " 43,\n",
       " 48,\n",
       " 7,\n",
       " 744,\n",
       " 406,\n",
       " 2608,\n",
       " 1333,\n",
       " 41,\n",
       " 810,\n",
       " 4,\n",
       " 3,\n",
       " 4916,\n",
       " 3700,\n",
       " 1333,\n",
       " 41,\n",
       " 5,\n",
       " 36,\n",
       " 54,\n",
       " 38,\n",
       " 7950,\n",
       " 28,\n",
       " 92,\n",
       " 2608,\n",
       " 4121,\n",
       " 12984,\n",
       " 5,\n",
       " 24,\n",
       " 22,\n",
       " 350,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 379,\n",
       " 3,\n",
       " 767,\n",
       " 2059,\n",
       " 4,\n",
       " 11,\n",
       " 12606,\n",
       " 4916,\n",
       " 3700,\n",
       " 1333,\n",
       " 41,\n",
       " 86,\n",
       " 5,\n",
       " 10,\n",
       " 705,\n",
       " 43,\n",
       " 48,\n",
       " 7,\n",
       " 2188,\n",
       " 10,\n",
       " 767,\n",
       " 2059,\n",
       " 7,\n",
       " 3,\n",
       " 814,\n",
       " 102,\n",
       " 5,\n",
       " 6,\n",
       " 253,\n",
       " 10,\n",
       " 767,\n",
       " 2059,\n",
       " 4,\n",
       " 10,\n",
       " 12606,\n",
       " 4916,\n",
       " 3700,\n",
       " 1333,\n",
       " 41,\n",
       " 19,\n",
       " 304,\n",
       " 88,\n",
       " 10,\n",
       " 814,\n",
       " 102,\n",
       " 5,\n",
       " 272,\n",
       " 11,\n",
       " 5524,\n",
       " 22595,\n",
       " 41,\n",
       " 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10262 10262\n"
     ]
    }
   ],
   "source": [
    "# Check vectors are the same length\n",
    "print(len(X_data), len(Y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Applying Sequence Classification with Keras\n",
    "\n",
    "Working from this post - https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/ - we can then apply an LSTM followed by a single dense layer.\n",
    "\n",
    "Documentation for padding - https://keras.io/preprocessing/sequence/#pad_sequences . We probably need to reserve 0 as a reserved character.\n",
    "\n",
    "Also here is the keras guide to sequential classification: https://keras.io/getting-started/sequential-model-guide/.\n",
    "\n",
    "The post here explains how to split data into training / test using Keras: https://gogul09.github.io/software/first-neural-network-keras.\n",
    "\n",
    "See here for BiDirectional LSTM: https://github.com/fchollet/keras/blob/master/examples/imdb_bidirectional_lstm.py.\n",
    "\n",
    "This time we'll try adding some dropout between the layers to prevent overfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# First we need to split out data into training and test data - go for 80:20\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# seed for reproducing same results\n",
    "seed = 9\n",
    "np.random.seed(seed)\n",
    "\n",
    "# split the data into training (80%) and testing (20%)\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(X_data, Y_data, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training data has length: 8209 and our test data has length: 2053\n"
     ]
    }
   ],
   "source": [
    "print(\"Our training data has length: {0} and our test data has length: {1}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we need to segment and pad our claim text sequences - we have already restricted our claims to length 250\n",
    "# We might want to experiment with changing this\n",
    "max_word_length = 250\n",
    "# Padding is performing by adding 0, which we have reserved as a PAD token above\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_word_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,    16,     9,    11,   564,\n",
       "           5,    15,    18,     3,  1076,    36,    19,    80,    12,\n",
       "           3,   319,   382,     5,    12,    36,    17,    26,    22,\n",
       "          39,   808,     7,     3,   358,   164,   171,    19,  1625,\n",
       "          12,     3,   472,    90,     6,  2148, 22072,    12,   219,\n",
       "        3702,     8,     3,   963,    55,     3,    35,     4,   244,\n",
       "        1727,  1083,  1042,     7,     2,  4480,    39,     4,     2,\n",
       "        1076,     8,     3,   533,    36,   165,    11,   164,    40,\n",
       "        1107,     7,    53,   775,   128,     2,   358,   164,    58,\n",
       "           3,   416,     5,     6,     3,   532,    40,  1274,    39,\n",
       "         808,     7,     2,   164,    40,    19,  1642,     7,     2,\n",
       "        1076,     5,     6,    36,    19,   520,     4,     3,  1548,\n",
       "         244,    63,    13,   531,     4,   244, 11435,     4,     3,\n",
       "         244,    67,     8,     6,     3,   437,    36,    19,   233,\n",
       "          27,   162,     2,   461,    39,    58,    31,     4,     2,\n",
       "        1076,     5,   374,     7,     2,   963,     5,     6, 20422,\n",
       "           2,   963,     6,     2,   532,    40,     9], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 classes\n"
     ]
    }
   ],
   "source": [
    "no_classes = len(class_dictionary)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "print(\"There are {0} classes\".format(no_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8209,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert labels to categorical one-hot encoding\n",
    "Y_train = to_categorical(Y_train, num_classes=no_classes)\n",
    "Y_test = to_categorical(Y_test, num_classes=no_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8209, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 808       \n",
      "=================================================================\n",
      "Total params: 3,292,408\n",
      "Trainable params: 3,292,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8209 samples, validate on 2053 samples\n",
      "Epoch 1/30\n",
      "8209/8209 [==============================] - 396s - loss: 1.6709 - acc: 0.3309 - val_loss: 1.4526 - val_acc: 0.3790\n",
      "Epoch 2/30\n",
      "8209/8209 [==============================] - 394s - loss: 1.3410 - acc: 0.4441 - val_loss: 1.3218 - val_acc: 0.4598\n",
      "Epoch 3/30\n",
      "8209/8209 [==============================] - 396s - loss: 1.0254 - acc: 0.6213 - val_loss: 1.2874 - val_acc: 0.5197\n",
      "Epoch 4/30\n",
      "8209/8209 [==============================] - 395s - loss: 0.7523 - acc: 0.7398 - val_loss: 1.4131 - val_acc: 0.5178\n",
      "Epoch 5/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.5372 - acc: 0.8182 - val_loss: 1.5832 - val_acc: 0.5090\n",
      "Epoch 6/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.3936 - acc: 0.8694 - val_loss: 1.7377 - val_acc: 0.4929\n",
      "Epoch 7/30\n",
      "8209/8209 [==============================] - 395s - loss: 0.2914 - acc: 0.9027 - val_loss: 1.9405 - val_acc: 0.5095\n",
      "Epoch 8/30\n",
      "8209/8209 [==============================] - 395s - loss: 0.2290 - acc: 0.9278 - val_loss: 2.1096 - val_acc: 0.4895\n",
      "Epoch 9/30\n",
      "8209/8209 [==============================] - 397s - loss: 0.1761 - acc: 0.9440 - val_loss: 2.3068 - val_acc: 0.5002\n",
      "Epoch 10/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.1313 - acc: 0.9572 - val_loss: 2.4725 - val_acc: 0.4851\n",
      "Epoch 11/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.1198 - acc: 0.9637 - val_loss: 2.6610 - val_acc: 0.4847\n",
      "Epoch 12/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.0923 - acc: 0.9709 - val_loss: 2.7212 - val_acc: 0.4803\n",
      "Epoch 13/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.0774 - acc: 0.9758 - val_loss: 2.7970 - val_acc: 0.4890\n",
      "Epoch 14/30\n",
      "8209/8209 [==============================] - 397s - loss: 0.0627 - acc: 0.9798 - val_loss: 2.9960 - val_acc: 0.4827\n",
      "Epoch 15/30\n",
      "8209/8209 [==============================] - 395s - loss: 0.0562 - acc: 0.9828 - val_loss: 2.9926 - val_acc: 0.4851\n",
      "Epoch 16/30\n",
      "8209/8209 [==============================] - 395s - loss: 0.0498 - acc: 0.9851 - val_loss: 3.1095 - val_acc: 0.4900\n",
      "Epoch 17/30\n",
      "8209/8209 [==============================] - 395s - loss: 0.0439 - acc: 0.9879 - val_loss: 3.2778 - val_acc: 0.4890\n",
      "Epoch 18/30\n",
      "8209/8209 [==============================] - 395s - loss: 0.0695 - acc: 0.9789 - val_loss: 3.1600 - val_acc: 0.4871\n",
      "Epoch 19/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.0455 - acc: 0.9861 - val_loss: 3.2324 - val_acc: 0.4817\n",
      "Epoch 20/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.0356 - acc: 0.9887 - val_loss: 3.4707 - val_acc: 0.4730\n",
      "Epoch 21/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.0563 - acc: 0.9822 - val_loss: 3.1421 - val_acc: 0.4657\n",
      "Epoch 22/30\n",
      "8209/8209 [==============================] - 395s - loss: 0.0358 - acc: 0.9892 - val_loss: 3.4339 - val_acc: 0.4837\n",
      "Epoch 23/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.0307 - acc: 0.9904 - val_loss: 3.4271 - val_acc: 0.4749\n",
      "Epoch 24/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.0250 - acc: 0.9932 - val_loss: 3.5631 - val_acc: 0.4803\n",
      "Epoch 25/30\n",
      "8209/8209 [==============================] - 395s - loss: 0.0230 - acc: 0.9940 - val_loss: 3.7125 - val_acc: 0.4754\n",
      "Epoch 26/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.0160 - acc: 0.9951 - val_loss: 3.7038 - val_acc: 0.4637\n",
      "Epoch 27/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.0201 - acc: 0.9945 - val_loss: 3.8368 - val_acc: 0.4754\n",
      "Epoch 28/30\n",
      "8209/8209 [==============================] - 396s - loss: 0.0142 - acc: 0.9954 - val_loss: 3.8746 - val_acc: 0.4774\n",
      "Epoch 29/30\n",
      "8209/8209 [==============================] - 397s - loss: 0.0201 - acc: 0.9931 - val_loss: 3.8466 - val_acc: 0.4754\n",
      "Epoch 30/30\n",
      "8209/8209 [==============================] - 395s - loss: 0.0120 - acc: 0.9966 - val_loss: 3.8682 - val_acc: 0.4749\n"
     ]
    }
   ],
   "source": [
    "# Now building our model \n",
    "embedding_vecor_length = 128\n",
    "vocabulary_size=25000\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_vecor_length, input_length=max_word_length))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, batch_size=64)\n",
    "model.save('claim_class_lstm_drop.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout applied directly to the LSTM using this format: ```model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))``` \n",
    "\n",
    "With dropout at 0.2 before and after LSTM:\n",
    "```\n",
    "Epoch 6/6\n",
    "8209/8209 [==============================] - 506s - loss: 0.3639 - acc: 0.8811 - val_loss: 1.6160 - val_acc: 0.5158\n",
    "```\n",
    "With dropout via LSTM input and gate:\n",
    "```\n",
    "Epoch 30/30\n",
    "8209/8209 [==============================] - 395s - loss: 0.0120 - acc: 0.9966 - val_loss: 3.8682 - val_acc: 0.4749\n",
    "```\n",
    "About the same. We need to look at the confusion matrix to have a look at where misclassification and overfitting is occurring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2053/2053 [==============================] - 33s    \n",
      "Accuracy: 51.10%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with this approach is the embedding layer is quickly overfitting to the data as it has the most parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
