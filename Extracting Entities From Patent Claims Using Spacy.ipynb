{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Entity Extraction\n",
    "\n",
    "In this notebook we will be looking at using spaCy (https://spacy.io/) to populate object models from patent claim data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's import spaCy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference here are some common object POS patterns as extracted from a patent specification using the reference numeral as an end point.\n",
    "```\n",
    "[('<DET><NOUN><NUM>', 63),\n",
    " ('<DET><NOUN><NOUN><NUM>', 50),\n",
    " ('<DET><VERB><NOUN><NUM>', 48),\n",
    " ('<DET><ADJ><NOUN><NUM>', 39),\n",
    " ('<DET><NOUN><NOUN><NOUN><NUM>', 35),\n",
    " ('<DET><ADJ><ADJ><NOUN><NOUN><NUM>', 14),\n",
    " ('<DET><NOUN><PUNCT><VERB><NOUN><NUM>', 8),\n",
    " ('<DET><ADJ><NOUN><NOUN><NUM>', 6),\n",
    " ('<DET><ADJ><CCONJ><ADJ><ADJ><NOUN><NOUN><NUM>', 4),\n",
    " ('<DET><NOUN><NOUN><NOUN><NOUN><NUM>', 3),\n",
    " ('<DET><NOUN><ADP><NOUN><NOUN><NUM>', 3),\n",
    " ('<DET><ADJ><CCONJ><ADJ><NOUN><NUM>', 3),\n",
    " ('<DET><NOUN><ADP><NOUN><NUM>', 3),\n",
    " ('<DET><NOUN><VERB><NOUN><NUM>', 2),\n",
    " ('<DET><NOUN><ADV><CCONJ><ADJ><NOUN><NUM>', 1),\n",
    " ('<DET><ADJ><VERB><NUM><PUNCT><NUM><ADP><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><ADP><ADV><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><ADV><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><ADV><VERB><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><VERB><NOUN><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><PUNCT><NOUN><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><VERB><ADP><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><ADP><ADJ><ADJ><ADJ><NOUN><NOUN><NUM>', 1),\n",
    " ('<DET><ADJ><NOUN><PUNCT><NOUN><PUNCT><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><PUNCT><NOUN><PUNCT><NOUN><PUNCT><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><VERB><NOUN><ADV><CCONJ><ADJ><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><ADP><ADJ><NOUN><NUM>', 1)]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some initial functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import DET, NOUN, CCONJ\n",
    "\n",
    "def simple_spacy_entity_finder(doc):\n",
    "    \"\"\" Find entities with reference numerals using POS data.\"\"\"\n",
    "    entity_list = list()\n",
    "    record = False\n",
    "    # Generate a list of tokens so we can iterate backwards through it\n",
    "    enum_doc_list = list(enumerate(doc))\n",
    "    last_end = 0\n",
    "    # Add indices\n",
    "    for i, word in enum_doc_list:\n",
    "        if word.pos == DET and not record:\n",
    "            # Start recording and record start index\n",
    "            record = True\n",
    "            start_index = i\n",
    "        else:        \n",
    "            if (word.pos == DET or word.pos == CCONJ or word.lemma_ == \";\") and record:\n",
    "                # Step back until last noun is found\n",
    "                for j, bword in reversed(enum_doc_list[last_end:i]):\n",
    "                    if bword.pos == NOUN:\n",
    "                        # Add np_chunk to buffer\n",
    "                        entity_list.append(doc[start_index:j+1])\n",
    "                        last_end = j\n",
    "                        break       \n",
    "                if word.pos == DET:\n",
    "                    # Set new start index\n",
    "                    record = True\n",
    "                    start_index = i\n",
    "                else:\n",
    "                    record = False\n",
    "    \n",
    "    entity_dict = dict()\n",
    "    # Now group by unique\n",
    "    for entity in entity_list:\n",
    "        \n",
    "        np_start = entity.start\n",
    "        # Ignore the determinant \n",
    "        if doc[np_start].pos == DET:\n",
    "            np_start += 1\n",
    "        # Generate a string representation excluding the determinant\n",
    "        np_string = doc[np_start:entity.end].text.lower()\n",
    "                                \n",
    "        if np_string not in entity_dict.keys():\n",
    "            entity_dict[np_string] = list()          \n",
    "        entity_dict[np_string].append(entity)\n",
    "    \n",
    "    return entity_list, entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import DET, NOUN\n",
    "\n",
    "def check_ant(doc, entity_dict):\n",
    "    \"\"\" Check antecedence - attempt to merge entries with incorrect antecedence.\"\"\"\n",
    "    \n",
    "    issue_keys_a = list()\n",
    "    issue_keys_the = list()\n",
    "    \n",
    "    # Look for entries with antecedence issues\n",
    "    for key in entity_dict:\n",
    "        entities = entity_dict[key]\n",
    "        # Check if first entry begins with \"a\" - flag if doesn't\n",
    "        first_entry = entities[0]\n",
    "        if first_entry[0].pos == DET and first_entry[0].lemma_ != \"a\" and first_entry[0].lemma_ != \"an\":\n",
    "            issue_keys_a.append(key)\n",
    "        \n",
    "        # If more than one entry check subsequent entries start with \"the\" - flag if don't\n",
    "        if len(entities) > 1:\n",
    "            for entity in entities[1:]:\n",
    "                if entity[0].pos == DET and entity[0].lemma_ != \"the\":\n",
    "                    issue_keys_the.append(key)\n",
    "    \n",
    "    return issue_keys_a, issue_keys_the\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def look_for_existing(doc, entity_dict):\n",
    "    \"\"\" Look for previously existing versions of problem keys.\"\"\"\n",
    "    # If more than one entry check subsequent entries start with \"the\" - flag if don't\n",
    "    issue_keys_a = list()\n",
    "    for key in entity_dict:\n",
    "        entities = entity_dict[key]\n",
    "        # Check if first entry begins with \"a\" - flag if doesn't\n",
    "        first_entry = entities[0]\n",
    "        if first_entry[0].pos == DET and first_entry[0].lemma_ != \"a\" and first_entry[0].lemma_ != \"an\":\n",
    "            issue_keys_a.append(key)\n",
    "    \n",
    "    for pkey in issue_keys_a:\n",
    "        problem_entities = entity_dict[pkey]\n",
    "        # i.e. list of two longer oblong spans\n",
    "        # Can we just work with the key initially?\n",
    "        for key in entity_dict.keys():\n",
    "            if len(pkey) > len(key) and key in pkey:\n",
    "                print(key, pkey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We now need to collate and create a set of entities\n",
    "def get_entity_set(entity_list):\n",
    "    \"\"\" Get a set of unique entity n-grams from a list of entities.\"\"\"\n",
    "    ngram_list = list()\n",
    "    for entity in entity_list:\n",
    "        ngram_list.append(\" \".join([word for word, pos in entity if (pos != 'DET')]))\n",
    "    return set(ngram_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing on Other Patent Data\n",
    "\n",
    "Lets test on different patent claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554570 records located.\n",
      "10 records sampled.\n"
     ]
    }
   ],
   "source": [
    "# Generate or create some test claim sets for analysis\n",
    "\n",
    "# (Looks like we can't pickle and load spaCy objects)\n",
    "from patentdata.corpus import USPublications\n",
    "\n",
    "pubs = USPublications(\"/media/SAMSUNG1/Patent_Downloads\")\n",
    "filegenerator = pubs.patentdoc_generator(['G', '06'], sample_size=10)\n",
    "docs = list(filegenerator)\n",
    "ent_from_claims = list()\n",
    "nlp_docs = list()\n",
    "for doc in docs:\n",
    "    nlp_doc = nlp(doc.claimset.get_claim(1).text)\n",
    "    entity_list, entity_dict = simple_spacy_entity_finder(nlp_doc)\n",
    "    nlp_docs.append(nlp_doc)\n",
    "    ent_from_claims.append(entity_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'energy collectors generate collector-local information': [the energy collectors generate collector-local information],\n",
       " 'energy park': [An energy park],\n",
       " 'information': [the information],\n",
       " 'plurality of energy collectors having known spatial location': [a plurality of energy collectors having known spatial location]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_from_claims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These terms are not explicitly introduced using 'a/an X':\n",
      " ['information', 'energy collectors generate collector-local information'] \n",
      "\n",
      "These terms do not use 'the' yet occur previously:\n",
      " [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ika, ikt = check_ant(nlp_docs[0], ent_from_claims[0])\n",
    "print(\"These terms are not explicitly introduced using 'a/an X':\\n\", ika, \"\\n\")\n",
    "\n",
    "print(\"These terms do not use 'the' yet occur previously:\\n\", ikt, \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1. An energy park including a plurality of energy collectors having known spatial location where the energy collectors generate collector-local information and the information from the plurality of collectors is spatially correlated."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. An energy park including a plurality of energy collectors having known spatial location where the energy collectors generate collector-local information and the information from the plurality of collectors is spatially correlated.\n",
      " \n",
      "\n",
      "{'energy park': [An energy park], 'plurality of energy collectors having known spatial location': [a plurality of energy collectors having known spatial location], 'information': [the information], 'energy collectors generate collector-local information': [the energy collectors generate collector-local information]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A method used for supporting designing of a printed circuit board including a plurality of conductive layers including conductive areas to which a constant potential is applied, comprising:\n",
      "specifying conductive areas including wiring from the conductive areas for each of the plurality of conductive layers;\n",
      "extracting areas which overlap each other from the specified conductive areas;\n",
      "specifying an interlayer connection member that electrically connects at least two of the plurality of conductive layers in the extracted area; and\n",
      "specifying an area in the extracted areas and within a predetermined distance from the specified interlayer connection member.\n",
      "\n",
      " \n",
      "\n",
      "{'printed circuit board': [a printed circuit board], '': [, ], 'method used for supporting designing': [A method used for supporting designing], 'conductive areas': [the conductive areas], 'constant potential is applied, comprising:\\nspecifying conductive areas including wiring': [a constant potential is applied, comprising:\n",
      "specifying conductive areas including wiring], 'area': [an area], 'plurality of conductive layers': [the plurality of conductive layers, the plurality of conductive layers], 'predetermined distance': [a predetermined distance], 'extracted areas': [the extracted areas], 'plurality of conductive layers including conductive areas': [a plurality of conductive layers including conductive areas], 'interlayer connection member': [an interlayer connection member], 'extracted area': [the extracted area], 'specified conductive areas': [the specified conductive areas]} \n",
      "------\n",
      "\n",
      "\n",
      "1. In recording file system information within a write-once optical disc, a file system information recording method of the write-once optical disc, comprising the step of recording file information of a former session and updated file information of a current session together within a metadata file of the current session. \n",
      " \n",
      "\n",
      "{'write-once optical disc': [a write-once optical disc, the write-once optical disc], 'step of recording file information': [the step of recording file information], 'former session': [a former session], 'metadata file': [a metadata file], 'current session': [a current session], 'file system information recording method': [a file system information recording method]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A system for reselling electronic devices in compliance with second hand dealer laws, the system comprising:\n",
      "an apparatus comprising a fingerprint reader, an identification reader, an exterior imaging component, a processor, an inspection area, an imaging component within the inspection area, and an electrical connector within the inspection area, the electrical connector capable of connection to an electronic device positioned within the inspection area;\n",
      "a network;\n",
      "a server with a database, the server connected to the apparatus over the network;\n",
      "wherein the processor of the apparatus is configured to validate an identification of a recycler, process a fingerprint of the reseller to the database, photograph an image of the recycler to the database, and acquire an identification of the electronic device, the processor configured to collect as transfer data the image of the reseller, the identification information of the reseller, the fingerprint of the reseller, the identification of the electronic device, the processor configured to transmit the transfer data to the server over the network;\n",
      "wherein the server is configured to analyze the transfer data in view of a second hand dealer law for a jurisdiction of a location of the apparatus, determine a procedure for compliance of the second hand dealer law, and comply with the second hand dealer law.\n",
      "\n",
      " \n",
      "\n",
      "{'identification information': [the identification information], 'location': [a location], 'identification': [an identification, an identification, the identification], 'database': [a database, the database, the database], 'procedure for compliance': [a procedure for compliance], 'processor configured to collect as transfer data': [the processor configured to collect as transfer data], 'jurisdiction': [a jurisdiction], 'electronic device': [an electronic device, the electronic device, the electronic device], 'apparatus': [an apparatus, the apparatus, the apparatus, the apparatus], 'system': [the system], 'image': [an image, the image], 'identification reader': [an identification reader], 'transfer data': [the transfer data], 'inspection area': [an inspection area, the inspection area, the inspection area, the inspection area], 'electrical connector': [an electrical connector], 'network': [a network, the network, the network], 'processor': [a processor, the processor, the processor], 'second hand dealer law': [a second hand dealer law, the second hand dealer law], 'transfer data in view': [the transfer data in view], 'recycler, process': [a recycler, process], 'system for reselling electronic devices in compliance with second hand dealer laws': [A system for reselling electronic devices in compliance with second hand dealer laws], 'reseller': [the reseller, the reseller, the reseller, the reseller], 'exterior imaging component': [an exterior imaging component], 'fingerprint': [a fingerprint, the fingerprint], 'electrical connector capable of connection': [the electrical connector capable of connection], 'server': [a server, the server, the server, the server], 'imaging component': [an imaging component], 'fingerprint reader': [a fingerprint reader], 'recycler': [the recycler]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A device, comprising:\n",
      "at least one encrypted processor that is configured to determine an input map associated with a texture pattern and calculate a reduced resolution pattern based on the input map; and\n",
      "at least one second processor, communicably connected to the at least one encrypted processor, that is configured to:\n",
      "receive the reduced resolution pattern and a plurality of similar patterns, each associated with a stored encrypted map template, and\n",
      "identify a match-set if a match exists between the reduced resolution pattern and the plurality of similar patterns,\n",
      "\n",
      "wherein the at least one encrypted processor is further configured to receive the match-set and identify if a match exists between the input map and any stored encrypted map template associated with the match-set.\n",
      "\n",
      " \n",
      "\n",
      "{'reduced resolution pattern': [a reduced resolution pattern, the reduced resolution pattern, the reduced resolution pattern], '': [, ], 'input map': [an input map, the input map, the input map], 'stored encrypted map template': [a stored encrypted map template, any stored encrypted map template], 'texture pattern': [a texture pattern], 'plurality of similar patterns': [a plurality of similar patterns, the plurality of similar patterns], 'device, comprising:\\nat least one encrypted processor': [A device, comprising:\n",
      "at least one encrypted processor], 'match': [a match, a match, a match], 'at least one encrypted processor': [the at least one encrypted processor, the at least one encrypted processor], 'match-set': [the match-set]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A control device for a hybrid vehicle including an engine, a motor/generator, and at least one driving wheel, the hybrid vehicle operable in an electric drive mode in which the vehicle is powered only by the motor/generator and a hybrid drive mode in which the vehicle is powered by both the engine and the motor/generator, the control device comprising:\n",
      "a controller configured to:\n",
      "set a first threshold level of an accelerator opening;\n",
      "set a second threshold level of the accelerator opening, wherein a hysteresis value is defined between the first threshold level and the second threshold level;\n",
      "change the hysteresis value based on at least one of a vehicle operating state and a driving environment;\n",
      "receive a signal corresponding to the accelerator opening;\n",
      "initiate a changeover from the hybrid drive mode to the electric drive mode if the accelerator opening is less than the first threshold level; and\n",
      "initiate a changeover from the electric drive mode to the hybrid drive mode if the accelerator opening is greater than the second threshold value.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "{'engine': [an engine, the engine], '': [], 'electric drive mode': [an electric drive mode, the electric drive mode, the electric drive mode], 'driving environment': [a driving environment], 'vehicle': [the vehicle, the vehicle], 'hybrid vehicle': [a hybrid vehicle, the hybrid vehicle], 'controller': [a controller], 'second threshold level': [a second threshold level, the second threshold level], 'signal': [a signal], 'changeover': [a changeover, a changeover], 'hysteresis value': [a hysteresis value, the hysteresis value], 'accelerator opening': [an accelerator opening, the accelerator opening, the accelerator opening, the accelerator opening, the accelerator opening], 'control device': [A control device, the control device], 'motor/generator': [a motor/generator, the motor/generator, the motor/generator], 'hybrid drive mode': [a hybrid drive mode, the hybrid drive mode, the hybrid drive mode], 'vehicle operating state': [a vehicle operating state], 'first threshold level': [a first threshold level, the first threshold level, the first threshold level]} \n",
      "------\n",
      "\n",
      "\n",
      "1. An information processing device, wherein a graphics plane configured to store a graphics image is a storage region where two image storage regions, which are an L (Left) region that is an image storage region to store an image for the left eye, and an R (Right) region that is an image storage region to store an image for the right eye, are disposed in an array;\n",
      "and wherein drawing of said image for the left eye used for animation as to said L region, and drawing of said image for the right eye used for animation as to said R region are individually performed.\n",
      "\n",
      " \n",
      "\n",
      "{'graphics image': [a graphics image], 'l (left) region': [an L (Left) region], 'graphics plane': [a graphics plane], 'image': [an image, an image], 'left eye': [the left eye], 'left eye used for animation as to said l region': [the left eye used for animation as to said L region], 'storage region where two image storage regions': [a storage region where two image storage regions], 'array': [an array], 'right eye': [the right eye], 'r (right) region': [an R (Right) region], 'image storage region': [an image storage region, an image storage region], 'information processing device': [An information processing device]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A system for filtering data to be pushed from a server to a communication device in accordance with a set of predefined rules, the system comprising: \n",
      "(a) a personal content filter database for storing the set of predefined rules, the set of predefined rules comprising user defined rules received from an associated user via a user interface on the communication device; and \n",
      "(b) a content filter engine for implementing the set of predefined rules by preventing restricted information from being transmitted to the communication device. \n",
      "\n",
      " \n",
      "\n",
      "{'set of predefined rules by preventing restricted information': [the set of predefined rules by preventing restricted information], 'communication device': [the communication device], 'user interface': [a user interface], 'set of predefined rules comprising user defined rules': [the set of predefined rules comprising user defined rules], 'content filter engine': [a content filter engine], 'system for filtering data': [A system for filtering data], 'system': [the system], 'set of predefined rules': [a set of predefined rules, the set of predefined rules], 'communication device in accordance': [a communication device in accordance], 'server': [a server], 'personal content filter database': [a personal content filter database], 'associated user': [an associated user]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A method for translating instant messages exchanged between two or more devices over a network by one or more users that communicate in different languages, the method comprising: \n",
      "establishing a user profile indicating at least one user language and one or more translation preferences of the one or more users; \n",
      "receiving a message as input composed by at least one of the users according to the user language; \n",
      "translating the message from the user language to at least one different language corresponding to the one or more translation preferences; and \n",
      "transmitting the message in translated form to at least one of the two or more devices. \n",
      "\n",
      " \n",
      "\n",
      "{'': [, , ], 'message as input': [a message as input], 'network': [a network], 'message': [the message], 'method': [the method], 'message in translated form': [the message in translated form], 'user language': [the user language], 'user language to at least one different language': [the user language to at least one different language], 'method for translating instant messages': [A method for translating instant messages], 'users': [the users], 'user profile indicating at least one user language': [a user profile indicating at least one user language]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A vehicle device control system comprising:\n",
      "a portable unit carried by a user; and\n",
      "a vehicle-side unit mounted in a vehicle for performing bilateral communications with the portable unit,\n",
      "in each of the bilateral communications, the vehicle-side unit transmitting a request signal in a predetermined area around the vehicle and the portable unit transmitting a response signal in response to the request signal of the vehicle-side unit,\n",
      "the vehicle-side unit checking up an identification code included in the response signal received from the portable unit with a pre-stored registration code, and\n",
      "the vehicle-side unit controlling a vehicle device mounted in the vehicle on condition that a checkup result indicates a successful checkup operation,\n",
      "wherein the vehicle-side unit includes:\n",
      "a vehicle-side transmission section for transmitting the request signal to the portable unit;\n",
      "a vehicle-side reception section for receiving the response signal from the portable unit; and\n",
      "a vehicle-side control section for instructing the vehicle-side transmission section to transmit the request signal at a predetermined transmission interval specific to the vehicle side unit, when the response signal is received from the portable unit by the vehicle-side reception section, and\n",
      "wherein the portable unit is operable with power supply from a battery, and includes:\n",
      "a portable-side reception section for receiving the request signal from the vehicle-side unit;\n",
      "a portable-side transmission section for transmitting the response signal to the vehicle-side unit;\n",
      "a portable-side memory section for storing the transmission interval in the vehicle-side unit; and\n",
      "a portable-side control section for instructing the portable-side transmission section to transmit the response signal when the request signal is received by the portable-side reception section, and for instructing the portable-side reception section to take a reception state at a timing of next transmission of the request signal in correspondence to the transmission interval stored in the portable-side memory section.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "{'': [], 'response signal in response': [a response signal in response], 'portable-side transmission section': [a portable-side transmission section, the portable-side transmission section], 'bilateral communications': [the bilateral communications], 'predetermined area': [a predetermined area], 'timing of next transmission': [a timing of next transmission], 'reception state': [a reception state], 'user': [a user], 'identification code': [an identification code], 'portable unit': [a portable unit, the portable unit, the portable unit, the portable unit, the portable unit, the portable unit, the portable unit], 'vehicle device': [a vehicle device], 'vehicle-side reception section': [a vehicle-side reception section, the vehicle-side reception section], 'battery': [a battery], 'checkup result': [a checkup result], 'portable-side control section': [a portable-side control section], 'response signal': [the response signal, the response signal, the response signal, the response signal, the response signal], 'portable-side reception section': [a portable-side reception section, the portable-side reception section, the portable-side reception section], 'vehicle-side control section': [a vehicle-side control section], 'successful checkup operation': [a successful checkup operation], 'request signal in correspondence': [the request signal in correspondence], 'vehicle device control system': [A vehicle device control system], 'request signal': [a request signal, the request signal, the request signal, the request signal, the request signal, the request signal], 'vehicle': [the vehicle], 'predetermined transmission interval': [a predetermined transmission interval], 'vehicle for performing bilateral communications': [a vehicle for performing bilateral communications], 'portable-side memory section': [a portable-side memory section], 'vehicle side unit': [the vehicle side unit], 'pre-stored registration code': [a pre-stored registration code], 'vehicle-side unit': [a vehicle-side unit, the vehicle-side unit, the vehicle-side unit, the vehicle-side unit, the vehicle-side unit, the vehicle-side unit, the vehicle-side unit, the vehicle-side unit, the vehicle-side unit], 'transmission interval': [the transmission interval, the transmission interval], 'portable unit is operable with power supply': [the portable unit is operable with power supply], 'vehicle on condition': [the vehicle on condition], 'vehicle-side transmission section': [a vehicle-side transmission section, the vehicle-side transmission section]} \n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d, e in zip(nlp_docs, ent_from_claims):\n",
    "    print(d, \"\\n\")\n",
    "    print(e, \"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Matching occurrences of \"the X\" with other entries looks generally useful (e.g. is needed across multiple claims). Phrases such as \"the given X\" or \"the selected X\" also appear.\n",
    "* There are some long sections that appear not to meet the simple parse.\n",
    "* Some have a blank entity?\n",
    "* We could use the noun_chunks as a second test and merge for greater accuracy?\n",
    "* Doesn't work so well on some method claims.\n",
    "* Need to stop on punctuation as well, i.e. \",\" or \";\"\n",
    "* \"said\" needs to be a DET.\n",
    "* Plurals cause an issue, e.g. \"multimedia data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " [An energy park, a plurality, energy collectors, known spatial location, the energy collectors, collector-local information, the information, the plurality, collectors] \n",
      "\n",
      "['energy park', 'plurality of energy collectors having known spatial location', 'information', 'energy collectors generate collector-local information']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A method, designing, a printed circuit board, a plurality, conductive layers, conductive areas, a constant potential, conductive areas, wiring, areas, the plurality, conductive layers, areas, the specified conductive areas, an interlayer connection member, the plurality, conductive layers, the extracted area, an area, the extracted areas, a predetermined distance, the specified interlayer connection member] \n",
      "\n",
      "['printed circuit board', '', 'method used for supporting designing', 'conductive areas', 'constant potential is applied, comprising:\\nspecifying conductive areas including wiring', 'area', 'plurality of conductive layers', 'predetermined distance', 'extracted areas', 'plurality of conductive layers including conductive areas', 'interlayer connection member', 'extracted area', 'specified conductive areas']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [file system information, a write-once optical disc, the write-once optical disc, the step, file information, a former session, information, a current session, a metadata file, the current session] \n",
      "\n",
      "['write-once optical disc', 'step of recording file information', 'former session', 'metadata file', 'current session', 'file system information recording method']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A system, electronic devices, compliance, second hand dealer laws, an apparatus, a fingerprint reader, the inspection area, the inspection area, connection, an electronic device, the inspection area, a database, the apparatus, the network, the processor, the apparatus, an identification, a recycler, process, a fingerprint, the reseller, the database, an image, the recycler, the database, an identification, the electronic device, the processor, transfer data, the image, the reseller, the reseller, the fingerprint, the reseller, the identification, the electronic device, the processor, the transfer data, the server, the network, the server, the transfer data, view, a second hand dealer law, a jurisdiction, a location, the apparatus, a procedure, compliance, the second hand dealer law, the second hand dealer law] \n",
      "\n",
      "['identification information', 'location', 'identification', 'database', 'procedure for compliance', 'processor configured to collect as transfer data', 'jurisdiction', 'electronic device', 'apparatus', 'system', 'image', 'identification reader', 'transfer data', 'inspection area', 'electrical connector', 'network', 'processor', 'second hand dealer law', 'transfer data in view', 'recycler, process', 'system for reselling electronic devices in compliance with second hand dealer laws', 'reseller', 'exterior imaging component', 'fingerprint', 'electrical connector capable of connection', 'server', 'imaging component', 'fingerprint reader', 'recycler']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A device, at least one encrypted processor, an input map, a texture pattern, a reduced resolution pattern, the input map, at least one second processor, the at least one encrypted processor, the reduced resolution pattern, a plurality, similar patterns, a stored encrypted map template, a match, the reduced resolution pattern, the plurality, similar patterns, the at least one encrypted processor, the match-set, a match, the input map, any stored encrypted map template, the match-set] \n",
      "\n",
      "['reduced resolution pattern', '', 'input map', 'stored encrypted map template', 'texture pattern', 'plurality of similar patterns', 'device, comprising:\\nat least one encrypted processor', 'match', 'at least one encrypted processor', 'match-set']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A control device, a hybrid vehicle, an engine, a motor/generator, wheel, the hybrid vehicle, an electric drive mode, the vehicle, the motor/generator, a hybrid drive mode, the vehicle, both the engine, the motor/generator, the control device, a controller, a first threshold level, an accelerator opening, a second threshold level, the accelerator opening, a hysteresis value, the first threshold level, the second threshold level, the hysteresis value, a vehicle operating state, a driving environment, a signal, the accelerator opening, a changeover, the hybrid drive mode, the electric drive mode, the accelerator opening, the first threshold level, a changeover, the electric drive mode, the hybrid drive mode, the accelerator opening, the second threshold value] \n",
      "\n",
      "['engine', '', 'electric drive mode', 'driving environment', 'vehicle', 'hybrid vehicle', 'controller', 'second threshold level', 'signal', 'changeover', 'hysteresis value', 'accelerator opening', 'control device', 'motor/generator', 'hybrid drive mode', 'vehicle operating state', 'first threshold level']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [An information processing device, a graphics plane, a graphics image, a storage region, two image storage regions, an L, region, an image storage region, an image, the left eye, an R, an image storage region, an image, the right eye, an array, said image, the left eye, animation, L region, drawing, said image, the right eye, animation, said R region] \n",
      "\n",
      "['graphics image', 'l (left) region', 'graphics plane', 'image', 'left eye', 'left eye used for animation as to said l region', 'storage region where two image storage regions', 'array', 'right eye', 'r (right) region', 'image storage region', 'information processing device']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A system, data, a server, a communication device, accordance, a set, predefined rules, a personal content filter database, the set, predefined rules, the set, predefined rules, user, rules, an associated user, a user interface, the communication device, a content filter engine, the set, predefined rules, information, the communication device] \n",
      "\n",
      "['set of predefined rules by preventing restricted information', 'communication device', 'user interface', 'set of predefined rules comprising user defined rules', 'content filter engine', 'system for filtering data', 'system', 'set of predefined rules', 'communication device in accordance', 'server', 'personal content filter database', 'associated user']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A method, instant messages, two or more devices, a network, one or more users, different languages, a user profile, at least one user language, one or more translation preferences, the one or more users, a message, input, the users, the user language, the message, the user language, at least one different language, the one or more translation preferences, the message, form, the two or more devices] \n",
      "\n",
      "['', 'message as input', 'network', 'message', 'method', 'message in translated form', 'user language', 'user language to at least one different language', 'method for translating instant messages', 'users', 'user profile indicating at least one user language']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A vehicle device control system, a portable unit, a user, a vehicle-side unit, a vehicle, bilateral communications, the portable unit, the bilateral communications, a request signal, a predetermined area, the vehicle, the portable unit, a response signal, response, the request signal, the vehicle-side unit, the vehicle-side unit, an identification code, the response signal, the portable unit, a pre-stored registration code, the vehicle-side unit, a vehicle device, the vehicle, condition, a checkup result, a successful checkup operation, the vehicle-side unit, a vehicle-side transmission section, the request signal, the portable unit, a vehicle-side reception section, the response signal, the portable unit, a vehicle-side control section, the vehicle-side transmission section, the request signal, a predetermined transmission interval, the vehicle side unit, the response signal, the portable unit, the vehicle-side reception section, the portable unit, power supply, a battery, a portable-side reception section, the request signal, the vehicle-side unit, a portable-side transmission section, the response signal, the vehicle-side unit, a portable-side memory section, the transmission interval, the vehicle-side unit, a portable-side control section, the portable-side transmission section, the response signal, the request signal, the portable-side reception section, the portable-side reception section, a reception state, a timing, next transmission, the request signal, correspondence, the transmission interval, the portable-side memory section] \n",
      "\n",
      "['', 'response signal in response', 'portable-side transmission section', 'bilateral communications', 'predetermined area', 'timing of next transmission', 'reception state', 'user', 'identification code', 'portable unit', 'vehicle device', 'vehicle-side reception section', 'battery', 'checkup result', 'portable-side control section', 'response signal', 'portable-side reception section', 'vehicle-side control section', 'successful checkup operation', 'request signal in correspondence', 'vehicle device control system', 'request signal', 'vehicle', 'predetermined transmission interval', 'vehicle for performing bilateral communications', 'portable-side memory section', 'vehicle side unit', 'pre-stored registration code', 'vehicle-side unit', 'transmission interval', 'portable unit is operable with power supply', 'vehicle on condition', 'vehicle-side transmission section']\n",
      "\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d, e in zip(nlp_docs, ent_from_claims):\n",
    "    print(\"----\\n\", list(d.noun_chunks), \"\\n\")\n",
    "    print(list(e.keys()))\n",
    "    print(\"\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I define the problem using probabilities?  \n",
    "\n",
    "Entities are latent variables of which the words are the visible / observable data.  \n",
    "\n",
    "Problem is aligning groups of tokens with entities. Classification in a case where we don't know what the classes are or how many classes there are.  \n",
    "\n",
    "P(entity | words)\n",
    "\n",
    "What do we know for certain:\n",
    "* It will have a form of DET ... NOUN or no DET but noun phrase ending in NNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def annotated_entity_extraction(doc):\n",
    "    entity_list = list()\n",
    "    record = False\n",
    "    # Generate a list of tokens so we can iterate backwards through it\n",
    "    enum_doc_list = list(enumerate(doc))\n",
    "    last_end = 0\n",
    "    # Add indices\n",
    "    for i, word in enum_doc_list:\n",
    "        print(i, word, record)\n",
    "        if word.pos == DET and not record:\n",
    "            # Start recording and record start index\n",
    "            record = True\n",
    "            start_index = i\n",
    "            print(\"Starting to record at {0}-{1}\".format(i, word))\n",
    "        else:        \n",
    "            if (word.pos == DET or word.pos == CCONJ or word.lemma_ == \";\" or word.lemma_ == '.') and record:\n",
    "                print(\"Stepping back at {0}-{1}\".format(i, word))\n",
    "                # Step back until last noun is found\n",
    "                added = False\n",
    "                for j, bword in reversed(enum_doc_list[last_end:i]):\n",
    "                    print(j, bword, last_end)\n",
    "                    if bword.pos == NOUN:\n",
    "                        # Add np_chunk to buffer\n",
    "                        print(\"-----> Adding from {0}-{1} = {2}\".format(j, i, doc[start_index:j+1]))\n",
    "                        entity_list.append(doc[start_index:j+1])\n",
    "                        last_end = j+1\n",
    "                        added = True\n",
    "                        break\n",
    "                # Here if nothing has been added, e.g. no noun found, we need to keep recording\n",
    "                if word.pos == DET:\n",
    "                    # Set new start index\n",
    "                    record = True\n",
    "                    start_index = i\n",
    "                    print(\"Starting to record again at {0}-{1}\".format(i, word))\n",
    "                else:\n",
    "                    if (word.pos == CCONJ and not added):\n",
    "                        record = True\n",
    "                    else:\n",
    "                        record = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " False\n",
      "1 1 False\n",
      "2 . False\n",
      "3 An False\n",
      "Starting to record at 3-An\n",
      "4 energy True\n",
      "5 park True\n",
      "6 including True\n",
      "7 a True\n",
      "Stepping back at 7-a\n",
      "6 including 0\n",
      "5 park 0\n",
      "-----> Adding from 5-7 = An energy park\n",
      "Starting to record again at 7-a\n",
      "8 plurality True\n",
      "9 of True\n",
      "10 energy True\n",
      "11 collectors True\n",
      "12 having True\n",
      "13 known True\n",
      "14 spatial True\n",
      "15 location True\n",
      "16 where True\n",
      "17 the True\n",
      "Stepping back at 17-the\n",
      "16 where 6\n",
      "15 location 6\n",
      "-----> Adding from 15-17 = a plurality of energy collectors having known spatial location\n",
      "Starting to record again at 17-the\n",
      "18 energy True\n",
      "19 collectors True\n",
      "20 generate True\n",
      "21 collector True\n",
      "22 - True\n",
      "23 local True\n",
      "24 information True\n",
      "25 and True\n",
      "Stepping back at 25-and\n",
      "24 information 16\n",
      "-----> Adding from 24-25 = the energy collectors generate collector-local information\n",
      "26 the False\n",
      "Starting to record at 26-the\n",
      "27 information True\n",
      "28 from True\n",
      "29 the True\n",
      "Stepping back at 29-the\n",
      "28 from 25\n",
      "27 information 25\n",
      "-----> Adding from 27-29 = the information\n",
      "Starting to record again at 29-the\n",
      "30 plurality True\n",
      "31 of True\n",
      "32 collectors True\n",
      "33 is True\n",
      "34 spatially True\n",
      "35 correlated True\n",
      "36 . True\n",
      "Stepping back at 36-.\n",
      "35 correlated 28\n",
      "34 spatially 28\n",
      "33 is 28\n",
      "32 collectors 28\n",
      "-----> Adding from 32-36 = the plurality of collectors\n",
      "37 \n",
      " False\n"
     ]
    }
   ],
   "source": [
    "annotated_entity_extraction(nlp_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " False\n",
      "1 1 False\n",
      "2 . False\n",
      "3 A False\n",
      "Starting to record at 3-A\n",
      "4 method True\n",
      "5 used True\n",
      "6 for True\n",
      "7 supporting True\n",
      "8 designing True\n",
      "9 of True\n",
      "10 a True\n",
      "Stepping back at 10-a\n",
      "9 of 0\n",
      "8 designing 0\n",
      "-----> Adding from 8-10 = A method used for supporting designing\n",
      "Starting to record again at 10-a\n",
      "11 printed True\n",
      "12 circuit True\n",
      "13 board True\n",
      "14 including True\n",
      "15 a True\n",
      "Stepping back at 15-a\n",
      "14 including 9\n",
      "13 board 9\n",
      "-----> Adding from 13-15 = a printed circuit board\n",
      "Starting to record again at 15-a\n",
      "16 plurality True\n",
      "17 of True\n",
      "18 conductive True\n",
      "19 layers True\n",
      "20 including True\n",
      "21 conductive True\n",
      "22 areas True\n",
      "23 to True\n",
      "24 which True\n",
      "25 a True\n",
      "Stepping back at 25-a\n",
      "24 which 14\n",
      "23 to 14\n",
      "22 areas 14\n",
      "-----> Adding from 22-25 = a plurality of conductive layers including conductive areas\n",
      "Starting to record again at 25-a\n",
      "26 constant True\n",
      "27 potential True\n",
      "28 is True\n",
      "29 applied True\n",
      "30 , True\n",
      "31 comprising True\n",
      "32 : True\n",
      "33 \n",
      " True\n",
      "34 specifying True\n",
      "35 conductive True\n",
      "36 areas True\n",
      "37 including True\n",
      "38 wiring True\n",
      "39 from True\n",
      "40 the True\n",
      "Stepping back at 40-the\n",
      "39 from 23\n",
      "38 wiring 23\n",
      "-----> Adding from 38-40 = a constant potential is applied, comprising:\n",
      "specifying conductive areas including wiring\n",
      "Starting to record again at 40-the\n",
      "41 conductive True\n",
      "42 areas True\n",
      "43 for True\n",
      "44 each True\n",
      "Stepping back at 44-each\n",
      "43 for 39\n",
      "42 areas 39\n",
      "-----> Adding from 42-44 = the conductive areas\n",
      "Starting to record again at 44-each\n",
      "45 of True\n",
      "46 the True\n",
      "Stepping back at 46-the\n",
      "45 of 43\n",
      "44 each 43\n",
      "43 for 43\n",
      "Starting to record again at 46-the\n",
      "47 plurality True\n",
      "48 of True\n",
      "49 conductive True\n",
      "50 layers True\n",
      "51 ; True\n",
      "Stepping back at 51-;\n",
      "50 layers 43\n",
      "-----> Adding from 50-51 = the plurality of conductive layers\n",
      "52 \n",
      " False\n",
      "53 extracting False\n",
      "54 areas False\n",
      "55 which False\n",
      "56 overlap False\n",
      "57 each False\n",
      "Starting to record at 57-each\n",
      "58 other True\n",
      "59 from True\n",
      "60 the True\n",
      "Stepping back at 60-the\n",
      "59 from 51\n",
      "58 other 51\n",
      "57 each 51\n",
      "56 overlap 51\n",
      "55 which 51\n",
      "54 areas 51\n",
      "-----> Adding from 54-60 = \n",
      "Starting to record again at 60-the\n",
      "61 specified True\n",
      "62 conductive True\n",
      "63 areas True\n",
      "64 ; True\n",
      "Stepping back at 64-;\n",
      "63 areas 55\n",
      "-----> Adding from 63-64 = the specified conductive areas\n",
      "65 \n",
      " False\n",
      "66 specifying False\n",
      "67 an False\n",
      "Starting to record at 67-an\n",
      "68 interlayer True\n",
      "69 connection True\n",
      "70 member True\n",
      "71 that True\n",
      "72 electrically True\n",
      "73 connects True\n",
      "74 at True\n",
      "75 least True\n",
      "76 two True\n",
      "77 of True\n",
      "78 the True\n",
      "Stepping back at 78-the\n",
      "77 of 64\n",
      "76 two 64\n",
      "75 least 64\n",
      "74 at 64\n",
      "73 connects 64\n",
      "72 electrically 64\n",
      "71 that 64\n",
      "70 member 64\n",
      "-----> Adding from 70-78 = an interlayer connection member\n",
      "Starting to record again at 78-the\n",
      "79 plurality True\n",
      "80 of True\n",
      "81 conductive True\n",
      "82 layers True\n",
      "83 in True\n",
      "84 the True\n",
      "Stepping back at 84-the\n",
      "83 in 71\n",
      "82 layers 71\n",
      "-----> Adding from 82-84 = the plurality of conductive layers\n",
      "Starting to record again at 84-the\n",
      "85 extracted True\n",
      "86 area True\n",
      "87 ; True\n",
      "Stepping back at 87-;\n",
      "86 area 83\n",
      "-----> Adding from 86-87 = the extracted area\n",
      "88 and False\n",
      "89 \n",
      " False\n",
      "90 specifying False\n",
      "91 an False\n",
      "Starting to record at 91-an\n",
      "92 area True\n",
      "93 in True\n",
      "94 the True\n",
      "Stepping back at 94-the\n",
      "93 in 87\n",
      "92 area 87\n",
      "-----> Adding from 92-94 = an area\n",
      "Starting to record again at 94-the\n",
      "95 extracted True\n",
      "96 areas True\n",
      "97 and True\n",
      "Stepping back at 97-and\n",
      "96 areas 93\n",
      "-----> Adding from 96-97 = the extracted areas\n",
      "98 within False\n",
      "99 a False\n",
      "Starting to record at 99-a\n",
      "100 predetermined True\n",
      "101 distance True\n",
      "102 from True\n",
      "103 the True\n",
      "Stepping back at 103-the\n",
      "102 from 97\n",
      "101 distance 97\n",
      "-----> Adding from 101-103 = a predetermined distance\n",
      "Starting to record again at 103-the\n",
      "104 specified True\n",
      "105 interlayer True\n",
      "106 connection True\n",
      "107 member True\n",
      "108 . True\n",
      "Stepping back at 108-.\n",
      "107 member 102\n",
      "-----> Adding from 107-108 = the specified interlayer connection member\n",
      "109 \n",
      "\n",
      " False\n"
     ]
    }
   ],
   "source": [
    "annotated_entity_extraction(nlp_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_entity_finder(doc):\n",
    "    \"\"\" Find entities using noun phrases/chunks.\"\"\"\n",
    "    entity_dict = dict()\n",
    "    for entity in doc.noun_chunks:\n",
    "        np_start = entity.start\n",
    "        # Ignore the determinant \n",
    "        if doc[np_start].pos == DET:\n",
    "            np_start += 1\n",
    "        # Generate a string representation excluding the determinant\n",
    "        np_string = doc[np_start:entity.end].text.lower()\n",
    "                                \n",
    "        if np_string not in entity_dict.keys():\n",
    "            entity_dict[np_string] = list()          \n",
    "        entity_dict[np_string].append(entity)\n",
    "        \n",
    "    return entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': [an area],\n",
       " 'areas': [areas, areas],\n",
       " 'conductive areas': [conductive areas, conductive areas],\n",
       " 'conductive layers': [conductive layers,\n",
       "  conductive layers,\n",
       "  conductive layers],\n",
       " 'constant potential': [a constant potential],\n",
       " 'designing': [designing],\n",
       " 'extracted area': [the extracted area],\n",
       " 'extracted areas': [the extracted areas],\n",
       " 'interlayer connection member': [an interlayer connection member],\n",
       " 'method': [A method],\n",
       " 'plurality': [a plurality, the plurality, the plurality],\n",
       " 'predetermined distance': [a predetermined distance],\n",
       " 'printed circuit board': [a printed circuit board],\n",
       " 'specified conductive areas': [the specified conductive areas],\n",
       " 'specified interlayer connection member': [the specified interlayer connection member],\n",
       " 'wiring': [wiring]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_entity_finder(nlp_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([A method used for supporting designing,\n",
       "  a printed circuit board,\n",
       "  a plurality of conductive layers including conductive areas,\n",
       "  a constant potential is applied, comprising:\n",
       "  specifying conductive areas including wiring,\n",
       "  the conductive areas,\n",
       "  ,\n",
       "  the plurality of conductive layers,\n",
       "  ,\n",
       "  the specified conductive areas,\n",
       "  an interlayer connection member,\n",
       "  the plurality of conductive layers,\n",
       "  the extracted area,\n",
       "  an area,\n",
       "  the extracted areas,\n",
       "  a predetermined distance],\n",
       " {'': [, ],\n",
       "  'area': [an area],\n",
       "  'conductive areas': [the conductive areas],\n",
       "  'constant potential is applied, comprising:\\nspecifying conductive areas including wiring': [a constant potential is applied, comprising:\n",
       "   specifying conductive areas including wiring],\n",
       "  'extracted area': [the extracted area],\n",
       "  'extracted areas': [the extracted areas],\n",
       "  'interlayer connection member': [an interlayer connection member],\n",
       "  'method used for supporting designing': [A method used for supporting designing],\n",
       "  'plurality of conductive layers': [the plurality of conductive layers,\n",
       "   the plurality of conductive layers],\n",
       "  'plurality of conductive layers including conductive areas': [a plurality of conductive layers including conductive areas],\n",
       "  'predetermined distance': [a predetermined distance],\n",
       "  'printed circuit board': [a printed circuit board],\n",
       "  'specified conductive areas': [the specified conductive areas]})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_spacy_entity_finder(nlp_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current session': [a current session, the current session],\n",
       " 'file information': [file information],\n",
       " 'file system information': [file system information],\n",
       " 'former session': [a former session],\n",
       " 'information': [information],\n",
       " 'metadata file': [a metadata file],\n",
       " 'step': [the step],\n",
       " 'write-once optical disc': [a write-once optical disc,\n",
       "  the write-once optical disc]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_entity_finder(nlp_docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([a write-once optical disc,\n",
       "  a file system information recording method,\n",
       "  the write-once optical disc,\n",
       "  the step of recording file information,\n",
       "  a former session,\n",
       "  a current session,\n",
       "  a metadata file],\n",
       " {'current session': [a current session],\n",
       "  'file system information recording method': [a file system information recording method],\n",
       "  'former session': [a former session],\n",
       "  'metadata file': [a metadata file],\n",
       "  'step of recording file information': [the step of recording file information],\n",
       "  'write-once optical disc': [a write-once optical disc,\n",
       "   the write-once optical disc]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_spacy_entity_finder(nlp_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Algorithm\n",
    "\n",
    "What do we know:\n",
    "* A DET or a NOUN will always form part of an entity.\n",
    "* A plural noun may not start with a DET.\n",
    "* An entity will consist of consecutive tokens.\n",
    "* The world following a DET will be part of the entity.\n",
    "* Each determinant can only be linked to one of the nouns in front of it before the next determinant or [\";\", \":\", \".\"] (and possibly \",\").\n",
    "* Entities with a \"the\" determinant should have occurred before.\n",
    "* There are no overlaps.\n",
    "* We can be more confident if a phrase is repeated.\n",
    "* We can be more confident still if the phrase is repeated that initially starts with \"a\" and the next occurrence starts with \"the\" or \"said\".\n",
    "* \"said\" should be taken as a DET.\n",
    "* There will be between 1 and number of NOUNS entities.\n",
    "* The boundary of an entity will be marked by NOUN NOTNOUN - however this pattern can also occur as part of the noun phrase for the entity.\n",
    "* Entity text sequences will not cross a \":\" or \";\".\n",
    "* Occurrences of an entity will have matching text including at least a matching noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definite constraints for a well-formed claim:\n",
    "* A NOUN will always form part of an entity;\n",
    "* A singular noun will have a determinant;\n",
    "* An entity will consist of consecutive tokens.\n",
    "* There are no overlaps in occurrences - a word can only be linked to a single entity.\n",
    "* There will be between 1 and number of NOUNS entities.\n",
    "* Entity text sequences will not cross a \":\" or \";\" or \".\" (and possibly a \",\").\n",
    "* The boundary of an entity will be marked by NOUN NOTNOUN - however this pattern can also occur as part of the noun phrase for the entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the probability of a set of entities, $ \\boldsymbol E $, given a claim as a sequence of words, $ \\boldsymbol W $: $$ P(\\boldsymbol E | \\boldsymbol W) $$   \n",
    "\n",
    "In fact we want to calculate: $$ \\underset{\\boldsymbol E}{\\operatorname{argmax}} P(\\boldsymbol E | \\boldsymbol W) $$\n",
    "\n",
    "Our claim has a length $ N $:$$\\boldsymbol W = (\\boldsymbol w_0, \\boldsymbol w_1, ..., \\boldsymbol w_{N})$$\n",
    "\n",
    "$N$ may be calculated as the length of the claim in tokens.\n",
    "\n",
    "Each word $\\boldsymbol w_i$ has:\n",
    "* text - $t_i$;\n",
    "* a simple POS tag - $pos_i$;\n",
    "* a more detailed POS tag - $posplus_i$;\n",
    "* a lemma (i.e. a normalised word form) - $lemma_i$; and\n",
    "* dependeny tree information - $dep_i$.\n",
    "\n",
    "I.e. $$ \\boldsymbol w_i = (t_i, pos_i, posplus_i, lemma_i, dep_i) $$\n",
    "\n",
    "We have $ M $ entities: $$\\boldsymbol E = (e_0, e_1, ..., e_{M})$$ \n",
    "\n",
    "where $\\boldsymbol e_0 $ indicates \"no related entity\" or a \"null\" token. $M$ is not known but will be greater than 2 and less than a number of nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An occurrence is a set of consecutive tokens: $$ \\boldsymbol o_k = [\\boldsymbol w_i, \\boldsymbol w_{i+1}, ..., \\boldsymbol w_{i+L_{k}}] $$ where $L_k$ is the length of occurrence $k$ which begins at word index $i$.\n",
    "\n",
    "$$ \\boldsymbol W = [o_1, o_2, ..., o_K] $$ where there are $K$ total occurrences in the claim. However, we don't know $K$ for sure. \n",
    "\n",
    "We do know the number of nouns $N_{noun}$. And we know $1 \\leqslant K \\leqslant N_{noun}$. Also $M \\leqslant K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An entity can have:\n",
    "* a set of one or more occurrences;\n",
    "* a string representation - possibly equal to common text across the set of occurrences;\n",
    "* a number (e.g. be singular or plural).\n",
    "\n",
    "An entity may be though of as a class label that is applied to a word: $$ \\sum_{i=0}^M p(e_i | w) = 1 $$\n",
    "\n",
    "We know that $ p(e_0 | pos = {DET}) = p(e_0 | pos = {NOUN}) = 0 $, i.e. that determinants and nouns will be assigned to some entity. We also know $ p(e_0 | t = \";\") = p(e_0 | t = \":\") = p(e_0 | t = \".\") = 1$.\n",
    "\n",
    "Entities are primarily just groupings of word spans, wherein the grouping creates a discrete entity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{i=0}^M P(\\boldsymbol o_k | e_i) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decomposing using Bayes' Rule: \n",
    "\n",
    "$$ \\underset{\\boldsymbol e}{\\operatorname{argmax}} P(\\boldsymbol e | \\boldsymbol w) = {P(\\boldsymbol w | \\boldsymbol e) P(\\boldsymbol e)}/ P(\\boldsymbol w)$$ \n",
    "\n",
    "where we can ignore the denominator as we are looking for argmax: $$ \\underset{\\boldsymbol e}{\\operatorname{argmax}} P(\\boldsymbol e | \\boldsymbol w) = {P(\\boldsymbol w | \\boldsymbol e) P(\\boldsymbol e)}$$\n",
    "\n",
    "In other models $P(\\boldsymbol w | \\boldsymbol e)$ and $P(\\boldsymbol e)$ may be approximated by a product of transitions (e.g. as per a hidden markov model). However, we have dependencies across sets of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each determinant can only be linked to one of the nouns in front of it before the next determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by setting each noun as a separate entity? And marking the tokens that are not an entity? Or look at confident selections e.g. DET NOUN [:;.,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can maybe start with a binary classification: $\\boldsymbol e = [0,1]$? No, we can confidently apply a positive determination but our negative determination is unknown, i.e. a word that is not positively marked may still form part of an entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate $M$ by counting the number of \"a\"/\"an\" determinants + the number of multiple nouns.  \n",
    "\n",
    "Issue multiple nouns are often introduced by \"a X of Ys\".  \n",
    "\n",
    "Also we have \"at least one X\" and \"one or more Ys\" - these may not be introduced by \"a\" or \"an\" and \"at least one\" may be referred to again as \"the at least one\".  \n",
    "\n",
    "Can we use an estimate of number of determinants as a lower bound?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works fairly well for a lower bound / initial estimate.  \n",
    "\n",
    "We can cross check later for missing plural nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we model a sequential constraint? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word $w_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pass\n",
      "\n",
      " [{}]\n",
      "1 [{}]\n",
      ". [{0: 1}]\n",
      "A [{0: 0}]\n",
      "device [{0: 0}]\n",
      ", [{0: 1}]\n",
      "comprising [{}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{}]\n",
      "at [{0: 0}]\n",
      "least [{}]\n",
      "one [{}]\n",
      "encrypted [{}]\n",
      "processor [{0: 0}]\n",
      "that [{}]\n",
      "is [{}]\n",
      "configured [{}]\n",
      "to [{}]\n",
      "determine [{}]\n",
      "an [{0: 0}]\n",
      "input [{0: 0}]\n",
      "map [{0: 0}]\n",
      "associated [{}]\n",
      "with [{}]\n",
      "a [{0: 0}]\n",
      "texture [{0: 0}]\n",
      "pattern [{0: 0}]\n",
      "and [{}]\n",
      "calculate [{}]\n",
      "a [{0: 0}]\n",
      "reduced [{}]\n",
      "resolution [{0: 0}]\n",
      "pattern [{0: 0}]\n",
      "based [{}]\n",
      "on [{}]\n",
      "the [{0: 0}]\n",
      "input [{0: 0}]\n",
      "map [{0: 0}]\n",
      "; [{0: 1}]\n",
      "and [{}]\n",
      "\n",
      " [{}]\n",
      "at [{0: 0}]\n",
      "least [{}]\n",
      "one [{}]\n",
      "second [{}]\n",
      "processor [{0: 0}]\n",
      ", [{0: 1}]\n",
      "communicably [{}]\n",
      "connected [{}]\n",
      "to [{}]\n",
      "the [{0: 0}]\n",
      "at [{}]\n",
      "least [{}]\n",
      "one [{}]\n",
      "encrypted [{}]\n",
      "processor [{0: 0}]\n",
      ", [{0: 1}]\n",
      "that [{0: 0}]\n",
      "is [{}]\n",
      "configured [{}]\n",
      "to [{}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{}]\n",
      "receive [{}]\n",
      "the [{0: 0}]\n",
      "reduced [{}]\n",
      "resolution [{0: 0}]\n",
      "pattern [{0: 0}]\n",
      "and [{}]\n",
      "a [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{}]\n",
      "similar [{}]\n",
      "patterns [{0: 0}]\n",
      ", [{0: 1}]\n",
      "each [{0: 0}]\n",
      "associated [{}]\n",
      "with [{}]\n",
      "a [{0: 0}]\n",
      "stored [{}]\n",
      "encrypted [{}]\n",
      "map [{0: 0}]\n",
      "template [{0: 0}]\n",
      ", [{0: 1}]\n",
      "and [{}]\n",
      "\n",
      " [{}]\n",
      "identify [{}]\n",
      "a [{0: 0}]\n",
      "match [{0: 0}]\n",
      "- [{}]\n",
      "set [{}]\n",
      "if [{}]\n",
      "a [{0: 0}]\n",
      "match [{0: 0}]\n",
      "exists [{}]\n",
      "between [{}]\n",
      "the [{0: 0}]\n",
      "reduced [{}]\n",
      "resolution [{0: 0}]\n",
      "pattern [{0: 0}]\n",
      "and [{}]\n",
      "the [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{}]\n",
      "similar [{}]\n",
      "patterns [{0: 0}]\n",
      ", [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "wherein [{}]\n",
      "the [{0: 0}]\n",
      "at [{}]\n",
      "least [{}]\n",
      "one [{}]\n",
      "encrypted [{}]\n",
      "processor [{0: 0}]\n",
      "is [{}]\n",
      "further [{}]\n",
      "configured [{}]\n",
      "to [{}]\n",
      "receive [{}]\n",
      "the [{0: 0}]\n",
      "match [{0: 0}]\n",
      "- [{}]\n",
      "set [{0: 0}]\n",
      "and [{}]\n",
      "identify [{}]\n",
      "if [{}]\n",
      "a [{0: 0}]\n",
      "match [{0: 0}]\n",
      "exists [{}]\n",
      "between [{}]\n",
      "the [{0: 0}]\n",
      "input [{0: 0}]\n",
      "map [{0: 0}]\n",
      "and [{}]\n",
      "any [{0: 0}]\n",
      "stored [{}]\n",
      "encrypted [{}]\n",
      "map [{0: 0}]\n",
      "template [{0: 0}]\n",
      "associated [{}]\n",
      "with [{}]\n",
      "the [{0: 0}]\n",
      "match [{0: 0}]\n",
      "- [{}]\n",
      "set [{0: 0}]\n",
      ". [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "Second pass\n",
      "device is e_0=0\n",
      "Next - A is e_0=0\n",
      "Found - A device\n",
      "map is e_0=0\n",
      "Next - input is e_0=0\n",
      "processor is e_0=0\n",
      "Next - at is e_0=0\n",
      "Found - at least one second processor\n",
      "processor is e_0=0\n",
      "Next - the is e_0=0\n",
      "Found - the at least one encrypted processor\n",
      "patterns is e_0=0\n",
      "Next - plurality is e_0=0\n",
      "template is e_0=0\n",
      "Next - map is e_0=0\n",
      "patterns is e_0=0\n",
      "Next - plurality is e_0=0\n",
      "set is e_0=0\n",
      "Next - match is e_0=0\n",
      "\n",
      " [{}]\n",
      "1 [{}]\n",
      ". [{0: 1}]\n",
      "A [{0: 0}]\n",
      "device [{0: 0}]\n",
      ", [{0: 1}]\n",
      "comprising [{}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{}]\n",
      "at [{0: 0}]\n",
      "least [{}]\n",
      "one [{}]\n",
      "encrypted [{}]\n",
      "processor [{0: 0}]\n",
      "that [{}]\n",
      "is [{}]\n",
      "configured [{}]\n",
      "to [{}]\n",
      "determine [{}]\n",
      "an [{0: 0}]\n",
      "input [{0: 0}]\n",
      "map [{0: 0}]\n",
      "associated [{}]\n",
      "with [{}]\n",
      "a [{0: 0}]\n",
      "texture [{0: 0}]\n",
      "pattern [{0: 0}]\n",
      "and [{}]\n",
      "calculate [{}]\n",
      "a [{0: 0}]\n",
      "reduced [{}]\n",
      "resolution [{0: 0}]\n",
      "pattern [{0: 0}]\n",
      "based [{}]\n",
      "on [{}]\n",
      "the [{0: 0}]\n",
      "input [{0: 0}]\n",
      "map [{0: 0}]\n",
      "; [{0: 1}]\n",
      "and [{}]\n",
      "\n",
      " [{}]\n",
      "at [{0: 0}]\n",
      "least [{0: 0}]\n",
      "one [{0: 0}]\n",
      "second [{0: 0}]\n",
      "processor [{0: 0}]\n",
      ", [{0: 1}]\n",
      "communicably [{}]\n",
      "connected [{}]\n",
      "to [{}]\n",
      "the [{0: 0}]\n",
      "at [{0: 0}]\n",
      "least [{0: 0}]\n",
      "one [{0: 0}]\n",
      "encrypted [{0: 0}]\n",
      "processor [{0: 0}]\n",
      ", [{0: 1}]\n",
      "that [{0: 0}]\n",
      "is [{}]\n",
      "configured [{}]\n",
      "to [{}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{}]\n",
      "receive [{}]\n",
      "the [{0: 0}]\n",
      "reduced [{}]\n",
      "resolution [{0: 0}]\n",
      "pattern [{0: 0}]\n",
      "and [{}]\n",
      "a [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{}]\n",
      "similar [{}]\n",
      "patterns [{0: 0}]\n",
      ", [{0: 1}]\n",
      "each [{0: 0}]\n",
      "associated [{}]\n",
      "with [{}]\n",
      "a [{0: 0}]\n",
      "stored [{}]\n",
      "encrypted [{}]\n",
      "map [{0: 0}]\n",
      "template [{0: 0}]\n",
      ", [{0: 1}]\n",
      "and [{}]\n",
      "\n",
      " [{}]\n",
      "identify [{}]\n",
      "a [{0: 0}]\n",
      "match [{0: 0}]\n",
      "- [{}]\n",
      "set [{}]\n",
      "if [{}]\n",
      "a [{0: 0}]\n",
      "match [{0: 0}]\n",
      "exists [{}]\n",
      "between [{}]\n",
      "the [{0: 0}]\n",
      "reduced [{}]\n",
      "resolution [{0: 0}]\n",
      "pattern [{0: 0}]\n",
      "and [{}]\n",
      "the [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{}]\n",
      "similar [{}]\n",
      "patterns [{0: 0}]\n",
      ", [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "wherein [{}]\n",
      "the [{0: 0}]\n",
      "at [{}]\n",
      "least [{}]\n",
      "one [{}]\n",
      "encrypted [{}]\n",
      "processor [{0: 0}]\n",
      "is [{}]\n",
      "further [{}]\n",
      "configured [{}]\n",
      "to [{}]\n",
      "receive [{}]\n",
      "the [{0: 0}]\n",
      "match [{0: 0}]\n",
      "- [{}]\n",
      "set [{0: 0}]\n",
      "and [{}]\n",
      "identify [{}]\n",
      "if [{}]\n",
      "a [{0: 0}]\n",
      "match [{0: 0}]\n",
      "exists [{}]\n",
      "between [{}]\n",
      "the [{0: 0}]\n",
      "input [{0: 0}]\n",
      "map [{0: 0}]\n",
      "and [{}]\n",
      "any [{0: 0}]\n",
      "stored [{}]\n",
      "encrypted [{}]\n",
      "map [{0: 0}]\n",
      "template [{0: 0}]\n",
      "associated [{}]\n",
      "with [{}]\n",
      "the [{0: 0}]\n",
      "match [{0: 0}]\n",
      "- [{}]\n",
      "set [{0: 0}]\n",
      ". [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "Third pass\n",
      ". is e_0=1 - looking back\n",
      ", is e_0=1 - looking back\n",
      "Next - device is e_0=0 and noun\n",
      "Next - A is e_0=0 and DET\n",
      "Last break set to 5\n",
      ": is e_0=1 - looking back\n",
      "; is e_0=1 - looking back\n",
      "Next - map is e_0=0 and noun\n",
      "Next - the is e_0=0 and DET\n",
      "Last break set to 38\n",
      ", is e_0=1 - looking back\n",
      "Next - processor is e_0=0 and noun\n",
      ", is e_0=1 - looking back\n",
      "Next - processor is e_0=0 and noun\n",
      "Next - the is e_0=0 and DET\n",
      "Last break set to 56\n",
      ": is e_0=1 - looking back\n",
      ", is e_0=1 - looking back\n",
      "Next - patterns is e_0=0 and noun\n",
      ", is e_0=1 - looking back\n",
      "Next - template is e_0=0 and noun\n",
      ", is e_0=1 - looking back\n",
      "Next - patterns is e_0=0 and noun\n",
      ". is e_0=1 - looking back\n",
      "Next - set is e_0=0 and noun\n",
      "\n",
      "--------\n",
      "\n",
      "Looking for matches for 'A device'\n",
      "Looking for matches for 'the input map'\n",
      "Looking for matches for 'the at least one encrypted processor'\n",
      "Unique entities include ['device', 'at least one encrypted processor', 'input map']\n",
      "[A device]\n",
      "[the at least one encrypted processor]\n",
      "[the input map]\n",
      "\n",
      " [{}]\n",
      "1 [{}]\n",
      ". [{0: 1}]\n",
      "A [{0: 0, 1: 1}]\n",
      "device [{0: 0, 1: 1}]\n",
      ", [{0: 1}]\n",
      "comprising [{}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{}]\n",
      "at [{0: 0}]\n",
      "least [{}]\n",
      "one [{}]\n",
      "encrypted [{}]\n",
      "processor [{0: 0}]\n",
      "that [{}]\n",
      "is [{}]\n",
      "configured [{}]\n",
      "to [{}]\n",
      "determine [{}]\n",
      "an [{0: 0}]\n",
      "input [{0: 0}]\n",
      "map [{0: 0}]\n",
      "associated [{}]\n",
      "with [{}]\n",
      "a [{0: 0}]\n",
      "texture [{0: 0}]\n",
      "pattern [{0: 0}]\n",
      "and [{}]\n",
      "calculate [{}]\n",
      "a [{0: 0}]\n",
      "reduced [{}]\n",
      "resolution [{0: 0}]\n",
      "pattern [{0: 0}]\n",
      "based [{}]\n",
      "on [{}]\n",
      "the [{0: 0, 3: 1}]\n",
      "input [{0: 0, 3: 1}]\n",
      "map [{0: 0, 3: 1}]\n",
      "; [{0: 1}]\n",
      "and [{}]\n",
      "\n",
      " [{}]\n",
      "at [{0: 0}]\n",
      "least [{0: 0}]\n",
      "one [{0: 0}]\n",
      "second [{0: 0}]\n",
      "processor [{0: 0}]\n",
      ", [{0: 1}]\n",
      "communicably [{}]\n",
      "connected [{}]\n",
      "to [{}]\n",
      "the [{0: 0, 2: 1}]\n",
      "at [{0: 0, 2: 1}]\n",
      "least [{0: 0, 2: 1}]\n",
      "one [{0: 0, 2: 1}]\n",
      "encrypted [{0: 0, 2: 1}]\n",
      "processor [{0: 0, 2: 1}]\n",
      ", [{0: 1}]\n",
      "that [{0: 0}]\n",
      "is [{}]\n",
      "configured [{}]\n",
      "to [{}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{}]\n",
      "receive [{}]\n",
      "the [{0: 0}]\n",
      "reduced [{}]\n",
      "resolution [{0: 0}]\n",
      "pattern [{0: 0}]\n",
      "and [{}]\n",
      "a [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{}]\n",
      "similar [{}]\n",
      "patterns [{0: 0}]\n",
      ", [{0: 1}]\n",
      "each [{0: 0}]\n",
      "associated [{}]\n",
      "with [{}]\n",
      "a [{0: 0}]\n",
      "stored [{}]\n",
      "encrypted [{}]\n",
      "map [{0: 0}]\n",
      "template [{0: 0}]\n",
      ", [{0: 1}]\n",
      "and [{}]\n",
      "\n",
      " [{}]\n",
      "identify [{}]\n",
      "a [{0: 0}]\n",
      "match [{0: 0}]\n",
      "- [{}]\n",
      "set [{}]\n",
      "if [{}]\n",
      "a [{0: 0}]\n",
      "match [{0: 0}]\n",
      "exists [{}]\n",
      "between [{}]\n",
      "the [{0: 0}]\n",
      "reduced [{}]\n",
      "resolution [{0: 0}]\n",
      "pattern [{0: 0}]\n",
      "and [{}]\n",
      "the [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{}]\n",
      "similar [{}]\n",
      "patterns [{0: 0}]\n",
      ", [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "wherein [{}]\n",
      "the [{0: 0}]\n",
      "at [{}]\n",
      "least [{}]\n",
      "one [{}]\n",
      "encrypted [{}]\n",
      "processor [{0: 0}]\n",
      "is [{}]\n",
      "further [{}]\n",
      "configured [{}]\n",
      "to [{}]\n",
      "receive [{}]\n",
      "the [{0: 0}]\n",
      "match [{0: 0}]\n",
      "- [{}]\n",
      "set [{0: 0}]\n",
      "and [{}]\n",
      "identify [{}]\n",
      "if [{}]\n",
      "a [{0: 0}]\n",
      "match [{0: 0}]\n",
      "exists [{}]\n",
      "between [{}]\n",
      "the [{0: 0}]\n",
      "input [{0: 0}]\n",
      "map [{0: 0}]\n",
      "and [{}]\n",
      "any [{0: 0}]\n",
      "stored [{}]\n",
      "encrypted [{}]\n",
      "map [{0: 0}]\n",
      "template [{0: 0}]\n",
      "associated [{}]\n",
      "with [{}]\n",
      "the [{0: 0}]\n",
      "match [{0: 0}]\n",
      "- [{}]\n",
      "set [{0: 0}]\n",
      ". [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "{'reduced resolution pattern': [a reduced resolution pattern, the reduced resolution pattern, the reduced resolution pattern], 'stored encrypted map template': [a stored encrypted map template, any stored encrypted map template], 'device': [A device], 'at least one second processor': [at least one second processor], 'texture pattern': [a texture pattern], 'match': [a match, a match], 'similar patterns': [similar patterns, similar patterns], 'match-set': [the match-set, the match-set], 'at least one encrypted processor': [at least one encrypted processor, the at least one encrypted processor, the at least one encrypted processor], 'plurality': [a plurality, the plurality], 'input map': [an input map, the input map, the input map]}\n",
      "([A device, comprising:\n",
      "at least one encrypted processor, an input map, a texture pattern, a reduced resolution pattern, the input map, the at least one encrypted processor, , the reduced resolution pattern, a plurality of similar patterns, , a stored encrypted map template, a match, a match, the reduced resolution pattern, the plurality of similar patterns, the at least one encrypted processor, the match-set, a match, the input map, any stored encrypted map template], {'reduced resolution pattern': [a reduced resolution pattern, the reduced resolution pattern, the reduced resolution pattern], '': [, ], 'input map': [an input map, the input map, the input map], 'stored encrypted map template': [a stored encrypted map template, any stored encrypted map template], 'texture pattern': [a texture pattern], 'plurality of similar patterns': [a plurality of similar patterns, the plurality of similar patterns], 'device, comprising:\\nat least one encrypted processor': [A device, comprising:\n",
      "at least one encrypted processor], 'match': [a match, a match, a match], 'at least one encrypted processor': [the at least one encrypted processor, the at least one encrypted processor], 'match-set': [the match-set]})\n"
     ]
    }
   ],
   "source": [
    "# This is our good algorithm\n",
    "\n",
    "# Start with all words relate to no entities\n",
    "p_all_e_word = dict()\n",
    "\n",
    "def check_start_phrase(token, doc):\n",
    "    \"\"\" Check for start of phrases 'at least one' and 'one or more' as determinant.\n",
    "    \n",
    "    Return true if located.\"\"\"\n",
    "    i = token.i\n",
    "    condition = (\n",
    "        doc[i:i+3].text.lower() == \"at least one\" or\n",
    "        doc[i:i+3].text.lower() == \"one or more\"\n",
    "    )\n",
    "    condition = condition and (doc[i-1].text.lower() != \"the\")\n",
    "    return condition\n",
    "\n",
    "def is_det(token, doc):\n",
    "    \"\"\" Wrapper function for determinant check.\"\"\"\n",
    "    # Add 'said' as custom determination\n",
    "    condition = (token.pos == DET or token.text == \"said\")\n",
    "    # Alternatively we can have the start phrases as above\n",
    "    condition = (condition or check_start_phrase(token, doc))\n",
    "    # Add check for 'a)' and 'a.' - this is not a det\n",
    "    condition = condition and (doc[token.i:token.i+2].text.lower() not in ['a)', 'a.'])\n",
    "    return condition\n",
    "\n",
    "# 6 is a good test claim\n",
    "doc = nlp_docs[4]\n",
    "noun_count = list()\n",
    "\n",
    "# Initialise probabilities\n",
    "for token in doc:\n",
    "    p_all_e_word[token] = dict()\n",
    "\n",
    "# First parse of tokens\n",
    "for token in doc:\n",
    "    p_all_e_word[token] = dict()\n",
    "    if is_det(token, doc):\n",
    "        p_all_e_word[token][0] = 0\n",
    "        # Also set next word as an entity\n",
    "        p_all_e_word[doc[token.i+1]][0] = 0\n",
    "    elif token.pos == NOUN:\n",
    "        p_all_e_word[token][0] = 0\n",
    "        noun_count.append(token)\n",
    "    if token.text in [\":\",\";\",\".\", \",\"]:\n",
    "        p_all_e_word[token][0] = 1\n",
    "\n",
    "print(\"First pass\")\n",
    "for token in doc:\n",
    "    print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n') \n",
    "#print(\"Number of nouns = {0}\".format(len(noun_count)))\n",
    "\n",
    "print(\"Second pass\")\n",
    "# Second parse to fill in probabilities given hard end points\n",
    "last_break = 0\n",
    "for token in doc:\n",
    "    # Look for hard end points\n",
    "    if p_all_e_word[token].get(0, None) == 1:\n",
    "        # Look at previous token\n",
    "        previous_word = doc[token.i - 1]\n",
    "        # If it is set as an entity (i.e. e_0=0)\n",
    "        if p_all_e_word[previous_word].get(0, None) == 0:\n",
    "            print(\"{0} is e_0=0\".format(previous_word))\n",
    "            # Go back to next e_0=0 entry\n",
    "            for j in range(token.i-2, last_break, -1):\n",
    "                if p_all_e_word[doc[j]].get(0, None) == 0:\n",
    "                    # If last e_0=0 entry is a determinant\n",
    "                    print(\"Next - {0} is e_0=0\".format(doc[j]))\n",
    "                    if is_det(doc[j], doc):\n",
    "                        print(\"Found - {0}\".format(doc[j:token.i]))\n",
    "                        # Set in between tokens as e_0=0\n",
    "                        for k in range(j+1, token.i):\n",
    "                            p_all_e_word[doc[k]][0] = 0 \n",
    "                    # Finish when hitting previous e_0 token\n",
    "                    break\n",
    "\n",
    "                    \n",
    "for token in doc:\n",
    "    print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n') \n",
    "#print(\"Number of nouns = {0}\".format(len(noun_count)))\n",
    "\n",
    "print(\"Third pass\") \n",
    "# Third parse - take any DET ... NOUN <boundary> portions\n",
    "last_break = 0\n",
    "spans_to_match = list()\n",
    "for token in doc:\n",
    "    # Look for hard end points\n",
    "    if p_all_e_word[token].get(0, None) == 1:\n",
    "        print(\"{0} is e_0=1 - looking back\".format(token))\n",
    "        # See if there is a continuous set of e_0 = 0 ending with a noun and starting with a DET\n",
    "        for j in range(token.i-1, last_break, -1):\n",
    "            # Look for e_0=0 and noun (do we need to limit to singular noun)\n",
    "            if p_all_e_word[doc[j]].get(0, None) == 0 and doc[j].pos == NOUN:\n",
    "                print(\"Next - {0} is e_0=0 and noun\".format(doc[j]))\n",
    "                # Look back for DET\n",
    "                for k in range(j, last_break, -1):\n",
    "                    if p_all_e_word[doc[k]].get(0, None) != 0:\n",
    "                        # Exit if don't meet a e_0 token\n",
    "                        break\n",
    "                    elif doc[k].pos == DET:\n",
    "                        print(\"Next - {0} is e_0=0 and DET\".format(doc[k]))\n",
    "                        print(\"Last break set to {0}\".format(token.i))\n",
    "                        spans_to_match.append((k,j+1))\n",
    "                        last_break = token.i\n",
    "                        break\n",
    "                break\n",
    "                \n",
    "print(\"\\n--------\\n\")\n",
    "\n",
    "entity_dict = dict()\n",
    "for stm in spans_to_match:\n",
    "    print(\"Looking for matches for '{0}'\".format(doc[stm[0]:stm[1]]))\n",
    "    non_det_string = doc[stm[0]+1:stm[1]].text\n",
    "    if non_det_string not in entity_dict.keys():\n",
    "        entity_dict[non_det_string] = list()\n",
    "    entity_dict[non_det_string].append(doc[stm[0]:stm[1]])\n",
    "    \n",
    "print(\"Unique entities include {0}\".format(list(entity_dict.keys()))) \n",
    "\n",
    "#Add entity values\n",
    "entity_count = 0\n",
    "for entity_string in entity_dict.keys():\n",
    "    entity_occurrences = entity_dict[entity_string]\n",
    "    entity_count += 1\n",
    "    print(entity_dict[entity_string])\n",
    "    for occurrence in entity_occurrences:\n",
    "        for token in occurrence:\n",
    "            p_all_e_word[token][entity_count] = 1\n",
    "        \n",
    "for token in doc:\n",
    "    print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n') \n",
    "\n",
    "# Compare with existing methods \n",
    "print(np_entity_finder(doc))\n",
    "print(simple_spacy_entity_finder(doc))\n",
    "\n",
    "# Here look for matches that are found with both methods that have an [\"a ...\",\"the ...\", ...] pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristics:\n",
    "* \"for\" marks a non-entity [e_0=1]\n",
    "* \"DET X of ...\" [e_0=0]\n",
    "* \"in X with\" [e_0=1]\n",
    "* \"at least one\" / \"one or more\" [e_0=0]\n",
    "* lemma = \\[\"comprise\", \"have\", \"be\", \"include\"\\] [e_0=1]\n",
    "* \"where\" in token.text [e_0=1] (e.g. \"where or wherein\")\n",
    "* \"associated with\" [e_0=1]\n",
    "* \"configured/adapted to\" [e_0=0]\n",
    "\n",
    "Also watch out for \"each of the plurality of X\" or \"at least one of the plurality of X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import NUM\n",
    "\n",
    "\n",
    "\n",
    "# We want to set these if they are not already set\n",
    "def set_probability(token, p_all_e_word, entity, new_value):\n",
    "    \"\"\" Set probability value if not set already\"\"\"\n",
    "    if entity not in p_all_e_word[token].keys():\n",
    "        if sum([v for k, v in p_all_e_word[token]] + new_value) <= 1: \n",
    "            p_all_e_word[token][entity] = new_value\n",
    "    return p_all_e_word\n",
    "            \n",
    "\n",
    "def heuristics(token, doc, p_all_e_word):\n",
    "    \"\"\" Apply heuristics to mark entity probabilities\"\"\"\n",
    "    entity_stop_chars = [\"\\n\",\":\",\";\",\".\", \",\"]\n",
    "    # Set stop characters as non-entity\n",
    "    if token.text in entity_stop_chars:\n",
    "        p_all_e_word[token][0] = 1\n",
    "    \n",
    "    # Set noun as entity\n",
    "    if token.pos == NOUN and p_all_e_word[token].get(0, None) != 1:\n",
    "        p_all_e_word[token][0] = 0\n",
    "    \n",
    "    # 'for' is an entity boundary\n",
    "    if token.lemma_ == \"for\":\n",
    "        p_all_e_word[token][0] = 1\n",
    "    \n",
    "    # \"comprise\", \"have\", \"be\", \"include\" do not relate to an entity\n",
    "    if token.lemma_ in [\"comprise\", \"have\", \"be\", \"include\"]:\n",
    "        p_all_e_word[token][0] = 1\n",
    "    \n",
    "    # \"where\" and \"wherein\" do not relate to an entity\n",
    "    if \"where\" in token.lemma_:\n",
    "         p_all_e_word[token][0] = 1\n",
    "    \n",
    "    # Look ahead - check not at end\n",
    "    if token.i < (len(doc)-1):\n",
    "        \n",
    "        # \"configured/adapted to\" do not relate to an entity\n",
    "        if doc[token.i+1].lemma_ == \"to\" and token.lemma_ in [\"configure\", \"adapt\"]:\n",
    "            p_all_e_word[token][0] = 1\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 1\n",
    "    \n",
    "    if token.i < (len(doc)-2):\n",
    "        # Set DETs as entity\n",
    "        if (\n",
    "            token.pos == DET or token.text == \"said\"\n",
    "        ) and (\n",
    "            doc[token.i:token.i+2].text.lower() not in ['a)', 'a.']\n",
    "        ):\n",
    "            p_all_e_word[token][0] = 0\n",
    "            p_all_e_word[doc[token.i+1]][0] = 0\n",
    "            \n",
    "        # DET X of .. relates to an entity\n",
    "        if token.pos == DET and doc[token.i+2].lemma_ == \"of\":\n",
    "            p_all_e_word[token][0] = 0\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 0\n",
    "            # Set of\n",
    "            p_all_e_word[doc[token.i + 2]][0] = 0\n",
    "            # Set term after off\n",
    "            p_all_e_word[doc[token.i + 3]][0] = 0\n",
    "            \n",
    "        # \"in X with\" does not relate to an entity\n",
    "        if token.lemma_ == \"in\" and doc[token.i+2].lemma_ == \"with\":\n",
    "            p_all_e_word[token][0] = 1\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 1\n",
    "            p_all_e_word[doc[token.i + 2]][0] = 1\n",
    "            \n",
    "        # Associated with does not relate to an entity\n",
    "        if doc[token.i:token.i+2].text.lower() == \"associated with\":\n",
    "            p_all_e_word[token][0] = 1\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 1\n",
    "    \n",
    "    if token.i < (len(doc)-3):\n",
    "        # \"at least NUM\" / \"NUM or more\" relates to an entity\n",
    "        if doc[token.i:token.i + 2].text.lower() == \"at least\" and doc[token.i + 2].pos == NUM:\n",
    "            p_all_e_word[token][0] = 0\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 0\n",
    "            p_all_e_word[doc[token.i + 2]][0] = 0\n",
    "        if doc[token.i+1:token.i + 3].text.lower() == \"or more\" and token.pos == NUM:\n",
    "            p_all_e_word[token][0] = 0\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 0\n",
    "            p_all_e_word[doc[token.i + 2]][0] = 0\n",
    "    \n",
    "    return p_all_e_word\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm generally is:\n",
    "* Mark as entity or not based on rules;\n",
    "* Look back from DET or punct break [':',';',',','.'] - set as non-entity until noun is found;\n",
    "* Look at noun phrase chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp_docs[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pass - entity label heuristics\n",
      "\n",
      " [{0: 1}]\n",
      "1 [{}]\n",
      ". [{0: 1}]\n",
      "A [{0: 0}]\n",
      "method [{0: 0}]\n",
      "for [{0: 1}]\n",
      "translating [{}]\n",
      "instant [{}]\n",
      "messages [{0: 0}]\n",
      "exchanged [{}]\n",
      "between [{}]\n",
      "two [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "devices [{0: 0}]\n",
      "over [{}]\n",
      "a [{0: 0}]\n",
      "network [{0: 0}]\n",
      "by [{}]\n",
      "one [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "users [{0: 0}]\n",
      "that [{}]\n",
      "communicate [{}]\n",
      "in [{}]\n",
      "different [{}]\n",
      "languages [{0: 0}]\n",
      ", [{0: 1}]\n",
      "the [{0: 0}]\n",
      "method [{0: 0}]\n",
      "comprising [{0: 1}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "establishing [{}]\n",
      "a [{0: 0}]\n",
      "user [{0: 0}]\n",
      "profile [{0: 0}]\n",
      "indicating [{}]\n",
      "at [{0: 0}]\n",
      "least [{0: 0}]\n",
      "one [{0: 0}]\n",
      "user [{0: 0}]\n",
      "language [{0: 0}]\n",
      "and [{}]\n",
      "one [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "translation [{0: 0}]\n",
      "preferences [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "one [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "users [{0: 0}]\n",
      "; [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "receiving [{}]\n",
      "a [{0: 0}]\n",
      "message [{0: 0}]\n",
      "as [{}]\n",
      "input [{0: 0}]\n",
      "composed [{}]\n",
      "by [{}]\n",
      "at [{0: 0}]\n",
      "least [{0: 0}]\n",
      "one [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "users [{0: 0}]\n",
      "according [{}]\n",
      "to [{}]\n",
      "the [{0: 0}]\n",
      "user [{0: 0}]\n",
      "language [{0: 0}]\n",
      "; [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "translating [{}]\n",
      "the [{0: 0}]\n",
      "message [{0: 0}]\n",
      "from [{}]\n",
      "the [{0: 0}]\n",
      "user [{0: 0}]\n",
      "language [{0: 0}]\n",
      "to [{}]\n",
      "at [{0: 0}]\n",
      "least [{0: 0}]\n",
      "one [{0: 0}]\n",
      "different [{}]\n",
      "language [{0: 0}]\n",
      "corresponding [{}]\n",
      "to [{}]\n",
      "the [{0: 0}]\n",
      "one [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "translation [{0: 0}]\n",
      "preferences [{0: 0}]\n",
      "; [{0: 1}]\n",
      "and [{}]\n",
      "\n",
      " [{0: 1}]\n",
      "transmitting [{}]\n",
      "the [{0: 0}]\n",
      "message [{0: 0}]\n",
      "in [{}]\n",
      "translated [{}]\n",
      "form [{0: 0}]\n",
      "to [{}]\n",
      "at [{0: 0}]\n",
      "least [{0: 0}]\n",
      "one [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "two [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "devices [{0: 0}]\n",
      ". [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "Second pass - look for DET ... NOUN groupings\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      ". is e_0=1 or DET - looking back\n",
      "Step back token - 1 with pos - 95\n",
      "Setting non-Noun\n",
      "A is e_0=1 or DET - looking back\n",
      "Step back token - . with pos - 95\n",
      "Setting non-Noun\n",
      "Step back token - 1 with pos - 95\n",
      "Setting non-Noun\n",
      "for is e_0=1 or DET - looking back\n",
      "Step back token - method with pos - 90\n",
      "Located plural noun: messages\n",
      "instant 8 {}\n",
      "Setting instant as e_0=0\n",
      "translating 5 {}\n",
      "for 4 {0: 1}\n",
      "Located plural noun: devices\n",
      "more 11 {0: 0}\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - over with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - devices with pos - 90\n",
      "Located plural noun: users\n",
      "more 19 {0: 0}\n",
      "Located plural noun: languages\n",
      "different 27 {}\n",
      "Setting different as e_0=0\n",
      "in 24 {}\n",
      "communicate 22 {}\n",
      "that 24 {}\n",
      "users 18 {0: 0}\n",
      ", is e_0=1 or DET - looking back\n",
      "Step back token - languages with pos - 90\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - , with pos - 95\n",
      "Setting non-Noun\n",
      "comprising is e_0=1 or DET - looking back\n",
      "Step back token - method with pos - 90\n",
      ": is e_0=1 or DET - looking back\n",
      "Step back token - comprising with pos - 98\n",
      "Setting non-Noun\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      "Step back token - : with pos - 95\n",
      "Setting non-Noun\n",
      "Step back token - comprising with pos - 98\n",
      "Setting non-Noun\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - establishing with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - \n",
      " with pos - 101\n",
      "Setting non-Noun\n",
      "Step back token - : with pos - 95\n",
      "Setting non-Noun\n",
      "Step back token - comprising with pos - 98\n",
      "Setting non-Noun\n",
      "Located plural noun: preferences\n",
      "translation 49 {0: 0}\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - of with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - preferences with pos - 90\n",
      "Located plural noun: users\n",
      "more 52 {0: 0}\n",
      "; is e_0=1 or DET - looking back\n",
      "Step back token - users with pos - 90\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - receiving with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - \n",
      " with pos - 101\n",
      "Setting non-Noun\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - of with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - one with pos - 91\n",
      "Setting non-Noun\n",
      "Step back token - least with pos - 84\n",
      "Setting non-Noun\n",
      "Step back token - at with pos - 84\n",
      "Setting non-Noun\n",
      "Step back token - by with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - composed with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - input with pos - 90\n",
      "Located plural noun: users\n",
      "the 70 {0: 0}\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - to with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - according with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - users with pos - 90\n",
      "; is e_0=1 or DET - looking back\n",
      "Step back token - language with pos - 90\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - translating with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - \n",
      " with pos - 101\n",
      "Setting non-Noun\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - from with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - message with pos - 90\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - to with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - corresponding with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - language with pos - 90\n",
      "Located plural noun: preferences\n",
      "translation 98 {0: 0}\n",
      "; is e_0=1 or DET - looking back\n",
      "Step back token - preferences with pos - 90\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      "Step back token - and with pos - 87\n",
      "Setting non-Noun\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - transmitting with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - \n",
      " with pos - 101\n",
      "Setting non-Noun\n",
      "Step back token - and with pos - 87\n",
      "Setting non-Noun\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - of with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - one with pos - 91\n",
      "Setting non-Noun\n",
      "Step back token - least with pos - 84\n",
      "Setting non-Noun\n",
      "Step back token - at with pos - 84\n",
      "Setting non-Noun\n",
      "Step back token - to with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - form with pos - 90\n",
      "Located plural noun: devices\n",
      "more 114 {0: 0}\n",
      ". is e_0=1 or DET - looking back\n",
      "Step back token - devices with pos - 90\n",
      "\n",
      " [{0: 1}]\n",
      "1 [{0: 1}]\n",
      ". [{0: 1}]\n",
      "A [{0: 0}]\n",
      "method [{0: 0}]\n",
      "for [{0: 1}]\n",
      "translating [{}]\n",
      "instant [{0: 0}]\n",
      "messages [{0: 0}]\n",
      "exchanged [{}]\n",
      "between [{}]\n",
      "two [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "devices [{0: 0}]\n",
      "over [{0: 1}]\n",
      "a [{0: 0}]\n",
      "network [{0: 0}]\n",
      "by [{}]\n",
      "one [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "users [{0: 0}]\n",
      "that [{}]\n",
      "communicate [{}]\n",
      "in [{}]\n",
      "different [{0: 0}]\n",
      "languages [{0: 0}]\n",
      ", [{0: 1}]\n",
      "the [{0: 0}]\n",
      "method [{0: 0}]\n",
      "comprising [{0: 1}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "establishing [{0: 1}]\n",
      "a [{0: 0}]\n",
      "user [{0: 0}]\n",
      "profile [{0: 0}]\n",
      "indicating [{}]\n",
      "at [{0: 0}]\n",
      "least [{0: 0}]\n",
      "one [{0: 0}]\n",
      "user [{0: 0}]\n",
      "language [{0: 0}]\n",
      "and [{}]\n",
      "one [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "translation [{0: 0}]\n",
      "preferences [{0: 0}]\n",
      "of [{0: 1}]\n",
      "the [{0: 0}]\n",
      "one [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "users [{0: 0}]\n",
      "; [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "receiving [{0: 1}]\n",
      "a [{0: 0}]\n",
      "message [{0: 0}]\n",
      "as [{}]\n",
      "input [{0: 0}]\n",
      "composed [{0: 1}]\n",
      "by [{0: 1}]\n",
      "at [{0: 1}]\n",
      "least [{0: 1}]\n",
      "one [{0: 1}]\n",
      "of [{0: 1}]\n",
      "the [{0: 0}]\n",
      "users [{0: 0}]\n",
      "according [{0: 1}]\n",
      "to [{0: 1}]\n",
      "the [{0: 0}]\n",
      "user [{0: 0}]\n",
      "language [{0: 0}]\n",
      "; [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "translating [{0: 1}]\n",
      "the [{0: 0}]\n",
      "message [{0: 0}]\n",
      "from [{0: 1}]\n",
      "the [{0: 0}]\n",
      "user [{0: 0}]\n",
      "language [{0: 0}]\n",
      "to [{}]\n",
      "at [{0: 0}]\n",
      "least [{0: 0}]\n",
      "one [{0: 0}]\n",
      "different [{0: 0}]\n",
      "language [{0: 0}]\n",
      "corresponding [{0: 1}]\n",
      "to [{0: 1}]\n",
      "the [{0: 0}]\n",
      "one [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "translation [{0: 0}]\n",
      "preferences [{0: 0}]\n",
      "; [{0: 1}]\n",
      "and [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "transmitting [{0: 1}]\n",
      "the [{0: 0}]\n",
      "message [{0: 0}]\n",
      "in [{}]\n",
      "translated [{}]\n",
      "form [{0: 0}]\n",
      "to [{0: 1}]\n",
      "at [{0: 1}]\n",
      "least [{0: 1}]\n",
      "one [{0: 1}]\n",
      "of [{0: 1}]\n",
      "the [{0: 0}]\n",
      "two [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "devices [{0: 0}]\n",
      ". [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "translating [{}]\n",
      "exchanged [{}]\n",
      "between [{}]\n",
      "by [{}]\n",
      "that [{}]\n",
      "communicate [{}]\n",
      "in [{}]\n",
      "indicating [{}]\n",
      "and [{}]\n",
      "as [{}]\n",
      "to [{}]\n",
      "in [{}]\n",
      "translated [{}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "Extracted possible occurrences:\n",
      "\n",
      "[A method, , instant messages, , two or more devices, a network, one or more users, , , different languages, the method, a user profile, at least one user language, one or more translation preferences, the one or more users, a message, input, the users, the user language, the message, the user language, at least one different language, the one or more translation preferences, the message, , form, the two or more devices, ]\n",
      "\n",
      "1. A method for translating instant messages exchanged between two or more devices over a network by one or more users that communicate in different languages, the method comprising: \n",
      "establishing a user profile indicating at least one user language and one or more translation preferences of the one or more users; \n",
      "receiving a message as input composed by at least one of the users according to the user language; \n",
      "translating the message from the user language to at least one different language corresponding to the one or more translation preferences; and \n",
      "transmitting the message in translated form to at least one of the two or more devices. \n",
      "\n",
      "\n",
      "OrderedDict([('method', [A method, the method]), ('instant messages', [instant messages]), ('two or more devices', [two or more devices, the two or more devices]), ('network', [a network]), ('one or more users', [one or more users, the one or more users]), ('different languages', [different languages]), ('user profile', [a user profile]), ('at least one user language', [at least one user language]), ('one or more translation preferences', [one or more translation preferences, the one or more translation preferences]), ('message', [a message, the message, the message]), ('input', [input]), ('users', [the users]), ('user language', [the user language, the user language]), ('at least one different language', [at least one different language]), ('form', [form])])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "# Try out with adding heuristics\n",
    "\n",
    "# Start with all words relate to no entities\n",
    "p_all_e_word = dict()\n",
    "\n",
    "# Initialise probabilities\n",
    "for token in doc:\n",
    "    p_all_e_word[token] = dict()\n",
    "\n",
    "# Is the order of our labelling important? Probably as we overwrite following probs    \n",
    "    \n",
    "# This can be combined with first pass easily - similar checks\n",
    "print(\"First pass - entity label heuristics\")\n",
    "for token in doc:\n",
    "    p_all_e_word = heuristics(token, doc, p_all_e_word)\n",
    "    print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n')\n",
    "   \n",
    "print(\"Second pass - look for DET ... NOUN groupings\") \n",
    "# Second parse - take any DET ... NOUN <boundary> portions\n",
    "last_break = 0\n",
    "spans_to_match = list()\n",
    "for token in doc:\n",
    "    # Look for hard end points or DET\n",
    "    if (p_all_e_word[token].get(0, None) == 1) or (token.pos == DET):\n",
    "        print(\"{0} is e_0=1 or DET - looking back\".format(token))\n",
    "        # Step back marking as e_0=1 until first NOUN      \n",
    "        for j in range(token.i-1, last_break, -1):\n",
    "            print(\"Step back token - {0} with pos - {1}\".format(doc[j], doc[j].pos))\n",
    "            if doc[j].pos != NOUN:\n",
    "                print(\"Setting non-Noun\")\n",
    "                p_all_e_word[doc[j]][0] = 1\n",
    "            else:\n",
    "                last_break = j\n",
    "                break\n",
    "    # Look at grouping from DET\n",
    "    if is_det(token, doc):\n",
    "        # Tweak for \"at least X\" and \"X or more\"\n",
    "        if (\n",
    "            doc[token.i:token.i + 2].text.lower() == \"at least\" and doc[token.i + 2].pos == NUM\n",
    "        ) or (\n",
    "            doc[token.i+1:token.i + 3].text.lower() == \"or more\" and token.pos == NUM\n",
    "        ):\n",
    "            #print(\"Head index set to {0}\".format())\n",
    "            head_index = doc[token.i+2].head.i\n",
    "        else: \n",
    "            head_index = token.head.i\n",
    "        possible_entity = True\n",
    "        # Step through intermediate tokens between current and head\n",
    "        for j in range(token.i, head_index):\n",
    "            # If head is outside of DET ... end_NOUN sequence\n",
    "            if doc[j].head.i < token.i and doc[j].head.i > head_index:\n",
    "                # Check for nested portions\n",
    "                possible_entity = False\n",
    "        if possible_entity:\n",
    "            for k in range(token.i, head_index + 1):\n",
    "                p_all_e_word[doc[k]][0] = 0 \n",
    "    # Need to adapt the above for at least one ... X and one or more ... Xs - \"at\" > head > \"least\" > \"one\" > X\n",
    "    # Look at plural nouns\n",
    "    if token.tag_ == \"NNS\":\n",
    "        print(\"Located plural noun: {0}\".format(token))\n",
    "        #Step back and mark as e_0=0 any preceding word that has the token as a head\n",
    "        for j in range(token.i-1, 0, -1):\n",
    "            print(doc[j], doc[j].head.i, p_all_e_word[doc[j]])\n",
    "            if p_all_e_word[doc[j]]:\n",
    "                break\n",
    "            elif (\n",
    "                doc[j].head.i == token.i\n",
    "            ):\n",
    "                print(\"Setting {0} as e_0=0\".format(doc[j]))\n",
    "                p_all_e_word[doc[j]][0] = 0\n",
    "    \n",
    "for token in doc:\n",
    "    print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n') \n",
    "    \n",
    "for token in doc:\n",
    "    if not p_all_e_word[token]:\n",
    "        print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n') \n",
    "    \n",
    "print(\"Extracted possible occurrences:\\n\")\n",
    "poss_occ = list()\n",
    "for token in doc[1:]:\n",
    "    # If transition\n",
    "    if p_all_e_word[token].get(0, 0) == 0 and p_all_e_word[doc[token.i-1]].get(0, 1) == 1:\n",
    "        # Add consecutive e_0=0\n",
    "        for j in range(token.i, len(doc)+1):\n",
    "            if p_all_e_word[doc[j]].get(0, 1) != 0:\n",
    "                poss_occ.append(doc[token.i:j])\n",
    "                break\n",
    "\n",
    "print(poss_occ)\n",
    "\n",
    "# Matching occurrences\n",
    "entity_dict = dict()\n",
    "# Now group by unique\n",
    "for entity in poss_occ:\n",
    "    np_start = entity.start\n",
    "    # Ignore the determinant \n",
    "    if doc[np_start].pos == DET:\n",
    "        np_start += 1\n",
    "    # Generate a string representation excluding the determinant\n",
    "    np_string = doc[np_start:entity.end].text.lower()                        \n",
    "    if np_string:\n",
    "        if np_string not in entity_dict.keys():\n",
    "            entity_dict[np_string] = list()          \n",
    "        entity_dict[np_string].append(entity)\n",
    "\n",
    "print(doc)\n",
    "# print(entity_dict)\n",
    "\n",
    "# Quick function to sort entities by occurrence\n",
    "# Need to sort the keys by the index of the first word in the first entry\n",
    "ordered_entities = OrderedDict(sorted(entity_dict.items(), key=lambda t: t[1][0][0].i))\n",
    "\n",
    "print(ordered_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper - http://cogprints.org/5025/1/nrc-48727.pdf - suggests a two-phase process:\n",
    "* Generate a \"gazetteer\" (a list of named entities) - similar to our first stage of simple_entity_extraction method;\n",
    "* Disambiguate names in \"gazetteer\" (this is similar to our second stage of simple_entity_extraction method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "* Need to look for entities with different names to merge based on number agreement and head agreement and presence before use of the in claim.\n",
    "* Also look for unassigned words between det and noun - mark as e_0=1 look for head = noun (two image storage regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for spans between e_0=1 - these must contain an occurrence. If there is only one DET-NOUN (check NP using head) or X NNS (check again using NP head) - that must be an entity. (This is the second parse?)\n",
    "\n",
    "Can we look backwards from DET? Anything that is not a NOUN is e_0=1?\n",
    "\n",
    "Plurals need looking at:\n",
    "```\n",
    "user [{0: 0}]\n",
    "defined [{}]\n",
    "rules [{0: 0}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 \n",
      " SPACE 1 1 SP\n",
      "1 1 1 PUNCT 1 1 LS\n",
      ". 2 . PUNCT 1 1 .\n",
      "A 3 a DET method 4 DT\n",
      "method 4 method NOUN method 4 NN\n",
      "for 5 for ADP method 4 IN\n",
      "translating 6 translate VERB for 5 VBG\n",
      "instant 7 instant ADJ messages 8 JJ\n",
      "messages 8 message NOUN translating 6 NNS\n",
      "exchanged 9 exchange VERB messages 8 VBN\n",
      "between 10 between ADP exchanged 9 IN\n",
      "two 11 two NUM devices 14 CD\n",
      "or 12 or CCONJ two 11 CC\n",
      "more 13 more ADJ two 11 JJR\n",
      "devices 14 device NOUN between 10 NNS\n",
      "over 15 over ADP exchanged 9 IN\n",
      "a 16 a DET network 17 DT\n",
      "network 17 network NOUN over 15 NN\n",
      "by 18 by ADP network 17 IN\n",
      "one 19 one NUM users 22 CD\n",
      "or 20 or CCONJ one 19 CC\n",
      "more 21 more ADJ one 19 JJR\n",
      "users 22 user NOUN by 18 NNS\n",
      "that 23 that ADJ communicate 24 WDT\n",
      "communicate 24 communicate VERB users 22 VBP\n",
      "in 25 in ADP communicate 24 IN\n",
      "different 26 different ADJ languages 27 JJ\n",
      "languages 27 language NOUN in 25 NNS\n",
      ", 28 , PUNCT languages 27 ,\n",
      "the 29 the DET method 30 DT\n",
      "method 30 method NOUN languages 27 NN\n",
      "comprising 31 comprise VERB method 30 VBG\n",
      ": 32 : PUNCT method 4 :\n",
      "\n",
      " 33 \n",
      " SPACE : 32 SP\n",
      "establishing 34 establish VERB method 4 VBG\n",
      "a 35 a DET profile 37 DT\n",
      "user 36 user NOUN profile 37 NN\n",
      "profile 37 profile NOUN establishing 34 NN\n",
      "indicating 38 indicate VERB profile 37 VBG\n",
      "at 39 at ADP least 40 IN\n",
      "least 40 least ADV one 41 RBS\n",
      "one 41 one NUM language 43 CD\n",
      "user 42 user NOUN language 43 NN\n",
      "language 43 language NOUN indicating 38 NN\n",
      "and 44 and CCONJ language 43 CC\n",
      "one 45 one NUM preferences 49 CD\n",
      "or 46 or CCONJ one 45 CC\n",
      "more 47 more ADJ one 45 JJR\n",
      "translation 48 translation NOUN preferences 49 NN\n",
      "preferences 49 preference NOUN language 43 NNS\n",
      "of 50 of ADP language 43 IN\n",
      "the 51 the DET users 55 DT\n",
      "one 52 one NUM users 55 CD\n",
      "or 53 or CCONJ one 52 CC\n",
      "more 54 more ADJ one 52 JJR\n",
      "users 55 user NOUN of 50 NNS\n",
      "; 56 ; PUNCT method 4 :\n",
      "\n",
      " 57 \n",
      " SPACE ; 56 SP\n",
      "receiving 58 receive VERB method 4 VBG\n",
      "a 59 a DET message 60 DT\n",
      "message 60 message NOUN receiving 58 NN\n",
      "as 61 as ADP message 60 IN\n",
      "input 62 input NOUN as 61 NN\n",
      "composed 63 compose VERB input 62 VBN\n",
      "by 64 by ADP composed 63 IN\n",
      "at 65 at ADV least 66 RB\n",
      "least 66 least ADV one 67 RBS\n",
      "one 67 one NUM by 64 CD\n",
      "of 68 of ADP one 67 IN\n",
      "the 69 the DET users 70 DT\n",
      "users 70 user NOUN of 68 NNS\n",
      "according 71 accord VERB users 70 VBG\n",
      "to 72 to ADP according 71 IN\n",
      "the 73 the DET language 75 DT\n",
      "user 74 user NOUN language 75 NN\n",
      "language 75 language NOUN to 72 NN\n",
      "; 76 ; PUNCT method 4 :\n",
      "\n",
      " 77 \n",
      " SPACE ; 76 SP\n",
      "translating 78 translate VERB method 4 VBG\n",
      "the 79 the DET message 80 DT\n",
      "message 80 message NOUN translating 78 NN\n",
      "from 81 from ADP message 80 IN\n",
      "the 82 the DET language 84 DT\n",
      "user 83 user NOUN language 84 NN\n",
      "language 84 language NOUN from 81 NN\n",
      "to 85 to PART translating 78 TO\n",
      "at 86 at ADV least 87 RB\n",
      "least 87 least ADV one 88 RBS\n",
      "one 88 one NUM language 90 CD\n",
      "different 89 different ADJ language 90 JJ\n",
      "language 90 language NOUN to 85 NN\n",
      "corresponding 91 correspond VERB language 90 VBG\n",
      "to 92 to ADP corresponding 91 IN\n",
      "the 93 the DET preferences 98 DT\n",
      "one 94 one NUM preferences 98 CD\n",
      "or 95 or CCONJ one 94 CC\n",
      "more 96 more ADJ one 94 JJR\n",
      "translation 97 translation NOUN preferences 98 NN\n",
      "preferences 98 preference NOUN to 92 NNS\n",
      "; 99 ; PUNCT language 90 :\n",
      "and 100 and CCONJ language 90 CC\n",
      "\n",
      " 101 \n",
      " SPACE and 100 SP\n",
      "transmitting 102 transmit VERB language 90 VBG\n",
      "the 103 the DET message 104 DT\n",
      "message 104 message NOUN transmitting 102 NN\n",
      "in 105 in ADP message 104 IN\n",
      "translated 106 translate VERB message 104 VBN\n",
      "form 107 form NOUN translated 106 NN\n",
      "to 108 to ADP translated 106 IN\n",
      "at 109 at ADV least 110 RB\n",
      "least 110 least ADV one 111 RBS\n",
      "one 111 one NUM to 108 CD\n",
      "of 112 of ADP one 111 IN\n",
      "the 113 the DET devices 117 DT\n",
      "two 114 two NUM devices 117 CD\n",
      "or 115 or CCONJ two 114 CC\n",
      "more 116 more ADJ two 114 JJR\n",
      "devices 117 device NOUN of 112 NNS\n",
      ". 118 . PUNCT method 4 .\n",
      "\n",
      "\n",
      " 119 \n",
      "\n",
      " SPACE . 118 SP\n"
     ]
    }
   ],
   "source": [
    "# Look at POS and head for each token\n",
    "for token in doc:\n",
    "    print(token.text, token.i, token.lemma_, token.pos_, token.head.text, token.head.i, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When matching what to do with 'the time scale' and 'the time scale display information' or 'a project' and:\n",
    "```\n",
    "A [{0: 0}]\n",
    "project [{}]\n",
    "information [{0: 0}]\n",
    "display [{0: 0}]\n",
    "device [{0: 0}]\n",
    ", [{0: 1}]\n",
    "comprising [{}]\n",
    ": [{0: 1}]\n",
    "```\n",
    "Only look for e_0 stretches of same number with matching pos and text? (Are we now getting to look at transitions?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are issues with \"(a)\" and \"a.\"\n",
    "\n",
    "Also \"response\" from \"in response\".\n",
    "\n",
    "Check det is not working for \"at least one\"\n",
    "\n",
    "We can iterate back from where e_0 = 1 - tokens between a last noun and determinant will be part of an entity. We can then match those across the claim. This is the simple entity finder but stepping back at [:;,.] as well as DET.  \n",
    "Pattern is:\n",
    "* If next step back is e_0=0;\n",
    "* If next e_0=0 is a check_det=True;\n",
    "* Fill in inbetween as e_0=0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another pattern is \"DET X FOR [phrase]\" - this is one entity? But contains references to other entities\n",
    "```\n",
    "a [{0: 0}]\n",
    "system [{0: 0}]\n",
    "for [{}]\n",
    "providing [{}]\n",
    "a [{0: 0}]\n",
    "plurality [{0: 0}]\n",
    "of [{}]\n",
    "football [{0: 0}]\n",
    "player [{0: 0}]\n",
    "types [{0: 0}]\n",
    "from [{}]\n",
    "which [{}]\n",
    "a [{0: 0}]\n",
    "football [{0: 0}]\n",
    "player [{0: 0}]\n",
    "type [{0: 0}]\n",
    "is [{}]\n",
    "selected \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a for clause as the whole entity string - e.g. \"A method for modeling electrical characteristics of cells having given circuit elements\" and \"a layout of cells having at least one cell\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Claims - Extracting Steps\n",
    "\n",
    "Let's try something similar for method steps. This may give us some clues for \"comprising\" X, Y, Z structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive algorithm:\n",
    "* Look for a comprising relating to a method.\n",
    "* Look for VERBs following at least one of ['\\n', ';', ','] after the comprising colon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Extracted Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May not be able to use dependency information from spaCy for looking at structure - link between \"comprising\" and subsequent features appear lost over long claim text.  \n",
    "\n",
    "Head of verb will give you the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity method has relationship comprising\n"
     ]
    }
   ],
   "source": [
    "# Look for [\"comprise\", \"have\", \"be\", \"include\"]\n",
    "for token in doc:\n",
    "    if token.lemma_ in [\"comprise\", \"have\", \"include\"]:\n",
    "        print(\"Entity {0} has relationship {1}\".format(token.head.text, token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
