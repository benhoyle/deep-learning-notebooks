{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Encoding Using LSTMs\n",
    "\n",
    "Here we will see if we can generate a word encoding using LSTMs.\n",
    "\n",
    "The idea is that we take tokenised words of the form: ```<W>the</W>``` and feed into an auto-encoder at a character-level. We then take the output of the trained encoder as a word encoder to output a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll start with some claim 1 data\n",
    "\n",
    "# Load data\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n1. A detector for atrial fibrillation or flutter (AF) comprising: \\nan impedance measuring unit comprising a measuring input, to which an atrial electrode line having an electrode for a unipolar measurement of an impedance in an atrium is connected and is implemented to generate an atrial impedance signal, obtained in a unipolar manner, in such a way that an impedance signal for each atrial cycle, comprising an atrial contraction and a following relaxation of said atrium, comprises multiple impedance values detected at different instants within a particular atrial cycle; \\nsaid impedance measuring unit comprising a signal input, via which a ventricle signal is to be supplied to said detector, which reflects instants of ventricular contractions in chronological assignment to said impedance signal; \\nan analysis unit configured to average multiple sequential impedance signal sections of a unipolar atrial impedance signal, which are each delimited by two sequential ventricular contractions, with one another, and to determine a maximum amplitude of an averaged unipolar atrial impedance signal section, \\nsaid analysis unit configured to compare said maximum amplitude to a comparison value, and if said maximum amplitude of said averaged unipolar atrial impedance signal is less than said comparison value, generate an AF suspicion signal. \\n\\n',\n",
       " 'A')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"raw_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll use spacy for tokenisation etc\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Convert each claim into a spacy 'doc' object\n",
    "spacy_data = [nlp(d[0]) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First let's look at some statistics regarding our words\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "word_lengths = list()\n",
    "for doc in spacy_data:\n",
    "    word_lengths = word_lengths + [len(word) for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAD8CAYAAAChHgmuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc039110eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 20, 20)\n",
    "plt.hist(word_lengths, bins)\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: \n",
    "\n",
    "We can probably truncate / split out words at length 15 or 20 and only affect a few words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We've built a character encoding tool in patentdata we can use to help us\n",
    "from patentdata.models.chardict import CharDict\n",
    "cd = CharDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1685371 words\n",
      "Here is a sample: [generate, an, atrial, impedance, signal, ,, obtained, in, a, unipolar]\n"
     ]
    }
   ],
   "source": [
    "# Let's test our character converter\n",
    "print(\"Here's 'test' in integers - {0}\".format(cd.text2int(\"test\")))\n",
    "\n",
    "# Let's test our character converter\n",
    "print(\"Here's 'Test' in integers - {0}\".format(cd.text2int(\"Test\")))\n",
    "\n",
    "# Let's generate a data set consisting of words in our claim\n",
    "words = [word for doc in spacy_data for word in doc]\n",
    "\n",
    "print(\"There are {0} words\".format(len(words)))\n",
    "print(\"Here is a sample: {0}\".format(words[50:60]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each word is a token so we need to remember to use the .text property\n",
    "type(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79, 71, 80]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[cd.startwordint] + cd.text2int(words[0].text) + [cd.endwordint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to convert each of our words to character indices and add <W> and </W> indices to the beginning and end\n",
    "int_words = [[cd.startwordint] + cd.text2int(word.text) + [cd.endwordint] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's save our int_words for future use\n",
    "with open(\"int_words.pkl\", \"wb\") as f:\n",
    "    pickle.dump(int_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our longest sequence is 124 integers long\n"
     ]
    }
   ],
   "source": [
    "# What is the maximum length of our int_words?\n",
    "max_len = max([len(iw) for iw in int_words])\n",
    "print(\"Our longest sequence is {0} integers long\".format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build our own truncation routine - we want to split sequences over 20 into several sequences\n",
    "def limit_length(sequences, limit):\n",
    "    \"\"\" Limits sequences to limit by splitting sequences over limit into sub-sequences.\"\"\"\n",
    "    new_seqs = list()\n",
    "    for seq in sequences:\n",
    "        if len(seq) > limit:\n",
    "            cut_seq = seq\n",
    "            while cut_seq:\n",
    "                new_seqs.append(cut_seq[:limit])\n",
    "                cut_seq = cut_seq[limit:]\n",
    "        else:\n",
    "            new_seqs.append(seq)\n",
    "    return new_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_words_limited = limit_length(int_words, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our longest sequence is 20 integers long\n"
     ]
    }
   ],
   "source": [
    "# What is the maximum length of our int_words?\n",
    "max_len_limited = max([len(iw) for iw in int_words_limited])\n",
    "print(\"Our longest sequence is {0} integers long\".format(max_len_limited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79, 50, 80]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_words_limited[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find out what our maximum integer index is\n",
    "max_index = max([max(iw) for iw in int_words_limited])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum integer value (i.e. char index) is: 82\n"
     ]
    }
   ],
   "source": [
    "print(\"The maximum integer value (i.e. char index) is: {0}\".format(max_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this post to help remind us of the next bit: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "X = sequence.pad_sequences(int_words_limited,  padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([79, 71, 80,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1687527 word samples\n"
     ]
    }
   ],
   "source": [
    "print(\"We have {0} word samples\".format(len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_limited = X[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our data has a shape of (20000, 20, 83)\n"
     ]
    }
   ],
   "source": [
    "X_l_one_hot = np.eye(cd.vocabulary_size)[X_limited]\n",
    "print(\"Our data has a shape of {0}\".format(X_l_one_hot.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_l_one_hot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was not possible because our X is too big!\n",
    "```\n",
    "X_one_hot = np.eye(max_index+1)[X]\n",
    "```\n",
    "However, I don't think we need to convert X into a one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear some memory\n",
    "del spacy_data, int_words, int_words_limited, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just have a think about dimensionality.  \n",
    "\n",
    "Our RNN output at each time stamp will be a vector of length \"num_of_characters\", i.e. 82 above.  \n",
    "\n",
    "Our hidden dimension we can set at 300. (We can experiment with changing this.)  \n",
    "\n",
    "Our sequence length is fixed to 20. (Again we can experiment with this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay - now let's have a think about our data for the model.  \n",
    "\n",
    "We have 1687527 \"word\" sequences. \n",
    "\n",
    "Isn't our input and output data the same?\n",
    "\n",
    "This is what I need to read: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html.\n",
    "\n",
    "Our input data is a a set of sequences where each entry in the sequence is a one-hot array based on the number of characters we are using (83).\n",
    "\n",
    "The character vocabulary size is fixed for the encoder and decoder. It is independent of the text. \n",
    "\n",
    "What are our decoder inputs and outputs?\n",
    "* decoder input = encoder input (but minus ```</W>```)\n",
    "* decoder output = decoder input shifted 1 later in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some hyperparameters - start with those used in this example -\n",
    "# https://github.com/fchollet/keras/blob/master/examples/lstm_seq2seq.py\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 20000  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of encoder tokens = number of different characters in our CharDict\n",
    "num_encoder_tokens = cd.vocabulary_size #= 83\n",
    "# Our decoder character space equals are encoder character space\n",
    "num_decoder_tokens = num_encoder_tokens\n",
    "\n",
    "# Our words are all set to the same max length (20)\n",
    "max_encoder_seq_length = 20\n",
    "max_decoder_seq_length = max_encoder_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (num_samples, max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (num_samples, max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (num_samples, max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, num_samples):\n",
    "    for t in range(0, max_encoder_seq_length):\n",
    "        encoder_input_data[i, t, X[i][t]] = 1\n",
    "        if t < (max_encoder_seq_length - 1):\n",
    "            # decoder input = encoder input for this autoencoder case but skipping last char\n",
    "            decoder_input_data[i, t, X[i][t]] = 1\n",
    "        if t > 0:\n",
    "            # Shift decoder target get so it is one ahead\n",
    "            decoder_target_data[i, t-1, X[i][t]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_encoder_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c0050bb84a8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define an input sequence and process it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_encoder_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_encoder_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/100\n",
      "16000/16000 [==============================] - 369s - loss: 0.8493 - val_loss: 0.6246\n",
      "Epoch 2/100\n",
      "16000/16000 [==============================] - 388s - loss: 0.5625 - val_loss: 0.5541\n",
      "Epoch 3/100\n",
      "16000/16000 [==============================] - 396s - loss: 0.4277 - val_loss: 0.3966\n",
      "Epoch 4/100\n",
      "16000/16000 [==============================] - 382s - loss: 0.3247 - val_loss: 0.3204\n",
      "Epoch 5/100\n",
      "16000/16000 [==============================] - 397s - loss: 0.2482 - val_loss: 0.2423\n",
      "Epoch 6/100\n",
      "16000/16000 [==============================] - 372s - loss: 0.1890 - val_loss: 0.2018\n",
      "Epoch 7/100\n",
      "16000/16000 [==============================] - 369s - loss: 0.1428 - val_loss: 0.1765\n",
      "Epoch 8/100\n",
      "16000/16000 [==============================] - 377s - loss: 0.1097 - val_loss: 0.1414\n",
      "Epoch 9/100\n",
      "16000/16000 [==============================] - 470s - loss: 0.0843 - val_loss: 0.1245\n",
      "Epoch 10/100\n",
      "16000/16000 [==============================] - 421s - loss: 0.0659 - val_loss: 0.1138\n",
      "Epoch 11/100\n",
      "16000/16000 [==============================] - 439s - loss: 0.0530 - val_loss: 0.0992\n",
      "Epoch 12/100\n",
      "16000/16000 [==============================] - 378s - loss: 0.0424 - val_loss: 0.0984\n",
      "Epoch 13/100\n",
      "16000/16000 [==============================] - 381s - loss: 0.0353 - val_loss: 0.0974\n",
      "Epoch 14/100\n",
      "16000/16000 [==============================] - 355s - loss: 0.0293 - val_loss: 0.0918\n",
      "Epoch 15/100\n",
      "16000/16000 [==============================] - 356s - loss: 0.0246 - val_loss: 0.0890\n",
      "Epoch 16/100\n",
      "16000/16000 [==============================] - 354s - loss: 0.0214 - val_loss: 0.0875\n",
      "Epoch 17/100\n",
      "16000/16000 [==============================] - 354s - loss: 0.0181 - val_loss: 0.0945\n",
      "Epoch 18/100\n",
      "16000/16000 [==============================] - 354s - loss: 0.0159 - val_loss: 0.0866\n",
      "Epoch 19/100\n",
      "16000/16000 [==============================] - 353s - loss: 0.0139 - val_loss: 0.0894\n",
      "Epoch 20/100\n",
      "16000/16000 [==============================] - 355s - loss: 0.0122 - val_loss: 0.0884\n",
      "Epoch 21/100\n",
      "16000/16000 [==============================] - 355s - loss: 0.0108 - val_loss: 0.0910\n",
      "Epoch 22/100\n",
      "16000/16000 [==============================] - 353s - loss: 0.0099 - val_loss: 0.0910\n",
      "Epoch 23/100\n",
      "16000/16000 [==============================] - 354s - loss: 0.0088 - val_loss: 0.0925\n",
      "Epoch 24/100\n",
      "16000/16000 [==============================] - 353s - loss: 0.0079 - val_loss: 0.0864\n",
      "Epoch 25/100\n",
      "16000/16000 [==============================] - 355s - loss: 0.0076 - val_loss: 0.0875\n",
      "Epoch 26/100\n",
      "16000/16000 [==============================] - 354s - loss: 0.0071 - val_loss: 0.0895\n",
      "Epoch 27/100\n",
      "16000/16000 [==============================] - 353s - loss: 0.0063 - val_loss: 0.0914\n",
      "Epoch 28/100\n",
      "16000/16000 [==============================] - 354s - loss: 0.0064 - val_loss: 0.0859\n",
      "Epoch 29/100\n",
      "16000/16000 [==============================] - 353s - loss: 0.0054 - val_loss: 0.0907\n",
      "Epoch 30/100\n",
      "16000/16000 [==============================] - 355s - loss: 0.0056 - val_loss: 0.0906\n",
      "Epoch 31/100\n",
      "16000/16000 [==============================] - 365s - loss: 0.0051 - val_loss: 0.0865\n",
      "Epoch 32/100\n",
      "16000/16000 [==============================] - 363s - loss: 0.0047 - val_loss: 0.0877\n",
      "Epoch 33/100\n",
      "16000/16000 [==============================] - 360s - loss: 0.0045 - val_loss: 0.0865\n",
      "Epoch 34/100\n",
      "16000/16000 [==============================] - 378s - loss: 0.0043 - val_loss: 0.0879\n",
      "Epoch 35/100\n",
      "16000/16000 [==============================] - 357s - loss: 0.0042 - val_loss: 0.1008\n",
      "Epoch 36/100\n",
      "16000/16000 [==============================] - 356s - loss: 0.0035 - val_loss: 0.0905\n",
      "Epoch 37/100\n",
      "16000/16000 [==============================] - 354s - loss: 0.0037 - val_loss: 0.0928\n",
      "Epoch 38/100\n",
      "16000/16000 [==============================] - 353s - loss: 0.0036 - val_loss: 0.0905\n",
      "Epoch 39/100\n",
      "16000/16000 [==============================] - 354s - loss: 0.0033 - val_loss: 0.0874\n",
      "Epoch 40/100\n",
      "16000/16000 [==============================] - 356s - loss: 0.0034 - val_loss: 0.0897\n",
      "Epoch 41/100\n",
      "16000/16000 [==============================] - 354s - loss: 0.0030 - val_loss: 0.0961\n",
      "Epoch 42/100\n",
      "16000/16000 [==============================] - 354s - loss: 0.0025 - val_loss: 0.0923\n",
      "Epoch 43/100\n",
      "16000/16000 [==============================] - 354s - loss: 0.0031 - val_loss: 0.0914\n",
      "Epoch 44/100\n",
      "16000/16000 [==============================] - 356s - loss: 0.0027 - val_loss: 0.0919\n",
      "Epoch 45/100\n",
      "16000/16000 [==============================] - 386s - loss: 0.0027 - val_loss: 0.0896\n",
      "Epoch 46/100\n",
      "16000/16000 [==============================] - 454s - loss: 0.0024 - val_loss: 0.0952\n",
      "Epoch 47/100\n",
      "16000/16000 [==============================] - 378s - loss: 0.0024 - val_loss: 0.0905\n",
      "Epoch 48/100\n",
      "16000/16000 [==============================] - 375s - loss: 0.0022 - val_loss: 0.0886\n",
      "Epoch 49/100\n",
      "16000/16000 [==============================] - 376s - loss: 0.0022 - val_loss: 0.0938\n",
      "Epoch 50/100\n",
      "16000/16000 [==============================] - 377s - loss: 0.0020 - val_loss: 0.0917\n",
      "Epoch 51/100\n",
      "16000/16000 [==============================] - 379s - loss: 0.0022 - val_loss: 0.0891\n",
      "Epoch 52/100\n",
      "16000/16000 [==============================] - 377s - loss: 0.0023 - val_loss: 0.0899\n",
      "Epoch 53/100\n",
      "16000/16000 [==============================] - 377s - loss: 0.0019 - val_loss: 0.0878\n",
      "Epoch 54/100\n",
      "16000/16000 [==============================] - 377s - loss: 0.0021 - val_loss: 0.0962\n",
      "Epoch 55/100\n",
      "16000/16000 [==============================] - 376s - loss: 0.0017 - val_loss: 0.0925\n",
      "Epoch 56/100\n",
      "16000/16000 [==============================] - 372s - loss: 0.0018 - val_loss: 0.0941\n",
      "Epoch 57/100\n",
      "16000/16000 [==============================] - 372s - loss: 0.0015 - val_loss: 0.0872\n",
      "Epoch 58/100\n",
      "16000/16000 [==============================] - 372s - loss: 0.0015 - val_loss: 0.0928\n",
      "Epoch 59/100\n",
      "16000/16000 [==============================] - 373s - loss: 0.0015 - val_loss: 0.0978\n",
      "Epoch 60/100\n",
      "16000/16000 [==============================] - 372s - loss: 0.0021 - val_loss: 0.0918\n",
      "Epoch 61/100\n",
      "16000/16000 [==============================] - 372s - loss: 0.0014 - val_loss: 0.0890\n",
      "Epoch 62/100\n",
      "16000/16000 [==============================] - 371s - loss: 0.0017 - val_loss: 0.0925\n",
      "Epoch 63/100\n",
      "16000/16000 [==============================] - 372s - loss: 0.0018 - val_loss: 0.0993\n",
      "Epoch 64/100\n",
      "16000/16000 [==============================] - 383s - loss: 0.0015 - val_loss: 0.0883\n",
      "Epoch 65/100\n",
      "16000/16000 [==============================] - 372s - loss: 0.0011 - val_loss: 0.0882\n",
      "Epoch 66/100\n",
      "16000/16000 [==============================] - 369s - loss: 0.0017 - val_loss: 0.0950\n",
      "Epoch 67/100\n",
      "16000/16000 [==============================] - 369s - loss: 0.0016 - val_loss: 0.0925\n",
      "Epoch 68/100\n",
      "16000/16000 [==============================] - 370s - loss: 0.0012 - val_loss: 0.0901\n",
      "Epoch 69/100\n",
      "16000/16000 [==============================] - 370s - loss: 0.0014 - val_loss: 0.0985\n",
      "Epoch 70/100\n",
      "16000/16000 [==============================] - 370s - loss: 0.0015 - val_loss: 0.0935\n",
      "Epoch 71/100\n",
      "16000/16000 [==============================] - 369s - loss: 0.0016 - val_loss: 0.0905\n",
      "Epoch 72/100\n",
      "16000/16000 [==============================] - 370s - loss: 0.0012 - val_loss: 0.0949\n",
      "Epoch 73/100\n",
      "16000/16000 [==============================] - 370s - loss: 0.0013 - val_loss: 0.0879\n",
      "Epoch 74/100\n",
      "16000/16000 [==============================] - 371s - loss: 0.0012 - val_loss: 0.0992\n",
      "Epoch 75/100\n",
      "16000/16000 [==============================] - 371s - loss: 9.6481e-04 - val_loss: 0.0952\n",
      "Epoch 76/100\n",
      "16000/16000 [==============================] - 370s - loss: 8.9855e-04 - val_loss: 0.0966\n",
      "Epoch 77/100\n",
      "16000/16000 [==============================] - 370s - loss: 0.0012 - val_loss: 0.1048\n",
      "Epoch 78/100\n",
      "16000/16000 [==============================] - 371s - loss: 0.0012 - val_loss: 0.0916\n",
      "Epoch 79/100\n",
      "16000/16000 [==============================] - 370s - loss: 0.0010 - val_loss: 0.0953\n",
      "Epoch 80/100\n",
      "16000/16000 [==============================] - 369s - loss: 9.8818e-04 - val_loss: 0.0927\n",
      "Epoch 81/100\n",
      "16000/16000 [==============================] - 371s - loss: 0.0014 - val_loss: 0.0900\n",
      "Epoch 82/100\n",
      "16000/16000 [==============================] - 370s - loss: 8.3965e-04 - val_loss: 0.0941\n",
      "Epoch 83/100\n",
      "16000/16000 [==============================] - 365s - loss: 0.0012 - val_loss: 0.0895\n",
      "Epoch 84/100\n",
      "16000/16000 [==============================] - 364s - loss: 9.3003e-04 - val_loss: 0.0962\n",
      "Epoch 85/100\n",
      "16000/16000 [==============================] - 364s - loss: 0.0010 - val_loss: 0.0906\n",
      "Epoch 86/100\n",
      "16000/16000 [==============================] - 364s - loss: 0.0012 - val_loss: 0.0897\n",
      "Epoch 87/100\n",
      "16000/16000 [==============================] - 365s - loss: 0.0011 - val_loss: 0.0933\n",
      "Epoch 88/100\n",
      "16000/16000 [==============================] - 365s - loss: 0.0022 - val_loss: 0.0887\n",
      "Epoch 89/100\n",
      "16000/16000 [==============================] - 410s - loss: 8.5796e-04 - val_loss: 0.0923\n",
      "Epoch 90/100\n",
      "16000/16000 [==============================] - 364s - loss: 7.1219e-04 - val_loss: 0.0920\n",
      "Epoch 91/100\n",
      "16000/16000 [==============================] - 365s - loss: 8.9989e-04 - val_loss: 0.0981\n",
      "Epoch 92/100\n",
      "16000/16000 [==============================] - 365s - loss: 0.0012 - val_loss: 0.0964\n",
      "Epoch 93/100\n",
      "16000/16000 [==============================] - 364s - loss: 8.0141e-04 - val_loss: 0.0938\n",
      "Epoch 94/100\n",
      "16000/16000 [==============================] - 364s - loss: 0.0011 - val_loss: 0.0970\n",
      "Epoch 95/100\n",
      "16000/16000 [==============================] - 364s - loss: 7.7962e-04 - val_loss: 0.0976\n",
      "Epoch 96/100\n",
      "16000/16000 [==============================] - 365s - loss: 9.3268e-04 - val_loss: 0.0992\n",
      "Epoch 97/100\n",
      "16000/16000 [==============================] - 364s - loss: 9.2464e-04 - val_loss: 0.0980\n",
      "Epoch 98/100\n",
      "16000/16000 [==============================] - 364s - loss: 7.5714e-04 - val_loss: 0.0971\n",
      "Epoch 99/100\n",
      "16000/16000 [==============================] - 363s - loss: 8.9627e-04 - val_loss: 0.1001\n",
      "Epoch 100/100\n",
      "16000/16000 [==============================] - 363s - loss: 8.6330e-04 - val_loss: 0.0977\n"
     ]
    }
   ],
   "source": [
    "if os.pat\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can actually test the model on words in X that are beyond index 20,000 (we had to limit for training to avoid memory errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can tweak this function to use our character dictionary\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, cd.startwordint] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = cd.int2char(sampled_token_index)\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_token_index == cd.endwordint or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extensions\n",
    "* Bi-directional LSTM for encoding and decoding (have an additional layer to resolve differences on decoder output).\n",
    "* CNN on input as additional vector portion to capture structure that is spatially invariant across the 1D character stream.\n",
    "* Use character embeddings rather than one hot vectors.\n",
    "* See if any faster without loss of accuracy with GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I have a trained encoder and decoder I can use these to generate a word vector and to decode that word vector.  \n",
    "\n",
    "I can then start looking at sequences of word vectors, e.g. sentences.  \n",
    "\n",
    "Some thoughts:\n",
    "* From spacy I also have lots of other cool labels, such as POS and dependency tree information. I want to jointly optimise to predict not only the next word vector in a sequence but the next POS etc label.\n",
    "* I can then generate sentence embeddings with shorter sequences, i.e. I can repeat the mthods above to generate a claim encoding. I can then try this on my classification model and my claim-to-title models.\n",
    "* On the claim-to-title models I can predict sequences of word vectors and then decode these to provide an output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
