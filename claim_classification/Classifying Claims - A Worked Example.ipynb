{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Claims - A Worked Example\n",
    "\n",
    "In this post we will look at manually converting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using USPTO data, where the claims are classified according to the International Patent Classification (IPC). To keep things simple we will use the first letter of the IPC (top level category). This is the same as the top level of the Cooperative Patent Classification (CPC).  \n",
    "\n",
    "The list of top level categories can be found here: https://rs.espacenet.com/help?locale=en_EP&method=handleHelpTopic&topic=ipc:\n",
    "* A Human Necessities\n",
    "* B Performing Operations; Transporting\n",
    "* C Chemistry; Metallurgy\n",
    "* D Textiles; Paper\n",
    "* E Fixed Constructions\n",
    "* F Mechanical Engineering; Lighting; Heating; Weapons; Blasting Engines or Pumps\n",
    "* G Physics\n",
    "* H Electricity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Getting Our Data\n",
    "\n",
    "Most of the task revolves around getting our data and placing it in a form where we can apply common machine learning libraries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start with getting 12,000 patent publications at random\n",
    "from patentdata.corpus import USPublications\n",
    "from patentdata.models.patentcorpus import LazyPatentCorpus\n",
    "import os, pickle\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some function that provide a wrapper around USPTO patent publication data as obtained from the bulk data download page (https://www.uspto.gov/learning-and-resources/bulk-data-products). This code can be found on GitHub - https://github.com/benhoyle/patentdata.\n",
    "\n",
    "The code below gets 12,000 random patent publications from data for years 2001 to 2017 across all classifications. From each document the text for claim 1 and the classifications are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "12000 claims and classifications loaded\n"
     ]
    }
   ],
   "source": [
    "# Get the claim 1 and classificationt text\n",
    "\n",
    "PIK = \"claim_and_class.data\"\n",
    "\n",
    "if os.path.isfile(PIK):\n",
    "    with open(PIK, \"rb\") as f:\n",
    "        print(\"Loading data\")\n",
    "        data = pickle.load(f)\n",
    "        print(\"{0} claims and classifications loaded\".format(len(data)))\n",
    "else:\n",
    "    # Load our list of records\n",
    "    PIK = \"12000records.data\"\n",
    "\n",
    "    if os.path.isfile(PIK):\n",
    "        with open(PIK, \"rb\") as f:\n",
    "            print(\"Loading data\")\n",
    "            records = pickle.load(f)\n",
    "            print(\"{0} records loaded\".format(len(records)))\n",
    "    else:\n",
    "        path = '/media/SAMSUNG1/Patent_Downloads'\n",
    "        ds = USPublications(path)\n",
    "        records = ds.get_records([], \"name\", sample_size=12000)\n",
    "        with open(PIK, \"wb\") as f:\n",
    "            pickle.dump(records, f)\n",
    "            print(\"{0} records saved\".format(len(records)))\n",
    "    \n",
    "    lzy = LazyPatentCorpus()\n",
    "    lzy.init_by_filenames(ds, records)\n",
    "    \n",
    "    data = list()\n",
    "    for i, pd in enumerate(lzy.documents):\n",
    "        try:\n",
    "            classifications = [c.as_string() for c in pd.classifications]\n",
    "        except:\n",
    "            classifications = \"\"\n",
    "        try:\n",
    "            claim1_text = pd.claimset.get_claim(1).text\n",
    "        except:\n",
    "            claim1_text = \"\"\n",
    "        current_data = (claim1_text, classifications)\n",
    "        data.append(current_data)\n",
    "        if (i % 500) == 0:\n",
    "            print(\"Saving a checkpoint at {0} files\".format(i))\n",
    "            print(\"Current data = \", current_data)\n",
    "            with open(PIK, \"wb\") as f:\n",
    "                pickle.dump(data, f)\n",
    "            \n",
    "    with open(PIK, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "    print(\"{0} claims saved\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then get rid of claims which are cancelled (these just have \"(canceled)\" as text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 11239 claims after filtering out cancelled claims\n"
     ]
    }
   ],
   "source": [
    "# Check for and remove 'cancelled' claims\n",
    "no_cancelled = [d for d in data if '(canceled)' not in d[0]]\n",
    "\n",
    "print(\"There are now {0} claims after filtering out cancelled claims\".format(len(no_cancelled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we filter the data to extract the first level classification. For simplicity we take the first classification (if there are several classifications) where the data exists. An example data entry is set out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 11238 claims after extracting classifications\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('\\n1. A sensing assembly for sensing a level of liquid in a reservoir, said sensing assembly comprising: \\na first input port for receiving a first input voltage signal; \\na second input port for receiving a second input voltage signal; \\nan excitation circuit electrically connected to said first and second input ports for receiving the first and second input voltage signals and for generating a first excitation signal and a second excitation signal, said excitation circuit includes first and second excitation electrodes extending along a portion of the reservoir, said first and second excitation electrodes disposed adjacent to and separated by said first receiving electrode; and \\na receiving circuit disposed adjacent said excitation circuit defining a variable capacitance with said excitation circuit, wherein said receiving circuit includes first and second receiving electrodes extending along a portion of the reservoir and a first trace connected to ground and extending between said first receiving electrode and said first and second excitation electrodes, wherein said first receiving electrode extends along a first non-linear path and said second receiving electrode extends along a second non-linear path differing from said first non-linear path, said receiving circuit producing an output voltage signal variable with the level of liquid in the reservoir due to capacitance changes between said excitation circuit and said receiving circuit due to dielectric changes created by the liquid. \\n\\n',\n",
       " 'G')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get classification in the form of A-H\n",
    "cleaner_data = list()\n",
    "for d in no_cancelled:\n",
    "    if len(d[1]) >= 1:\n",
    "        if len(d[1][0]) > 3:\n",
    "            classification = d[1][0][2]\n",
    "            cleaner_data.append(\n",
    "                (d[0], classification)\n",
    "            )\n",
    "\n",
    "print(\"There are now {0} claims after extracting classifications\".format(len(cleaner_data)))\n",
    "cleaner_data[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to start by getting a feel for how the classifications are distributed across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'A': 1777,\n",
       "         'B': 1449,\n",
       "         'C': 865,\n",
       "         'D': 54,\n",
       "         'E': 269,\n",
       "         'F': 735,\n",
       "         'G': 3335,\n",
       "         'H': 2754})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check how our data is distributed across the classes\n",
    "class_count = Counter([d[1] for d in cleaner_data])\n",
    "class_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is quite interesting is we see that the data is mainly clusted around class A, B, G and H. Classes C, D, E and F have a limited number of associated claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the characters in the data to use a reduced set of printable characters\n",
    "# There is a function in patentdata to do this\n",
    "from patentdata.models.lib.utils import clean_characters\n",
    "\n",
    "cleaner_data = [(clean_characters(d[0]), d[1]) for d in cleaner_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = cleaner_data\n",
    "with open(\"raw_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(raw_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising into Words and Building a Dictionary\n",
    "\n",
    "Now we need to split our claim text into word tokens and build a dictionary of words.\n",
    "\n",
    "We will start by using NLTK word_tokenize out of the box. We will assume some errors may creep in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_in_words = [(word_tokenize(d[0]), d[1]) for d in cleaner_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1',\n",
       "  '.',\n",
       "  'A',\n",
       "  'sensing',\n",
       "  'assembly',\n",
       "  'for',\n",
       "  'sensing',\n",
       "  'a',\n",
       "  'level',\n",
       "  'of',\n",
       "  'liquid',\n",
       "  'in',\n",
       "  'a',\n",
       "  'reservoir',\n",
       "  ',',\n",
       "  'said',\n",
       "  'sensing',\n",
       "  'assembly',\n",
       "  'comprising',\n",
       "  ':',\n",
       "  'a',\n",
       "  'first',\n",
       "  'input',\n",
       "  'port',\n",
       "  'for',\n",
       "  'receiving',\n",
       "  'a',\n",
       "  'first',\n",
       "  'input',\n",
       "  'voltage',\n",
       "  'signal',\n",
       "  ';',\n",
       "  'a',\n",
       "  'second',\n",
       "  'input',\n",
       "  'port',\n",
       "  'for',\n",
       "  'receiving',\n",
       "  'a',\n",
       "  'second',\n",
       "  'input',\n",
       "  'voltage',\n",
       "  'signal',\n",
       "  ';',\n",
       "  'an',\n",
       "  'excitation',\n",
       "  'circuit',\n",
       "  'electrically',\n",
       "  'connected',\n",
       "  'to',\n",
       "  'said',\n",
       "  'first',\n",
       "  'and',\n",
       "  'second',\n",
       "  'input',\n",
       "  'ports',\n",
       "  'for',\n",
       "  'receiving',\n",
       "  'the',\n",
       "  'first',\n",
       "  'and',\n",
       "  'second',\n",
       "  'input',\n",
       "  'voltage',\n",
       "  'signals',\n",
       "  'and',\n",
       "  'for',\n",
       "  'generating',\n",
       "  'a',\n",
       "  'first',\n",
       "  'excitation',\n",
       "  'signal',\n",
       "  'and',\n",
       "  'a',\n",
       "  'second',\n",
       "  'excitation',\n",
       "  'signal',\n",
       "  ',',\n",
       "  'said',\n",
       "  'excitation',\n",
       "  'circuit',\n",
       "  'includes',\n",
       "  'first',\n",
       "  'and',\n",
       "  'second',\n",
       "  'excitation',\n",
       "  'electrodes',\n",
       "  'extending',\n",
       "  'along',\n",
       "  'a',\n",
       "  'portion',\n",
       "  'of',\n",
       "  'the',\n",
       "  'reservoir',\n",
       "  ',',\n",
       "  'said',\n",
       "  'first',\n",
       "  'and',\n",
       "  'second',\n",
       "  'excitation',\n",
       "  'electrodes',\n",
       "  'disposed',\n",
       "  'adjacent',\n",
       "  'to',\n",
       "  'and',\n",
       "  'separated',\n",
       "  'by',\n",
       "  'said',\n",
       "  'first',\n",
       "  'receiving',\n",
       "  'electrode',\n",
       "  ';',\n",
       "  'and',\n",
       "  'a',\n",
       "  'receiving',\n",
       "  'circuit',\n",
       "  'disposed',\n",
       "  'adjacent',\n",
       "  'said',\n",
       "  'excitation',\n",
       "  'circuit',\n",
       "  'defining',\n",
       "  'a',\n",
       "  'variable',\n",
       "  'capacitance',\n",
       "  'with',\n",
       "  'said',\n",
       "  'excitation',\n",
       "  'circuit',\n",
       "  ',',\n",
       "  'wherein',\n",
       "  'said',\n",
       "  'receiving',\n",
       "  'circuit',\n",
       "  'includes',\n",
       "  'first',\n",
       "  'and',\n",
       "  'second',\n",
       "  'receiving',\n",
       "  'electrodes',\n",
       "  'extending',\n",
       "  'along',\n",
       "  'a',\n",
       "  'portion',\n",
       "  'of',\n",
       "  'the',\n",
       "  'reservoir',\n",
       "  'and',\n",
       "  'a',\n",
       "  'first',\n",
       "  'trace',\n",
       "  'connected',\n",
       "  'to',\n",
       "  'ground',\n",
       "  'and',\n",
       "  'extending',\n",
       "  'between',\n",
       "  'said',\n",
       "  'first',\n",
       "  'receiving',\n",
       "  'electrode',\n",
       "  'and',\n",
       "  'said',\n",
       "  'first',\n",
       "  'and',\n",
       "  'second',\n",
       "  'excitation',\n",
       "  'electrodes',\n",
       "  ',',\n",
       "  'wherein',\n",
       "  'said',\n",
       "  'first',\n",
       "  'receiving',\n",
       "  'electrode',\n",
       "  'extends',\n",
       "  'along',\n",
       "  'a',\n",
       "  'first',\n",
       "  'non-linear',\n",
       "  'path',\n",
       "  'and',\n",
       "  'said',\n",
       "  'second',\n",
       "  'receiving',\n",
       "  'electrode',\n",
       "  'extends',\n",
       "  'along',\n",
       "  'a',\n",
       "  'second',\n",
       "  'non-linear',\n",
       "  'path',\n",
       "  'differing',\n",
       "  'from',\n",
       "  'said',\n",
       "  'first',\n",
       "  'non-linear',\n",
       "  'path',\n",
       "  ',',\n",
       "  'said',\n",
       "  'receiving',\n",
       "  'circuit',\n",
       "  'producing',\n",
       "  'an',\n",
       "  'output',\n",
       "  'voltage',\n",
       "  'signal',\n",
       "  'variable',\n",
       "  'with',\n",
       "  'the',\n",
       "  'level',\n",
       "  'of',\n",
       "  'liquid',\n",
       "  'in',\n",
       "  'the',\n",
       "  'reservoir',\n",
       "  'due',\n",
       "  'to',\n",
       "  'capacitance',\n",
       "  'changes',\n",
       "  'between',\n",
       "  'said',\n",
       "  'excitation',\n",
       "  'circuit',\n",
       "  'and',\n",
       "  'said',\n",
       "  'receiving',\n",
       "  'circuit',\n",
       "  'due',\n",
       "  'to',\n",
       "  'dielectric',\n",
       "  'changes',\n",
       "  'created',\n",
       "  'by',\n",
       "  'the',\n",
       "  'liquid',\n",
       "  '.'],\n",
       " 'G')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in_words[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max claim length = 6134\n"
     ]
    }
   ],
   "source": [
    "# What is our maximum claim length?\n",
    "print(\"Max claim length = {0}\".format(max([len(d[0]) for d in data_in_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 2,\n",
       "         4: 3,\n",
       "         5: 23,\n",
       "         6: 9,\n",
       "         7: 1,\n",
       "         10: 2,\n",
       "         11: 3,\n",
       "         12: 6,\n",
       "         13: 3,\n",
       "         14: 5,\n",
       "         15: 7,\n",
       "         16: 8,\n",
       "         17: 7,\n",
       "         18: 12,\n",
       "         19: 16,\n",
       "         20: 12,\n",
       "         21: 16,\n",
       "         22: 12,\n",
       "         23: 18,\n",
       "         24: 14,\n",
       "         25: 17,\n",
       "         26: 19,\n",
       "         27: 19,\n",
       "         28: 23,\n",
       "         29: 25,\n",
       "         30: 26,\n",
       "         31: 19,\n",
       "         32: 21,\n",
       "         33: 22,\n",
       "         34: 21,\n",
       "         35: 29,\n",
       "         36: 31,\n",
       "         37: 29,\n",
       "         38: 31,\n",
       "         39: 26,\n",
       "         40: 36,\n",
       "         41: 40,\n",
       "         42: 34,\n",
       "         43: 35,\n",
       "         44: 49,\n",
       "         45: 39,\n",
       "         46: 28,\n",
       "         47: 30,\n",
       "         48: 33,\n",
       "         49: 44,\n",
       "         50: 50,\n",
       "         51: 38,\n",
       "         52: 43,\n",
       "         53: 56,\n",
       "         54: 54,\n",
       "         55: 55,\n",
       "         56: 58,\n",
       "         57: 65,\n",
       "         58: 50,\n",
       "         59: 73,\n",
       "         60: 58,\n",
       "         61: 76,\n",
       "         62: 86,\n",
       "         63: 55,\n",
       "         64: 64,\n",
       "         65: 56,\n",
       "         66: 61,\n",
       "         67: 75,\n",
       "         68: 67,\n",
       "         69: 67,\n",
       "         70: 64,\n",
       "         71: 68,\n",
       "         72: 83,\n",
       "         73: 64,\n",
       "         74: 64,\n",
       "         75: 77,\n",
       "         76: 73,\n",
       "         77: 81,\n",
       "         78: 84,\n",
       "         79: 69,\n",
       "         80: 77,\n",
       "         81: 56,\n",
       "         82: 68,\n",
       "         83: 91,\n",
       "         84: 68,\n",
       "         85: 82,\n",
       "         86: 84,\n",
       "         87: 86,\n",
       "         88: 76,\n",
       "         89: 71,\n",
       "         90: 73,\n",
       "         91: 81,\n",
       "         92: 68,\n",
       "         93: 68,\n",
       "         94: 70,\n",
       "         95: 80,\n",
       "         96: 81,\n",
       "         97: 78,\n",
       "         98: 81,\n",
       "         99: 73,\n",
       "         100: 110,\n",
       "         101: 91,\n",
       "         102: 76,\n",
       "         103: 75,\n",
       "         104: 77,\n",
       "         105: 68,\n",
       "         106: 88,\n",
       "         107: 72,\n",
       "         108: 74,\n",
       "         109: 75,\n",
       "         110: 82,\n",
       "         111: 70,\n",
       "         112: 64,\n",
       "         113: 70,\n",
       "         114: 73,\n",
       "         115: 53,\n",
       "         116: 68,\n",
       "         117: 58,\n",
       "         118: 64,\n",
       "         119: 59,\n",
       "         120: 79,\n",
       "         121: 73,\n",
       "         122: 65,\n",
       "         123: 65,\n",
       "         124: 60,\n",
       "         125: 74,\n",
       "         126: 61,\n",
       "         127: 65,\n",
       "         128: 63,\n",
       "         129: 61,\n",
       "         130: 69,\n",
       "         131: 58,\n",
       "         132: 64,\n",
       "         133: 67,\n",
       "         134: 51,\n",
       "         135: 55,\n",
       "         136: 52,\n",
       "         137: 61,\n",
       "         138: 52,\n",
       "         139: 58,\n",
       "         140: 56,\n",
       "         141: 49,\n",
       "         142: 50,\n",
       "         143: 63,\n",
       "         144: 53,\n",
       "         145: 59,\n",
       "         146: 57,\n",
       "         147: 54,\n",
       "         148: 54,\n",
       "         149: 56,\n",
       "         150: 35,\n",
       "         151: 37,\n",
       "         152: 34,\n",
       "         153: 42,\n",
       "         154: 45,\n",
       "         155: 46,\n",
       "         156: 45,\n",
       "         157: 55,\n",
       "         158: 39,\n",
       "         159: 47,\n",
       "         160: 35,\n",
       "         161: 31,\n",
       "         162: 39,\n",
       "         163: 43,\n",
       "         164: 47,\n",
       "         165: 46,\n",
       "         166: 42,\n",
       "         167: 41,\n",
       "         168: 39,\n",
       "         169: 27,\n",
       "         170: 43,\n",
       "         171: 36,\n",
       "         172: 32,\n",
       "         173: 41,\n",
       "         174: 36,\n",
       "         175: 30,\n",
       "         176: 43,\n",
       "         177: 43,\n",
       "         178: 35,\n",
       "         179: 36,\n",
       "         180: 41,\n",
       "         181: 31,\n",
       "         182: 40,\n",
       "         183: 28,\n",
       "         184: 21,\n",
       "         185: 29,\n",
       "         186: 38,\n",
       "         187: 25,\n",
       "         188: 35,\n",
       "         189: 39,\n",
       "         190: 35,\n",
       "         191: 24,\n",
       "         192: 37,\n",
       "         193: 30,\n",
       "         194: 30,\n",
       "         195: 23,\n",
       "         196: 26,\n",
       "         197: 28,\n",
       "         198: 22,\n",
       "         199: 23,\n",
       "         200: 25,\n",
       "         201: 26,\n",
       "         202: 25,\n",
       "         203: 23,\n",
       "         204: 14,\n",
       "         205: 22,\n",
       "         206: 12,\n",
       "         207: 21,\n",
       "         208: 22,\n",
       "         209: 22,\n",
       "         210: 33,\n",
       "         211: 16,\n",
       "         212: 27,\n",
       "         213: 17,\n",
       "         214: 11,\n",
       "         215: 14,\n",
       "         216: 15,\n",
       "         217: 26,\n",
       "         218: 15,\n",
       "         219: 15,\n",
       "         220: 18,\n",
       "         221: 18,\n",
       "         222: 13,\n",
       "         223: 16,\n",
       "         224: 16,\n",
       "         225: 16,\n",
       "         226: 18,\n",
       "         227: 19,\n",
       "         228: 15,\n",
       "         229: 15,\n",
       "         230: 13,\n",
       "         231: 11,\n",
       "         232: 16,\n",
       "         233: 15,\n",
       "         234: 20,\n",
       "         235: 11,\n",
       "         236: 4,\n",
       "         237: 13,\n",
       "         238: 18,\n",
       "         239: 11,\n",
       "         240: 20,\n",
       "         241: 10,\n",
       "         242: 14,\n",
       "         243: 9,\n",
       "         244: 11,\n",
       "         245: 13,\n",
       "         246: 10,\n",
       "         247: 11,\n",
       "         248: 16,\n",
       "         249: 16,\n",
       "         250: 18,\n",
       "         251: 8,\n",
       "         252: 14,\n",
       "         253: 11,\n",
       "         254: 11,\n",
       "         255: 13,\n",
       "         256: 13,\n",
       "         257: 19,\n",
       "         258: 8,\n",
       "         259: 8,\n",
       "         260: 6,\n",
       "         261: 5,\n",
       "         262: 14,\n",
       "         263: 9,\n",
       "         264: 11,\n",
       "         265: 13,\n",
       "         266: 11,\n",
       "         267: 10,\n",
       "         268: 9,\n",
       "         269: 7,\n",
       "         270: 4,\n",
       "         271: 10,\n",
       "         272: 9,\n",
       "         273: 7,\n",
       "         274: 7,\n",
       "         275: 15,\n",
       "         276: 7,\n",
       "         277: 9,\n",
       "         278: 16,\n",
       "         279: 9,\n",
       "         280: 6,\n",
       "         281: 8,\n",
       "         282: 10,\n",
       "         283: 9,\n",
       "         284: 7,\n",
       "         285: 9,\n",
       "         286: 4,\n",
       "         287: 9,\n",
       "         288: 8,\n",
       "         289: 5,\n",
       "         290: 6,\n",
       "         291: 8,\n",
       "         292: 7,\n",
       "         293: 8,\n",
       "         294: 2,\n",
       "         295: 6,\n",
       "         296: 11,\n",
       "         297: 3,\n",
       "         298: 5,\n",
       "         299: 4,\n",
       "         300: 2,\n",
       "         301: 2,\n",
       "         302: 4,\n",
       "         303: 6,\n",
       "         304: 5,\n",
       "         305: 9,\n",
       "         306: 2,\n",
       "         307: 5,\n",
       "         308: 8,\n",
       "         309: 1,\n",
       "         310: 2,\n",
       "         311: 4,\n",
       "         312: 2,\n",
       "         313: 6,\n",
       "         314: 2,\n",
       "         315: 4,\n",
       "         316: 1,\n",
       "         317: 3,\n",
       "         318: 5,\n",
       "         319: 2,\n",
       "         320: 2,\n",
       "         321: 1,\n",
       "         322: 2,\n",
       "         323: 4,\n",
       "         324: 7,\n",
       "         325: 4,\n",
       "         326: 3,\n",
       "         327: 4,\n",
       "         328: 4,\n",
       "         329: 7,\n",
       "         330: 4,\n",
       "         331: 5,\n",
       "         332: 6,\n",
       "         333: 4,\n",
       "         334: 3,\n",
       "         335: 1,\n",
       "         336: 3,\n",
       "         337: 5,\n",
       "         338: 5,\n",
       "         339: 3,\n",
       "         340: 2,\n",
       "         341: 4,\n",
       "         342: 5,\n",
       "         343: 3,\n",
       "         344: 5,\n",
       "         345: 4,\n",
       "         346: 2,\n",
       "         347: 3,\n",
       "         348: 5,\n",
       "         349: 4,\n",
       "         350: 5,\n",
       "         351: 2,\n",
       "         352: 1,\n",
       "         353: 3,\n",
       "         354: 4,\n",
       "         355: 6,\n",
       "         356: 2,\n",
       "         357: 3,\n",
       "         358: 3,\n",
       "         359: 5,\n",
       "         360: 2,\n",
       "         362: 6,\n",
       "         363: 3,\n",
       "         364: 3,\n",
       "         365: 1,\n",
       "         366: 1,\n",
       "         367: 2,\n",
       "         368: 3,\n",
       "         369: 2,\n",
       "         370: 1,\n",
       "         371: 3,\n",
       "         372: 1,\n",
       "         373: 2,\n",
       "         374: 1,\n",
       "         375: 1,\n",
       "         376: 5,\n",
       "         377: 1,\n",
       "         378: 2,\n",
       "         379: 1,\n",
       "         380: 5,\n",
       "         381: 1,\n",
       "         382: 3,\n",
       "         383: 1,\n",
       "         384: 1,\n",
       "         385: 1,\n",
       "         386: 1,\n",
       "         387: 1,\n",
       "         389: 1,\n",
       "         391: 4,\n",
       "         392: 2,\n",
       "         393: 3,\n",
       "         394: 2,\n",
       "         395: 3,\n",
       "         396: 2,\n",
       "         397: 3,\n",
       "         398: 3,\n",
       "         399: 2,\n",
       "         400: 2,\n",
       "         401: 1,\n",
       "         402: 3,\n",
       "         403: 1,\n",
       "         405: 4,\n",
       "         406: 3,\n",
       "         409: 3,\n",
       "         412: 2,\n",
       "         413: 3,\n",
       "         414: 2,\n",
       "         415: 1,\n",
       "         416: 2,\n",
       "         417: 2,\n",
       "         419: 2,\n",
       "         420: 1,\n",
       "         421: 1,\n",
       "         422: 1,\n",
       "         423: 1,\n",
       "         425: 2,\n",
       "         426: 1,\n",
       "         427: 1,\n",
       "         428: 3,\n",
       "         430: 3,\n",
       "         431: 1,\n",
       "         432: 2,\n",
       "         433: 1,\n",
       "         434: 1,\n",
       "         435: 5,\n",
       "         436: 1,\n",
       "         437: 3,\n",
       "         439: 2,\n",
       "         441: 1,\n",
       "         443: 1,\n",
       "         445: 1,\n",
       "         446: 1,\n",
       "         447: 1,\n",
       "         449: 1,\n",
       "         451: 4,\n",
       "         452: 2,\n",
       "         456: 2,\n",
       "         457: 1,\n",
       "         458: 1,\n",
       "         459: 1,\n",
       "         460: 2,\n",
       "         461: 1,\n",
       "         462: 1,\n",
       "         463: 1,\n",
       "         464: 1,\n",
       "         465: 1,\n",
       "         468: 1,\n",
       "         472: 1,\n",
       "         473: 2,\n",
       "         474: 1,\n",
       "         476: 1,\n",
       "         478: 1,\n",
       "         479: 1,\n",
       "         480: 1,\n",
       "         481: 3,\n",
       "         483: 2,\n",
       "         485: 1,\n",
       "         487: 1,\n",
       "         491: 1,\n",
       "         492: 1,\n",
       "         497: 1,\n",
       "         499: 2,\n",
       "         501: 2,\n",
       "         503: 1,\n",
       "         504: 2,\n",
       "         507: 1,\n",
       "         509: 1,\n",
       "         516: 1,\n",
       "         519: 1,\n",
       "         524: 1,\n",
       "         527: 1,\n",
       "         530: 1,\n",
       "         531: 3,\n",
       "         538: 1,\n",
       "         540: 1,\n",
       "         542: 1,\n",
       "         544: 1,\n",
       "         546: 1,\n",
       "         547: 1,\n",
       "         548: 1,\n",
       "         551: 1,\n",
       "         556: 1,\n",
       "         564: 1,\n",
       "         566: 1,\n",
       "         567: 1,\n",
       "         568: 1,\n",
       "         569: 1,\n",
       "         572: 1,\n",
       "         573: 1,\n",
       "         574: 1,\n",
       "         580: 1,\n",
       "         582: 1,\n",
       "         584: 1,\n",
       "         589: 1,\n",
       "         591: 1,\n",
       "         594: 1,\n",
       "         595: 1,\n",
       "         601: 1,\n",
       "         612: 1,\n",
       "         613: 1,\n",
       "         614: 1,\n",
       "         615: 1,\n",
       "         618: 1,\n",
       "         622: 2,\n",
       "         626: 1,\n",
       "         627: 2,\n",
       "         628: 1,\n",
       "         633: 1,\n",
       "         635: 1,\n",
       "         637: 1,\n",
       "         647: 1,\n",
       "         657: 2,\n",
       "         660: 1,\n",
       "         664: 1,\n",
       "         666: 2,\n",
       "         674: 1,\n",
       "         680: 1,\n",
       "         682: 1,\n",
       "         691: 1,\n",
       "         694: 1,\n",
       "         696: 2,\n",
       "         698: 1,\n",
       "         705: 1,\n",
       "         709: 1,\n",
       "         710: 1,\n",
       "         713: 1,\n",
       "         715: 1,\n",
       "         717: 1,\n",
       "         719: 1,\n",
       "         738: 1,\n",
       "         742: 1,\n",
       "         751: 1,\n",
       "         767: 1,\n",
       "         772: 1,\n",
       "         780: 1,\n",
       "         803: 1,\n",
       "         816: 1,\n",
       "         834: 1,\n",
       "         865: 1,\n",
       "         867: 2,\n",
       "         870: 1,\n",
       "         886: 1,\n",
       "         887: 1,\n",
       "         904: 1,\n",
       "         906: 1,\n",
       "         915: 1,\n",
       "         948: 1,\n",
       "         968: 1,\n",
       "         1008: 1,\n",
       "         1055: 1,\n",
       "         1059: 1,\n",
       "         1060: 1,\n",
       "         1062: 1,\n",
       "         1067: 1,\n",
       "         1078: 1,\n",
       "         1094: 1,\n",
       "         1134: 1,\n",
       "         1149: 1,\n",
       "         1165: 1,\n",
       "         1177: 1,\n",
       "         1242: 1,\n",
       "         1277: 1,\n",
       "         1286: 1,\n",
       "         1296: 1,\n",
       "         1299: 1,\n",
       "         1374: 1,\n",
       "         1394: 1,\n",
       "         1396: 1,\n",
       "         1411: 1,\n",
       "         1426: 1,\n",
       "         1493: 1,\n",
       "         1497: 1,\n",
       "         1502: 1,\n",
       "         1520: 1,\n",
       "         1602: 1,\n",
       "         1637: 1,\n",
       "         1719: 1,\n",
       "         1739: 1,\n",
       "         1948: 1,\n",
       "         2078: 1,\n",
       "         2141: 1,\n",
       "         2198: 1,\n",
       "         2235: 1,\n",
       "         2245: 1,\n",
       "         2442: 1,\n",
       "         2477: 2,\n",
       "         2947: 1,\n",
       "         3087: 1,\n",
       "         3466: 1,\n",
       "         6134: 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_length_counter = Counter([len(d[0]) for d in data_in_words])\n",
    "claim_length_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 10262 claims after removing long claims\n"
     ]
    }
   ],
   "source": [
    "# We might want to filter all claims over 250 tokens long\n",
    "filtered_data = [d for d in data_in_words if len(d[0]) <= 250]\n",
    "print(\"There are now {0} claims after removing long claims\".format(len(filtered_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's clear some memory\n",
    "del data_in_words, cleaner_data, no_cancelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26585 different tokens in our dataset.\n",
      "\n",
      "The 100 most common words are: [('the', 80735), ('a', 69013), ('of', 42528), (',', 39512), ('and', 33553), ('to', 29191), (';', 23056), ('.', 20649), ('said', 16925), ('an', 14311), ('in', 14166), ('for', 12766), ('first', 12460), ('comprising', 11484), ('1', 10922), ('at', 9921), (':', 9633), ('is', 9629), (')', 9320), ('second', 9024), ('one', 8572), ('A', 8283), ('(', 7875), ('with', 7604), ('from', 7384), ('least', 7112), ('on', 6544), ('by', 6284), ('wherein', 6275), ('or', 5983), ('that', 5768), ('having', 5422), ('device', 5307), ('data', 4805), ('plurality', 4546), ('which', 4397), ('method', 4339), ('each', 3564), ('surface', 3559), ('portion', 3551), ('signal', 3506), ('system', 3188), ('unit', 3174), ('layer', 3091), ('being', 2973), ('between', 2852), ('information', 2783), ('configured', 2691), ('image', 2640), ('end', 2582)]\n"
     ]
    }
   ],
   "source": [
    "# Look at the words in our claims\n",
    "word_counter = Counter(sum([d[0] for d in filtered_data], list()))\n",
    "print(\"There are {0} different tokens in our dataset.\\n\".format(len(word_counter)))\n",
    "print(\"The 100 most common words are:\", word_counter.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 100 least common words are: [('de-activate', 1), ('rubber-type', 1), ('0509237', 1), ('press-molding', 1), ('Plus', 1), ('Inter-integrated', 1), ('bar-like', 1), ('re-emission', 1), ('thereshold', 1), ('unnecessary', 1), ('Invaplex', 1), ('FMS-like', 1), ('chines', 1), ('expectations', 1), ('missed', 1), ('opener', 1), ('post-crosslinking', 1), ('xml', 1), ('poxvirus', 1), ('canted', 1), ('fingerprinting', 1), ('microcapillary', 1), ('xy', 1), ('Al-Cu', 1), ('2nd', 1), ('Shift-1', 1), ('pseudo-NMOS', 1), ('mediation', 1), ('trash', 1), ('anti-microbial', 1), ('phenylsulfonyl', 1), ('-COO', 1), ('ketoaldonic', 1), ('counter-rotating', 1), ('chisel-shaped', 1), ('2-hour', 1), ('encourage', 1), ('Machine-readable', 1), ('diffusive', 1), ('replaceablely', 1), ('hematological', 1), ('integrally-formed', 1), ('gm/9000', 1), ('tails', 1), ('R3_NO2', 1), ('non-reducing', 1), ('diffusible', 1), ('boot-toe', 1), ('judge', 1), ('electrode-side', 1), ('autopilot', 1), ('condenser-evaporator', 1), ('Irgamet', 1), ('cyclo', 1), ('colpopexy', 1), ('Taraxaci', 1), ('NO:83', 1), ('jump', 1), ('phosphorescent', 1), ('0.4eV_LUMO', 1), ('characteristically', 1), ('portals', 1), ('ring-opening', 1), ('Natantia', 1), ('right-handed', 1), ('203190', 1), ('squareness', 1), ('58,427', 1), ('Palmitoyl', 1), ('semi-finished', 1), ('PAPRs', 1), ('-SO2H', 1), ('CCD', 1), ('14-25', 1), ('pulse-width', 1), ('pre-moistening', 1), ('task-specific', 1), ('lapped', 1), ('clenching', 1), ('SOD', 1), ('disc-based', 1), ('986', 1), ('transflective', 1), ('Ebola', 1), ('MgB2', 1), ('combustibles', 1), ('Loading', 1), ('pulse-width-modulated', 1), ('narrow-field', 1), ('flood-fill', 1), ('topped', 1), ('isophthalic', 1), ('ki', 1), ('consequence', 1), ('electro-crystallization', 1), ('hetero-coagulated', 1), ('endodontic', 1), ('touch-contact', 1), ('rR', 1), ('workstations', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"The 100 least common words are:\", word_counter.most_common()[:-100-1:-1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"word_counter.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word_counter, f)\n",
    "with open(\"filtered_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(filtered_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 1), ('c', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text = [['a', 'b'], ['a', 'a'], ['c']]\n",
    "c = Counter(sum(X_text, list()))\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_text = [d[1] for d in raw_data]\n",
    "    \n",
    "class_dictionary = {word: i for i, word in enumerate(sorted(Counter(Y_text).keys()))} \n",
    "class_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now bringing this all together a little\n",
    "\n",
    "def build_dataset(raw_data, length_filter=250, vocabulary_size=25000):\n",
    "    \"\"\" Convert raw data in the form of (claim_text, class) into \n",
    "    X, Y format where X is numeric data and Y is label data.\n",
    "    \n",
    "    length_filter is a parameter to filter by token length \n",
    "    (e.g. to exclude long claims).\"\"\"\n",
    "    # Tokenise and filter\n",
    "    raw_data = [(word_tokenize(d[0]), d[1]) for d in raw_data]\n",
    "    raw_data = [d for d in raw_data if len(d[0]) <= length_filter]\n",
    "    \n",
    "    # Create labels first\n",
    "    Y_text = [d[1] for d in raw_data]\n",
    "    class_dictionary = {c: i for i, c in enumerate(sorted(Counter(Y_text).keys()))} \n",
    "    Y_data = [class_dictionary[y_text] for y_text in Y_text]\n",
    "    reverse_class_dictionary = dict(zip(class_dictionary.values(), class_dictionary.keys()))\n",
    "    \n",
    "    # Create X in text\n",
    "    X_text = [d[0] for d in raw_data]\n",
    "    \n",
    "    # Change to lowercase\n",
    "    X_text = [[word.lower() for word in x_text] for x_text in X_text]\n",
    "    \n",
    "    # Create complete wordset for dictionary generation\n",
    "    words = sum(X_text, list())\n",
    "    # Reserve slots for PAD and UNK tokens\n",
    "    count = [('PAD', 0), ['UNK', -1]]\n",
    "    count.extend(Counter(words).most_common(vocabulary_size - 2))\n",
    "    \n",
    "    # Build dictionary\n",
    "    word_dictionary = {word: i for i, (word, _) in enumerate(count)}\n",
    "        \n",
    "    # Build X in indexes\n",
    "    X_data = list()\n",
    "    unk_count = 0\n",
    "    # Go through claims replacing words with index \n",
    "    for x_text in X_text:\n",
    "        x_data = list()\n",
    "        for word in x_text:\n",
    "            if word in word_dictionary:\n",
    "                index = word_dictionary[word]\n",
    "            else:\n",
    "                index = 1  # dictionary['UNK']\n",
    "                unk_count = unk_count + 1\n",
    "            x_data.append(index)\n",
    "        X_data.append(x_data)\n",
    "        \n",
    "    count[1][1] = unk_count\n",
    "    reverse_word_dictionary = dict(zip(word_dictionary.values(), word_dictionary.keys())) \n",
    "    \n",
    "    return X_data, Y_data, count, word_dictionary, reverse_word_dictionary, class_dictionary, reverse_class_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "filename = \"X_Y_data.pkl\"\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        print(\"Loading data\")\n",
    "        X_data, Y_data, count, word_dictionary, reverse_word_dictionary, class_dictionary, reverse_class_dictionary = pickle.load(f)\n",
    "else:\n",
    "    X_data, Y_data, count, word_dictionary, reverse_word_dictionary, class_dictionary, reverse_class_dictionary = build_dataset(raw_data)\n",
    "    print('Most common words (+UNK)', count[:5])\n",
    "    print('Sample data', X_data[0], Y_data[0])\n",
    "    with open(filename, \"wb\") as f:\n",
    "        data = (X_data, Y_data, count, word_dictionary, reverse_word_dictionary, class_dictionary, reverse_class_dictionary)\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dictionary[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dictionary[\"computer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_class_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 7, 7, 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10262 10262\n"
     ]
    }
   ],
   "source": [
    "# Check vectors are the same length\n",
    "print(len(X_data), len(Y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Applying Sequence Classification with Keras\n",
    "\n",
    "Working from this post - https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/ - we can then apply an LSTM followed by a single dense layer.\n",
    "\n",
    "Documentation for padding - https://keras.io/preprocessing/sequence/#pad_sequences . We probably need to reserve 0 as a reserved character.\n",
    "\n",
    "Also here is the keras guide to sequential classification: https://keras.io/getting-started/sequential-model-guide/.\n",
    "\n",
    "The post here explains how to split data into training / test using Keras: https://gogul09.github.io/software/first-neural-network-keras.\n",
    "\n",
    "See here for BiDirectional LSTM: https://github.com/fchollet/keras/blob/master/examples/imdb_bidirectional_lstm.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# First we need to split out data into training and test data - go for 80:20\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# seed for reproducing same results\n",
    "seed = 9\n",
    "np.random.seed(seed)\n",
    "\n",
    "# split the data into training (80%) and testing (20%)\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(X_data, Y_data, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training data has length: 8209 and our test data has length: 2053\n"
     ]
    }
   ],
   "source": [
    "print(\"Our training data has length: {0} and our test data has length: {1}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we need to segment and pad our claim text sequences - we have already restricted our claims to length 250\n",
    "# We might want to experiment with changing this\n",
    "max_word_length = 250\n",
    "# Padding is performing by adding 0, which we have reserved as a PAD token above\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_word_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,    16,     9,    11,   564,\n",
       "           5,    15,    18,     3,  1076,    36,    19,    80,    12,\n",
       "           3,   319,   382,     5,    12,    36,    17,    26,    22,\n",
       "          39,   808,     7,     3,   358,   164,   171,    19,  1625,\n",
       "          12,     3,   472,    90,     6,  2148, 22072,    12,   219,\n",
       "        3702,     8,     3,   963,    55,     3,    35,     4,   244,\n",
       "        1727,  1083,  1042,     7,     2,  4480,    39,     4,     2,\n",
       "        1076,     8,     3,   533,    36,   165,    11,   164,    40,\n",
       "        1107,     7,    53,   775,   128,     2,   358,   164,    58,\n",
       "           3,   416,     5,     6,     3,   532,    40,  1274,    39,\n",
       "         808,     7,     2,   164,    40,    19,  1642,     7,     2,\n",
       "        1076,     5,     6,    36,    19,   520,     4,     3,  1548,\n",
       "         244,    63,    13,   531,     4,   244, 11435,     4,     3,\n",
       "         244,    67,     8,     6,     3,   437,    36,    19,   233,\n",
       "          27,   162,     2,   461,    39,    58,    31,     4,     2,\n",
       "        1076,     5,   374,     7,     2,   963,     5,     6, 20422,\n",
       "           2,   963,     6,     2,   532,    40,     9], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 classes\n"
     ]
    }
   ],
   "source": [
    "no_classes = len(class_dictionary)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "print(\"There are {0} classes\".format(no_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8209,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert labels to categorical one-hot encoding\n",
    "Y_train = to_categorical(Y_train, num_classes=no_classes)\n",
    "Y_test = to_categorical(Y_test, num_classes=no_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8209, 8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 250, 128)          3200000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 808       \n",
      "=================================================================\n",
      "Total params: 3,292,408\n",
      "Trainable params: 3,292,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8209 samples, validate on 2053 samples\n",
      "Epoch 1/30\n",
      "8209/8209 [==============================] - 373s - loss: 1.6462 - acc: 0.3220 - val_loss: 1.4812 - val_acc: 0.3892\n",
      "Epoch 2/30\n",
      "8209/8209 [==============================] - 361s - loss: 1.2952 - acc: 0.4601 - val_loss: 1.3047 - val_acc: 0.4442\n",
      "Epoch 3/30\n",
      "8209/8209 [==============================] - 363s - loss: 0.9728 - acc: 0.6294 - val_loss: 1.2868 - val_acc: 0.5231\n",
      "Epoch 4/30\n",
      "8209/8209 [==============================] - 362s - loss: 0.6702 - acc: 0.7627 - val_loss: 1.3479 - val_acc: 0.5022\n",
      "Epoch 5/30\n",
      "8209/8209 [==============================] - 361s - loss: 0.4416 - acc: 0.8554 - val_loss: 1.5451 - val_acc: 0.5153\n",
      "Epoch 6/30\n",
      "8209/8209 [==============================] - 362s - loss: 0.3011 - acc: 0.9022 - val_loss: 1.7189 - val_acc: 0.5226\n",
      "Epoch 7/30\n",
      "8209/8209 [==============================] - 362s - loss: 0.1961 - acc: 0.9414 - val_loss: 1.9058 - val_acc: 0.5095\n",
      "Epoch 8/30\n",
      "8209/8209 [==============================] - 361s - loss: 0.1501 - acc: 0.9559 - val_loss: 2.0808 - val_acc: 0.5061\n",
      "Epoch 9/30\n",
      "8209/8209 [==============================] - 361s - loss: 0.1016 - acc: 0.9708 - val_loss: 2.3301 - val_acc: 0.5217\n",
      "Epoch 10/30\n",
      "8209/8209 [==============================] - 361s - loss: 0.0697 - acc: 0.9808 - val_loss: 2.4141 - val_acc: 0.5022\n",
      "Epoch 11/30\n",
      "8209/8209 [==============================] - 361s - loss: 0.0582 - acc: 0.9828 - val_loss: 2.6726 - val_acc: 0.5037\n",
      "Epoch 12/30\n",
      "8209/8209 [==============================] - 362s - loss: 0.0620 - acc: 0.9825 - val_loss: 2.7497 - val_acc: 0.4934\n",
      "Epoch 13/30\n",
      "8209/8209 [==============================] - 361s - loss: 0.0486 - acc: 0.9861 - val_loss: 2.7475 - val_acc: 0.5012\n",
      "Epoch 14/30\n",
      "8209/8209 [==============================] - 358s - loss: 0.0244 - acc: 0.9940 - val_loss: 3.0469 - val_acc: 0.5007\n",
      "Epoch 15/30\n",
      "8209/8209 [==============================] - 359s - loss: 0.0175 - acc: 0.9961 - val_loss: 3.1546 - val_acc: 0.5051\n",
      "Epoch 16/30\n",
      "8209/8209 [==============================] - 359s - loss: 0.0227 - acc: 0.9934 - val_loss: 3.2591 - val_acc: 0.5041\n",
      "Epoch 17/30\n",
      "8209/8209 [==============================] - 359s - loss: 0.0272 - acc: 0.9921 - val_loss: 3.2457 - val_acc: 0.4871\n",
      "Epoch 18/30\n",
      "8209/8209 [==============================] - 359s - loss: 0.0708 - acc: 0.9778 - val_loss: 3.1070 - val_acc: 0.4832\n",
      "Epoch 19/30\n",
      "8209/8209 [==============================] - 359s - loss: 0.0335 - acc: 0.9904 - val_loss: 3.3187 - val_acc: 0.4998\n",
      "Epoch 20/30\n",
      "8209/8209 [==============================] - 358s - loss: 0.0280 - acc: 0.9917 - val_loss: 3.2127 - val_acc: 0.5056\n",
      "Epoch 21/30\n",
      "8209/8209 [==============================] - 359s - loss: 0.0099 - acc: 0.9974 - val_loss: 3.5311 - val_acc: 0.5056\n",
      "Epoch 22/30\n",
      "8209/8209 [==============================] - 358s - loss: 0.0084 - acc: 0.9976 - val_loss: 3.4709 - val_acc: 0.5007\n",
      "Epoch 23/30\n",
      "8209/8209 [==============================] - 359s - loss: 0.0030 - acc: 0.9996 - val_loss: 3.7571 - val_acc: 0.5037\n",
      "Epoch 24/30\n",
      "8209/8209 [==============================] - 359s - loss: 0.0015 - acc: 0.9998 - val_loss: 3.8310 - val_acc: 0.5041\n",
      "Epoch 25/30\n",
      "8209/8209 [==============================] - 358s - loss: 0.0015 - acc: 0.9996 - val_loss: 3.9034 - val_acc: 0.5110\n",
      "Epoch 26/30\n",
      "8209/8209 [==============================] - 359s - loss: 0.0010 - acc: 0.9998 - val_loss: 3.9770 - val_acc: 0.5080\n",
      "Epoch 27/30\n",
      "8209/8209 [==============================] - 359s - loss: 9.3875e-04 - acc: 0.9998 - val_loss: 4.0387 - val_acc: 0.5129\n",
      "Epoch 28/30\n",
      "8209/8209 [==============================] - 358s - loss: 0.0010 - acc: 0.9998 - val_loss: 4.0730 - val_acc: 0.5085\n",
      "Epoch 29/30\n",
      "8209/8209 [==============================] - 359s - loss: 8.3174e-04 - acc: 0.9996 - val_loss: 4.1287 - val_acc: 0.5110\n",
      "Epoch 30/30\n",
      "8209/8209 [==============================] - 359s - loss: 0.0011 - acc: 0.9996 - val_loss: 4.1619 - val_acc: 0.5119\n"
     ]
    }
   ],
   "source": [
    "# Now building our model \n",
    "embedding_vecor_length = 128\n",
    "vocabulary_size=25000\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_vecor_length, input_length=max_word_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=30, batch_size=64)\n",
    "model.save('claim_class_lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we are overfitting. Probably the embedding layer - it has 3.2m parameters!  \n",
    "\n",
    "Let's try with some dropout.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2053/2053 [==============================] - 33s    \n",
      "Accuracy: 51.10%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. We need to install h5py. Let's say in JSON for now and add h5py to our Docker file. See here for more details - https://machinelearningmastery.com/save-load-keras-deep-learning-models/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Multi-Layer Perceptron to Claim Data\n",
    "\n",
    "See this example here - https://github.com/fchollet/keras/blob/master/examples/reuters_mlp.py ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
