{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Claims - Getting the Data\n",
    "\n",
    "Here are a short series of notebooks that investigate classifying patent claims using deep learning techniques.\n",
    "\n",
    "In this post we will generate a data source to use for classifying patent claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using USPTO data, where the claims are classified according to the International Patent Classification (IPC). To keep things simple we will use the first letter of the IPC (top level category). This is the same as the top level of the Cooperative Patent Classification (CPC).  \n",
    "\n",
    "The list of top level categories can be found here: https://rs.espacenet.com/help?locale=en_EP&method=handleHelpTopic&topic=ipc:\n",
    "* A Human Necessities\n",
    "* B Performing Operations; Transporting\n",
    "* C Chemistry; Metallurgy\n",
    "* D Textiles; Paper\n",
    "* E Fixed Constructions\n",
    "* F Mechanical Engineering; Lighting; Heating; Weapons; Blasting Engines or Pumps\n",
    "* G Physics\n",
    "* H Electricity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Getting Our Data\n",
    "\n",
    "Most of the task revolves around getting our data and placing it in a form where we can apply common machine learning libraries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with getting 12,000 patent publications at random\n",
    "from patentdata.corpus import USPublications\n",
    "from patentdata.models.patentcorpus import LazyPatentCorpus\n",
    "import os, pickle\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some function that provide a wrapper around USPTO patent publication data as obtained from the bulk data download page (https://www.uspto.gov/learning-and-resources/bulk-data-products). This code can be found on GitHub - https://github.com/benhoyle/patentdata.\n",
    "\n",
    "The code below gets 12,000 random patent publications from data for years 2001 to 2017 across all classifications. From each document the text for claim 1 and the classifications are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "12000 claims and classifications loaded\n"
     ]
    }
   ],
   "source": [
    "# Get the claim 1 and classificationt text\n",
    "\n",
    "PIK = \"claim_and_class.data\"\n",
    "\n",
    "if os.path.isfile(PIK):\n",
    "    with open(PIK, \"rb\") as f:\n",
    "        print(\"Loading data\")\n",
    "        data = pickle.load(f)\n",
    "        print(\"{0} claims and classifications loaded\".format(len(data)))\n",
    "else:\n",
    "    # Load our list of records\n",
    "    PIK = \"12000records.data\"\n",
    "\n",
    "    if os.path.isfile(PIK):\n",
    "        with open(PIK, \"rb\") as f:\n",
    "            print(\"Loading data\")\n",
    "            records = pickle.load(f)\n",
    "            print(\"{0} records loaded\".format(len(records)))\n",
    "    else:\n",
    "        path = '/media/SAMSUNG1/Patent_Downloads'\n",
    "        ds = USPublications(path)\n",
    "        records = ds.get_records([], \"name\", sample_size=12000)\n",
    "        with open(PIK, \"wb\") as f:\n",
    "            pickle.dump(records, f)\n",
    "            print(\"{0} records saved\".format(len(records)))\n",
    "    \n",
    "    lzy = LazyPatentCorpus()\n",
    "    lzy.init_by_filenames(ds, records)\n",
    "    \n",
    "    data = list()\n",
    "    for i, pd in enumerate(lzy.documents):\n",
    "        try:\n",
    "            classifications = [c.as_string() for c in pd.classifications]\n",
    "        except:\n",
    "            classifications = \"\"\n",
    "        try:\n",
    "            claim1_text = pd.claimset.get_claim(1).text\n",
    "        except:\n",
    "            claim1_text = \"\"\n",
    "        current_data = (claim1_text, classifications)\n",
    "        data.append(current_data)\n",
    "        if (i % 500) == 0:\n",
    "            print(\"Saving a checkpoint at {0} files\".format(i))\n",
    "            print(\"Current data = \", current_data)\n",
    "            with open(PIK, \"wb\") as f:\n",
    "                pickle.dump(data, f)\n",
    "            \n",
    "    with open(PIK, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "    print(\"{0} claims saved\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then get rid of claims which are cancelled (these just have \"(canceled)\" as text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 11239 claims after filtering out cancelled claims\n"
     ]
    }
   ],
   "source": [
    "# Check for and remove 'cancelled' claims\n",
    "no_cancelled = [d for d in data if '(canceled)' not in d[0]]\n",
    "\n",
    "print(\"There are now {0} claims after filtering out cancelled claims\".format(len(no_cancelled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we filter the data to extract the first level classification. For simplicity we take the first classification (if there are several classifications) where the data exists. An example data entry is set out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 11238 claims after extracting classifications\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('\\n1. A sensing assembly for sensing a level of liquid in a reservoir, said sensing assembly comprising: \\na first input port for receiving a first input voltage signal; \\na second input port for receiving a second input voltage signal; \\nan excitation circuit electrically connected to said first and second input ports for receiving the first and second input voltage signals and for generating a first excitation signal and a second excitation signal, said excitation circuit includes first and second excitation electrodes extending along a portion of the reservoir, said first and second excitation electrodes disposed adjacent to and separated by said first receiving electrode; and \\na receiving circuit disposed adjacent said excitation circuit defining a variable capacitance with said excitation circuit, wherein said receiving circuit includes first and second receiving electrodes extending along a portion of the reservoir and a first trace connected to ground and extending between said first receiving electrode and said first and second excitation electrodes, wherein said first receiving electrode extends along a first non-linear path and said second receiving electrode extends along a second non-linear path differing from said first non-linear path, said receiving circuit producing an output voltage signal variable with the level of liquid in the reservoir due to capacitance changes between said excitation circuit and said receiving circuit due to dielectric changes created by the liquid. \\n\\n',\n",
       " 'G')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get classification in the form of A-H\n",
    "cleaner_data = list()\n",
    "for d in no_cancelled:\n",
    "    if len(d[1]) >= 1:\n",
    "        if len(d[1][0]) > 3:\n",
    "            classification = d[1][0][2]\n",
    "            cleaner_data.append(\n",
    "                (d[0], classification)\n",
    "            )\n",
    "\n",
    "print(\"There are now {0} claims after extracting classifications\".format(len(cleaner_data)))\n",
    "cleaner_data[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to start by getting a feel for how the classifications are distributed across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'A': 1777,\n",
       "         'B': 1449,\n",
       "         'C': 865,\n",
       "         'D': 54,\n",
       "         'E': 269,\n",
       "         'F': 735,\n",
       "         'G': 3335,\n",
       "         'H': 2754})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check how our data is distributed across the classes\n",
    "class_count = Counter([d[1] for d in cleaner_data])\n",
    "class_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is quite interesting is we see that the data is mainly clusted around class A, B, G and H. Classes C, D, E and F have a limited number of associated claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help further processing we will clean the characters of the text. The patentdata library has a function that replaces certain characters (e.g. different versions of '\"' or '-') with a reduced set of common printable characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the characters in the data to use a reduced set of printable characters\n",
    "# There is a function in patentdata to do this\n",
    "from patentdata.models.lib.utils import clean_characters\n",
    "\n",
    "cleaner_data = [(clean_characters(d[0]), d[1]) for d in cleaner_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will save our claims and classifications as a Pickle file called \"raw_data\". We can load this quickly in subsequent notebooks to apply classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = cleaner_data\n",
    "with open(\"raw_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(raw_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
