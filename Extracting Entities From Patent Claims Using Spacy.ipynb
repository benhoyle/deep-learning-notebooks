{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Entity Extraction\n",
    "\n",
    "In this notebook we will be looking at using spaCy (https://spacy.io/) to populate object models from patent claim data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's import spaCy\n",
    "import spacy\n",
    "from spacy.symbols import DET, NOUN, CCONJ, VERB, PUNCT\n",
    "\n",
    "nlp = spacy.load('en') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference here are some common object POS patterns as extracted from a patent specification using the reference numeral as an end point.\n",
    "```\n",
    "[('<DET><NOUN><NUM>', 63),\n",
    " ('<DET><NOUN><NOUN><NUM>', 50),\n",
    " ('<DET><VERB><NOUN><NUM>', 48),\n",
    " ('<DET><ADJ><NOUN><NUM>', 39),\n",
    " ('<DET><NOUN><NOUN><NOUN><NUM>', 35),\n",
    " ('<DET><ADJ><ADJ><NOUN><NOUN><NUM>', 14),\n",
    " ('<DET><NOUN><PUNCT><VERB><NOUN><NUM>', 8),\n",
    " ('<DET><ADJ><NOUN><NOUN><NUM>', 6),\n",
    " ('<DET><ADJ><CCONJ><ADJ><ADJ><NOUN><NOUN><NUM>', 4),\n",
    " ('<DET><NOUN><NOUN><NOUN><NOUN><NUM>', 3),\n",
    " ('<DET><NOUN><ADP><NOUN><NOUN><NUM>', 3),\n",
    " ('<DET><ADJ><CCONJ><ADJ><NOUN><NUM>', 3),\n",
    " ('<DET><NOUN><ADP><NOUN><NUM>', 3),\n",
    " ('<DET><NOUN><VERB><NOUN><NUM>', 2),\n",
    " ('<DET><NOUN><ADV><CCONJ><ADJ><NOUN><NUM>', 1),\n",
    " ('<DET><ADJ><VERB><NUM><PUNCT><NUM><ADP><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><ADP><ADV><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><ADV><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><ADV><VERB><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><VERB><NOUN><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><PUNCT><NOUN><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><VERB><ADP><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><ADP><ADJ><ADJ><ADJ><NOUN><NOUN><NUM>', 1),\n",
    " ('<DET><ADJ><NOUN><PUNCT><NOUN><PUNCT><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><PUNCT><NOUN><PUNCT><NOUN><PUNCT><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><VERB><NOUN><ADV><CCONJ><ADJ><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><ADP><ADJ><NOUN><NUM>', 1)]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some initial functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_spacy_entity_finder(doc):\n",
    "    \"\"\" Find entities with reference numerals using POS data.\"\"\"\n",
    "    entity_list = list()\n",
    "    record = False\n",
    "    # Generate a list of tokens so we can iterate backwards through it\n",
    "    enum_doc_list = list(enumerate(doc))\n",
    "    last_end = 0\n",
    "    # Add indices\n",
    "    for i, word in enum_doc_list:\n",
    "        if word.pos == DET and not record:\n",
    "            # Start recording and record start index\n",
    "            record = True\n",
    "            start_index = i\n",
    "        else:        \n",
    "            if (word.pos == DET or word.pos == CCONJ or word.lemma_ == \";\") and record:\n",
    "                # Step back until last noun is found\n",
    "                for j, bword in reversed(enum_doc_list[last_end:i]):\n",
    "                    if bword.pos == NOUN:\n",
    "                        # Add np_chunk to buffer\n",
    "                        entity_list.append(doc[start_index:j+1])\n",
    "                        last_end = j\n",
    "                        break       \n",
    "                if word.pos == DET:\n",
    "                    # Set new start index\n",
    "                    record = True\n",
    "                    start_index = i\n",
    "                else:\n",
    "                    record = False\n",
    "    \n",
    "    entity_dict = dict()\n",
    "    # Now group by unique\n",
    "    for entity in entity_list:\n",
    "        \n",
    "        np_start = entity.start\n",
    "        # Ignore the determinant \n",
    "        if doc[np_start].pos == DET:\n",
    "            np_start += 1\n",
    "        # Generate a string representation excluding the determinant\n",
    "        np_string = doc[np_start:entity.end].text.lower()\n",
    "                                \n",
    "        if np_string not in entity_dict.keys():\n",
    "            entity_dict[np_string] = list()          \n",
    "        entity_dict[np_string].append(entity)\n",
    "    \n",
    "    return entity_list, entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_ant(doc, entity_dict):\n",
    "    \"\"\" Check antecedence - attempt to merge entries with incorrect antecedence.\"\"\"\n",
    "    \n",
    "    issue_keys_a = list()\n",
    "    issue_keys_the = list()\n",
    "    \n",
    "    # Look for entries with antecedence issues\n",
    "    for key in entity_dict:\n",
    "        entities = entity_dict[key]\n",
    "        # Check if first entry begins with \"a\" - flag if doesn't\n",
    "        first_entry = entities[0]\n",
    "        if first_entry[0].pos == DET and first_entry[0].lemma_ != \"a\" and first_entry[0].lemma_ != \"an\":\n",
    "            issue_keys_a.append(key)\n",
    "        \n",
    "        # If more than one entry check subsequent entries start with \"the\" - flag if don't\n",
    "        if len(entities) > 1:\n",
    "            for entity in entities[1:]:\n",
    "                if entity[0].pos == DET and entity[0].lemma_ != \"the\":\n",
    "                    issue_keys_the.append(key)\n",
    "    \n",
    "    return issue_keys_a, issue_keys_the\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def look_for_existing(doc, entity_dict):\n",
    "    \"\"\" Look for previously existing versions of problem keys.\"\"\"\n",
    "    # If more than one entry check subsequent entries start with \"the\" - flag if don't\n",
    "    issue_keys_a = list()\n",
    "    for key in entity_dict:\n",
    "        entities = entity_dict[key]\n",
    "        # Check if first entry begins with \"a\" - flag if doesn't\n",
    "        first_entry = entities[0]\n",
    "        if first_entry[0].pos == DET and first_entry[0].lemma_ != \"a\" and first_entry[0].lemma_ != \"an\":\n",
    "            issue_keys_a.append(key)\n",
    "    \n",
    "    for pkey in issue_keys_a:\n",
    "        problem_entities = entity_dict[pkey]\n",
    "        # i.e. list of two longer oblong spans\n",
    "        # Can we just work with the key initially?\n",
    "        for key in entity_dict.keys():\n",
    "            if len(pkey) > len(key) and key in pkey:\n",
    "                print(key, pkey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We now need to collate and create a set of entities\n",
    "def get_entity_set(entity_list):\n",
    "    \"\"\" Get a set of unique entity n-grams from a list of entities.\"\"\"\n",
    "    ngram_list = list()\n",
    "    for entity in entity_list:\n",
    "        ngram_list.append(\" \".join([word for word, pos in entity if (pos != 'DET')]))\n",
    "    return set(ngram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_entity_finder(doc):\n",
    "    \"\"\" Find entities using noun phrases/chunks.\"\"\"\n",
    "    entity_dict = dict()\n",
    "    for entity in doc.noun_chunks:\n",
    "        np_start = entity.start\n",
    "        # Ignore the determinant \n",
    "        if doc[np_start].pos == DET:\n",
    "            np_start += 1\n",
    "        # Generate a string representation excluding the determinant\n",
    "        np_string = doc[np_start:entity.end].text.lower()\n",
    "                                \n",
    "        if np_string not in entity_dict.keys():\n",
    "            entity_dict[np_string] = list()          \n",
    "        entity_dict[np_string].append(entity)\n",
    "        \n",
    "    return entity_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing on Other Patent Data\n",
    "\n",
    "Lets test on different patent claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554570 records located.\n",
      "10 records sampled.\n"
     ]
    }
   ],
   "source": [
    "# Generate or create some test claim sets for analysis\n",
    "\n",
    "# (Looks like we can't pickle and load spaCy objects)\n",
    "from patentdata.corpus import USPublications\n",
    "\n",
    "pubs = USPublications(\"/media/SAMSUNG1/Patent_Downloads\")\n",
    "filegenerator = pubs.patentdoc_generator(['G', '06'], sample_size=10)\n",
    "docs = list(filegenerator)\n",
    "ent_from_claims = list()\n",
    "nlp_docs = list()\n",
    "for doc in docs:\n",
    "    nlp_doc = nlp(doc.claimset.get_claim(1).text)\n",
    "    entity_list, entity_dict = simple_spacy_entity_finder(nlp_doc)\n",
    "    nlp_docs.append(nlp_doc)\n",
    "    ent_from_claims.append(entity_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'communication system': [a communication system],\n",
       " 'experience': [the experience],\n",
       " 'game system': [A game system],\n",
       " 'interactive apparatus': [an interactive apparatus,\n",
       "  the interactive apparatus],\n",
       " 'interactive application': [an interactive application,\n",
       "  the interactive application,\n",
       "  the interactive application],\n",
       " 'offline': [an offline],\n",
       " 'online experience': [an online experience]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_from_claims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These terms are not explicitly introduced using 'a/an X':\n",
      " ['experience'] \n",
      "\n",
      "These terms do not use 'the' yet occur previously:\n",
      " [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ika, ikt = check_ant(nlp_docs[0], ent_from_claims[0])\n",
    "print(\"These terms are not explicitly introduced using 'a/an X':\\n\", ika, \"\\n\")\n",
    "\n",
    "print(\"These terms do not use 'the' yet occur previously:\\n\", ikt, \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1. A game system comprising:\n",
       "an interactive apparatus having a communication system; and\n",
       "an interactive application, the interactive application and the interactive apparatus are independently operable to provide an offline and an online experience, wherein at least one of the interactive application and interactive apparatus is configured modify its operation based on the experience of the other.\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. A game system comprising:\n",
      "an interactive apparatus having a communication system; and\n",
      "an interactive application, the interactive application and the interactive apparatus are independently operable to provide an offline and an online experience, wherein at least one of the interactive application and interactive apparatus is configured modify its operation based on the experience of the other.\n",
      "\n",
      " \n",
      "\n",
      "{'interactive apparatus': [an interactive apparatus, the interactive apparatus], 'game system': [A game system], 'communication system': [a communication system], 'offline': [an offline], 'interactive application': [an interactive application, the interactive application, the interactive application], 'experience': [the experience], 'online experience': [an online experience]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A server for providing affiliate store information, comprising:\n",
      "a receiver for collecting transaction count information relating to a number of transactions made in each affiliate store and receiving search condition information from a mobile communication terminal;\n",
      "a database for storing affiliate store information generated based on the collected transaction count information according to a predetermined criteria;\n",
      "a generator for extracting the affiliate store information of at least one affiliate store from the database according to the search condition information and generating a search result using the extracted affiliate store information; and\n",
      "a transmitter for transmitting the search result to the mobile communication terminal.\n",
      "\n",
      " \n",
      "\n",
      "{'predetermined criteria': [a predetermined criteria], 'mobile communication terminal': [a mobile communication terminal], 'receiver for collecting transaction count information': [a receiver for collecting transaction count information], 'generator': [a generator], 'affiliate store': [each affiliate store], 'server for providing affiliate store information': [A server for providing affiliate store information], 'collected transaction count information': [the collected transaction count information], 'database for storing affiliate store information': [a database for storing affiliate store information], 'extracted affiliate store information': [the extracted affiliate store information], 'transmitter': [a transmitter], 'search result': [a search result, the search result], 'number of transactions': [a number of transactions], 'database': [the database], 'affiliate store information of at least one affiliate store': [the affiliate store information of at least one affiliate store], 'search condition information': [the search condition information]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A cover for a mobile device, comprising:\n",
      "side surfaces configured to be adjacent at least a portion one or more side surfaces of a mobile device;\n",
      "a rear surface configured to be adjacent at least a portion of a rear surface of the mobile device and connected to the side surfaces, the side surfaces and the rear surface form an opening that receives at least a portion of the mobile device;\n",
      "a connector configured to connect to a port of the mobile device;\n",
      "a circuit that connects the connector to a communication module; and\n",
      "the communication module configured to execute transactions with contactless devices and communicate with the mobile device through the circuit and the connector.\n",
      "\n",
      " \n",
      "\n",
      "{'portion': [a portion, a portion, a portion], 'rear surface': [a rear surface, a rear surface], 'cover': [A cover], 'port': [a port], 'opening': [an opening], 'circuit': [a circuit, the circuit], 'rear surface form': [the rear surface form], 'communication module': [a communication module], 'mobile device, comprising:\\nside surfaces': [a mobile device, comprising:\n",
      "side surfaces], 'communication module configured to execute transactions with contactless devices': [the communication module configured to execute transactions with contactless devices], 'mobile device': [a mobile device, the mobile device, the mobile device, the mobile device, the mobile device], 'side surfaces': [the side surfaces, the side surfaces], 'connector': [a connector, the connector]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A dynamic voltage scaling scheduling method for resource-sharing and hard real-time tasks, applicable for scheduling tasks in a delayed task set, comprising:\n",
      "determining a property of a task, and executing one of the following steps, when the task belongs to the delayed task set or the task does not belong to the task collection but a waiting time has exceeded a period of the task;\n",
      "when one task in the delayed task set requires for being executed, increasing a working voltage required for executing the task, removing the task from the delayed task set, and returning to the step of determining the property of the task;\n",
      "when one task in the delayed task set requires for sharing resources, setting the working voltage required by the task as a current working voltage or as a larger one in least upper bounds of all tasks requiring for sharing resources, and returning to the step of determining the property of the task; and\n",
      "when one task not belonging to the delayed task set exists, and the waiting time of the task has exceeded the period of the task, reducing the working voltage required for executing the task, adding the task in the delayed task set, and returning to the step of determining the property of the task.\n",
      "\n",
      " \n",
      "\n",
      "{'delayed task set requires for sharing resources': [the delayed task set requires for sharing resources], 'waiting time': [a waiting time, the waiting time], 'larger one in least upper bounds': [a larger one in least upper bounds], 'step': [the step, the step, the step], 'dynamic voltage scaling scheduling method for resource-sharing': [A dynamic voltage scaling scheduling method for resource-sharing], 'task': [a task, the task, the task, the task, the task, the task, the task, the task, the task, the task, the task, the task, the task], 'delayed task': [a delayed task, the delayed task, the delayed task, the delayed task, the delayed task], 'property': [a property, the property, the property, the property], 'working voltage': [a working voltage, the working voltage, the working voltage], 'delayed task set': [the delayed task set], 'current working voltage': [a current working voltage], 'task collection': [the task collection], 'following steps': [the following steps], 'tasks requiring for sharing resources': [all tasks requiring for sharing resources], 'period': [a period, the period]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A method of forming conductive traces on a touch sensor panel, comprising:\n",
      "forming and patterning a stackup of a first conductive material over a substrate in one or more border areas of the touch sensor panel to create one or more traces having widths maximized so that the one or more traces and any separation areas between the traces in any particular portion of the border area occupy a full width of that portion of the border area; and\n",
      "forming and patterning a layer of a second conductive material over the substrate to create one or more rows, each row coupled to a different trace, the rows forming part of a plurality of sensors on the touch sensor panel.\n",
      "\n",
      " \n",
      "\n",
      "{'': [], 'different trace': [a different trace], 'plurality of sensors': [a plurality of sensors], 'touch sensor panel': [a touch sensor panel, the touch sensor panel], 'layer': [a layer], 'substrate': [a substrate, the substrate], 'row': [each row], 'rows forming part': [the rows forming part], 'full width': [a full width], 'traces': [the traces], 'separation areas': [any separation areas], 'portion': [that portion], 'stackup': [a stackup], 'second conductive material': [a second conductive material], 'method of forming conductive traces': [A method of forming conductive traces], 'first conductive material': [a first conductive material], 'border area': [the border area, the border area], 'particular portion': [any particular portion]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A method for a storage system, the storage system including a first controller, a second controller and a plurality of storage devices, each of the first and second controllers communicatively coupled to each one of the storage devices, the method comprising:\n",
      "starting a timer that expires after a first time period; and\n",
      "subsequent to starting the timer, transmitting a first message from the first controller to a memory element shared by the first and second controllers, the first message capable of notifying the second controller of an imminent failure of the first controller, wherein subsequent to transmitting the first message to the shared memory element and before or when the timer expires, the first controller becomes unavailable to facilitate access to the storage devices.\n",
      "\n",
      " \n",
      "\n",
      "{'': [, , , ], 'first time period': [a first time period], 'method': [A method, the method], 'first message': [a first message, the first message, the first message], 'shared memory element': [the shared memory element], 'second controller': [a second controller, the second controller], 'plurality of storage devices': [a plurality of storage devices], 'first controller': [a first controller, the first controller, the first controller], 'first controller becomes unavailable to facilitate access': [the first controller becomes unavailable to facilitate access], 'storage system': [a storage system, the storage system], 'storage devices': [the storage devices], 'timer': [a timer, the timer, the timer], 'imminent failure': [an imminent failure], 'memory element': [a memory element]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A method comprising:\n",
      "initiating a first interaction of a payment cardholder computer system executing a browser application with an item of content associated with a merchant Website;\n",
      "before completion of the first interaction, initiating a proxy interaction of the payment cardholder computer system executing a browser application with an item of content associated with the merchant Website;\n",
      "communicating with a payment card network connected to the merchant Website; and\n",
      "using the proxy interaction to determine whether the merchant Website is registered with the payment card network.\n",
      "\n",
      " \n",
      "\n",
      "{'merchant website': [a merchant Website, the merchant Website, the merchant Website], 'method': [A method], 'browser application': [a browser application, a browser application], 'payment card network': [a payment card network], 'merchant': [the merchant], 'proxy interaction': [a proxy interaction, the proxy interaction], 'first interaction': [a first interaction, the first interaction], 'item of content': [an item of content, an item of content], 'payment cardholder computer system': [a payment cardholder computer system, the payment cardholder computer system]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A power-saving method for a mobile terminal, comprising the steps of:\n",
      "monitoring, by the mobile terminal, data services in accordance with a monitoring strategy for a data service power-saving state;\n",
      "triggering a data service power-saving operation when a triggering condition set in the monitoring strategy is met; and\n",
      "performing, by the mobile terminal, the data service power-saving operation.\n",
      "\n",
      " \n",
      "\n",
      "{'data service power-saving state': [a data service power-saving state], 'mobile terminal': [a mobile terminal, the mobile terminal], 'triggering condition': [a triggering condition], 'mobile terminal, data services in accordance': [the mobile terminal, data services in accordance], 'monitoring strategy': [a monitoring strategy, the monitoring strategy], 'power-saving method': [A power-saving method], 'data service power-saving operation': [a data service power-saving operation], 'steps of:\\nmonitoring': [the steps of:\n",
      "monitoring]} \n",
      "------\n",
      "\n",
      "\n",
      "1-10. (canceled)\n",
      " \n",
      "\n",
      "{} \n",
      "------\n",
      "\n",
      "\n",
      "1. A system for automatically replacing problematic media files comprising:\n",
      "a first media store configured to store a plurality of digitally encoded local media files;\n",
      "a second media store configured to store a plurality of digitally encoded source media files;\n",
      "a media diagnostic engine configured to identify whether problems exist within one of the media files located in the first media store, wherein when a problem is identified, the associated file having the problem is able to be referred to as a problematic file, wherein the media diagnostic engine is configured to identify whether problems exist based upon at least one of a user input and a software based error detection algorithm determination; and\n",
      "a media replacement engine configured to replace a problematic file in the first media store with a copy of a corresponding media file from the second media store.\n",
      "\n",
      " \n",
      "\n",
      "{'problematic file': [a problematic file, a problematic file], 'media diagnostic engine is configured to identify whether problems': [the media diagnostic engine is configured to identify whether problems], 'plurality of digitally encoded source media files': [a plurality of digitally encoded source media files], 'associated file': [the associated file], 'problem': [a problem, the problem], 'second media store': [a second media store], 'media files': [the media files], 'system for automatically replacing problematic media files': [A system for automatically replacing problematic media files], 'first media store': [a first media store, the first media store, the first media store], 'copy': [a copy], 'plurality of digitally encoded local media files': [a plurality of digitally encoded local media files], 'media diagnostic engine configured to identify whether problems': [a media diagnostic engine configured to identify whether problems], 'user input': [a user input], 'media replacement engine': [a media replacement engine], 'corresponding media file': [a corresponding media file], 'software based error detection algorithm determination': [a software based error detection algorithm determination]} \n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d, e in zip(nlp_docs, ent_from_claims):\n",
    "    print(d, \"\\n\")\n",
    "    print(e, \"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Matching occurrences of \"the X\" with other entries looks generally useful (e.g. is needed across multiple claims). Phrases such as \"the given X\" or \"the selected X\" also appear.\n",
    "* There are some long sections that appear not to meet the simple parse.\n",
    "* Some have a blank entity?\n",
    "* We could use the noun_chunks as a second test and merge for greater accuracy?\n",
    "* Doesn't work so well on some method claims.\n",
    "* Need to stop on punctuation as well, i.e. \",\" or \";\"\n",
    "* \"said\" needs to be a DET.\n",
    "* Plurals cause an issue, e.g. \"multimedia data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " [A game system, an interactive apparatus, a communication system, an interactive application, the interactive application, the interactive apparatus, an offline, an online experience, the interactive application, interactive apparatus, its operation, the experience] \n",
      "\n",
      "['interactive apparatus', 'game system', 'communication system', 'offline', 'interactive application', 'experience', 'online experience']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A server, affiliate store information, a receiver, transaction count information, a number, transactions, each affiliate store, search condition information, a mobile communication terminal, a database, affiliate store information, the collected transaction count information, a predetermined criteria, the affiliate store information, at least one affiliate store, the database, the search condition information, a search result, the extracted affiliate store information, a transmitter, the search result, the mobile communication terminal] \n",
      "\n",
      "['predetermined criteria', 'mobile communication terminal', 'receiver for collecting transaction count information', 'generator', 'affiliate store', 'server for providing affiliate store information', 'collected transaction count information', 'database for storing affiliate store information', 'extracted affiliate store information', 'transmitter', 'search result', 'number of transactions', 'database', 'affiliate store information of at least one affiliate store', 'search condition information']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A cover, a mobile device, at least a portion, a mobile device, at least a portion, a rear surface, the mobile device, the side surfaces, the side surfaces, the rear surface form, at least a portion, the mobile device, a port, the mobile device, the connector, a communication module, transactions, contactless devices, the mobile device, the circuit, the connector] \n",
      "\n",
      "['portion', 'rear surface', 'cover', 'port', 'opening', 'circuit', 'rear surface form', 'communication module', 'mobile device, comprising:\\nside surfaces', 'communication module configured to execute transactions with contactless devices', 'mobile device', 'side surfaces', 'connector']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [\n",
      "1. A dynamic voltage scaling, scheduling method, resource-sharing, hard real-time tasks, scheduling tasks, a delayed task, a property, a task, the following steps, the task, the delayed task, the task, the task collection, a waiting time, a period, the task, one task, the delayed task, a working voltage, the task, the task, the delayed task, the step, the property, the task, one task, the delayed task, sharing resources, the working voltage, the task, a current working voltage, a larger one, least upper bounds, all tasks, sharing resources, the step, the property, the task, the delayed task, set, the waiting time, the task, the period, the task, the working voltage, the task, the task, the delayed task, the step, the property, the task] \n",
      "\n",
      "['delayed task set requires for sharing resources', 'waiting time', 'larger one in least upper bounds', 'step', 'dynamic voltage scaling scheduling method for resource-sharing', 'task', 'delayed task', 'property', 'working voltage', 'delayed task set', 'current working voltage', 'task collection', 'following steps', 'tasks requiring for sharing resources', 'period']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A method, conductive traces, a touch sensor panel, a stackup, a first conductive material, a substrate, one or more border areas, the touch sensor panel, one or more traces, widths, that the one or more traces, any separation areas, the traces, any particular portion, the border area, a full width, that portion, the border area, a layer, a second conductive material, the substrate, one or more rows, a different trace, part, a plurality, sensors, the touch sensor panel] \n",
      "\n",
      "['', 'different trace', 'plurality of sensors', 'touch sensor panel', 'layer', 'substrate', 'row', 'rows forming part', 'full width', 'traces', 'separation areas', 'portion', 'stackup', 'second conductive material', 'method of forming conductive traces', 'first conductive material', 'border area', 'particular portion']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A method, a storage system, the storage system, a first controller, a second controller, a plurality, storage devices, the first and second controllers, the storage devices, the method, a timer, a first time period, the timer, a first message, the first controller, a memory element, the first and second controllers, the second controller, an imminent failure, the first controller, the first message, the shared memory element, the timer, the first controller, access, the storage devices] \n",
      "\n",
      "['', 'first time period', 'method', 'first message', 'shared memory element', 'second controller', 'plurality of storage devices', 'first controller', 'first controller becomes unavailable to facilitate access', 'storage system', 'storage devices', 'timer', 'imminent failure', 'memory element']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A method, a first interaction, a payment cardholder computer system, a browser application, an item, content, a merchant Website, completion, the first interaction, a proxy interaction, the payment cardholder computer system, a browser application, an item, content, the merchant Website, a payment card network, the merchant Website, the proxy interaction, the merchant, the payment card network] \n",
      "\n",
      "['merchant website', 'method', 'browser application', 'payment card network', 'merchant', 'proxy interaction', 'first interaction', 'item of content', 'payment cardholder computer system']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A power-saving method, a mobile terminal, the steps, the mobile terminal, accordance, a monitoring strategy, a data service power-saving state, a data service power-saving operation, a triggering condition, the monitoring strategy, the mobile terminal] \n",
      "\n",
      "['data service power-saving state', 'mobile terminal', 'triggering condition', 'mobile terminal, data services in accordance', 'monitoring strategy', 'power-saving method', 'data service power-saving operation', 'steps of:\\nmonitoring']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [] \n",
      "\n",
      "[]\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A system, problematic media files, a plurality, digitally encoded local media files, a second media store, a plurality, digitally encoded source media files, a media diagnostic engine, problems, the media files, the first media store, a problem, the problem, a problematic file, the media diagnostic engine, problems, a user input, a software based error detection algorithm determination, a media replacement engine, a problematic file, the first media store, a copy, a corresponding media file, the second media store] \n",
      "\n",
      "['problematic file', 'media diagnostic engine is configured to identify whether problems', 'plurality of digitally encoded source media files', 'associated file', 'problem', 'second media store', 'media files', 'system for automatically replacing problematic media files', 'first media store', 'copy', 'plurality of digitally encoded local media files', 'media diagnostic engine configured to identify whether problems', 'user input', 'media replacement engine', 'corresponding media file', 'software based error detection algorithm determination']\n",
      "\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d, e in zip(nlp_docs, ent_from_claims):\n",
    "    print(\"----\\n\", list(d.noun_chunks), \"\\n\")\n",
    "    print(list(e.keys()))\n",
    "    print(\"\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I define the problem using probabilities?  \n",
    "\n",
    "Entities are latent variables of which the words are the visible / observable data.  \n",
    "\n",
    "Problem is aligning groups of tokens with entities. Classification in a case where we don't know what the classes are or how many classes there are.  \n",
    "\n",
    "P(entity | words)\n",
    "\n",
    "What do we know for certain:\n",
    "* It will have a form of DET ... NOUN or no DET but noun phrase ending in NNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'affiliate store': [each affiliate store],\n",
       " 'affiliate store information': [affiliate store information,\n",
       "  affiliate store information,\n",
       "  the affiliate store information],\n",
       " 'at least one affiliate store': [at least one affiliate store],\n",
       " 'collected transaction count information': [the collected transaction count information],\n",
       " 'database': [a database, the database],\n",
       " 'extracted affiliate store information': [the extracted affiliate store information],\n",
       " 'mobile communication terminal': [a mobile communication terminal,\n",
       "  the mobile communication terminal],\n",
       " 'number': [a number],\n",
       " 'predetermined criteria': [a predetermined criteria],\n",
       " 'receiver': [a receiver],\n",
       " 'search condition information': [search condition information,\n",
       "  the search condition information],\n",
       " 'search result': [a search result, the search result],\n",
       " 'server': [A server],\n",
       " 'transaction count information': [transaction count information],\n",
       " 'transactions': [transactions],\n",
       " 'transmitter': [a transmitter]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_entity_finder(nlp_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([A server for providing affiliate store information,\n",
       "  a receiver for collecting transaction count information,\n",
       "  a number of transactions,\n",
       "  each affiliate store,\n",
       "  a mobile communication terminal,\n",
       "  a database for storing affiliate store information,\n",
       "  the collected transaction count information,\n",
       "  a predetermined criteria,\n",
       "  a generator,\n",
       "  the affiliate store information of at least one affiliate store,\n",
       "  the database,\n",
       "  the search condition information,\n",
       "  a search result,\n",
       "  the extracted affiliate store information,\n",
       "  a transmitter,\n",
       "  the search result],\n",
       " {'affiliate store': [each affiliate store],\n",
       "  'affiliate store information of at least one affiliate store': [the affiliate store information of at least one affiliate store],\n",
       "  'collected transaction count information': [the collected transaction count information],\n",
       "  'database': [the database],\n",
       "  'database for storing affiliate store information': [a database for storing affiliate store information],\n",
       "  'extracted affiliate store information': [the extracted affiliate store information],\n",
       "  'generator': [a generator],\n",
       "  'mobile communication terminal': [a mobile communication terminal],\n",
       "  'number of transactions': [a number of transactions],\n",
       "  'predetermined criteria': [a predetermined criteria],\n",
       "  'receiver for collecting transaction count information': [a receiver for collecting transaction count information],\n",
       "  'search condition information': [the search condition information],\n",
       "  'search result': [a search result, the search result],\n",
       "  'server for providing affiliate store information': [A server for providing affiliate store information],\n",
       "  'transmitter': [a transmitter]})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_spacy_entity_finder(nlp_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'at least a portion': [at least a portion,\n",
       "  at least a portion,\n",
       "  at least a portion],\n",
       " 'circuit': [the circuit],\n",
       " 'communication module': [a communication module],\n",
       " 'connector': [the connector, the connector],\n",
       " 'contactless devices': [contactless devices],\n",
       " 'cover': [A cover],\n",
       " 'mobile device': [a mobile device,\n",
       "  a mobile device,\n",
       "  the mobile device,\n",
       "  the mobile device,\n",
       "  the mobile device,\n",
       "  the mobile device],\n",
       " 'port': [a port],\n",
       " 'rear surface': [a rear surface],\n",
       " 'rear surface form': [the rear surface form],\n",
       " 'side surfaces': [the side surfaces, the side surfaces],\n",
       " 'transactions': [transactions]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_entity_finder(nlp_docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([A cover, a mobile device, comprising:\n",
       "  side surfaces, a portion, a mobile device, a rear surface, a portion, a rear surface, the mobile device, the side surfaces, the side surfaces, the rear surface form, an opening, a portion, the mobile device, a connector, a port, the mobile device, a circuit, the connector, a communication module, the communication module configured to execute transactions with contactless devices, the mobile device, the circuit],\n",
       " {'circuit': [a circuit, the circuit],\n",
       "  'communication module': [a communication module],\n",
       "  'communication module configured to execute transactions with contactless devices': [the communication module configured to execute transactions with contactless devices],\n",
       "  'connector': [a connector, the connector],\n",
       "  'cover': [A cover],\n",
       "  'mobile device': [a mobile device,\n",
       "   the mobile device,\n",
       "   the mobile device,\n",
       "   the mobile device,\n",
       "   the mobile device],\n",
       "  'mobile device, comprising:\\nside surfaces': [a mobile device, comprising:\n",
       "   side surfaces],\n",
       "  'opening': [an opening],\n",
       "  'port': [a port],\n",
       "  'portion': [a portion, a portion, a portion],\n",
       "  'rear surface': [a rear surface, a rear surface],\n",
       "  'rear surface form': [the rear surface form],\n",
       "  'side surfaces': [the side surfaces, the side surfaces]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_spacy_entity_finder(nlp_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Algorithm\n",
    "\n",
    "What do we know:\n",
    "* A DET or a NOUN will always form part of an entity.\n",
    "* A plural noun may not start with a DET.\n",
    "* An entity will consist of consecutive tokens.\n",
    "* The world following a DET will be part of the entity.\n",
    "* Each determinant can only be linked to one of the nouns in front of it before the next determinant or [\";\", \":\", \".\"] (and possibly \",\").\n",
    "* Entities with a \"the\" determinant should have occurred before.\n",
    "* There are no overlaps.\n",
    "* We can be more confident if a phrase is repeated.\n",
    "* We can be more confident still if the phrase is repeated that initially starts with \"a\" and the next occurrence starts with \"the\" or \"said\".\n",
    "* \"said\" should be taken as a DET.\n",
    "* There will be between 1 and number of NOUNS entities.\n",
    "* The boundary of an entity will be marked by NOUN NOTNOUN - however this pattern can also occur as part of the noun phrase for the entity.\n",
    "* Entity text sequences will not cross a \":\" or \";\".\n",
    "* Occurrences of an entity will have matching text including at least a matching noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definite constraints for a well-formed claim:\n",
    "* A NOUN will always form part of an entity;\n",
    "* A singular noun will have a determinant;\n",
    "* An entity will consist of consecutive tokens.\n",
    "* There are no overlaps in occurrences - a word can only be linked to a single entity.\n",
    "* There will be between 1 and number of NOUNS entities.\n",
    "* Entity text sequences will not cross a \":\" or \";\" or \".\" (and possibly a \",\").\n",
    "* The boundary of an entity will be marked by NOUN NOTNOUN - however this pattern can also occur as part of the noun phrase for the entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the probability of a set of entities, $ \\boldsymbol E $, given a claim as a sequence of words, $ \\boldsymbol W $: $$ P(\\boldsymbol E | \\boldsymbol W) $$   \n",
    "\n",
    "In fact we want to calculate: $$ \\underset{\\boldsymbol E}{\\operatorname{argmax}} P(\\boldsymbol E | \\boldsymbol W) $$\n",
    "\n",
    "Our claim has a length $ N $:$$\\boldsymbol W = (\\boldsymbol w_0, \\boldsymbol w_1, ..., \\boldsymbol w_{N})$$\n",
    "\n",
    "$N$ may be calculated as the length of the claim in tokens.\n",
    "\n",
    "Each word $\\boldsymbol w_i$ has:\n",
    "* text - $t_i$;\n",
    "* a simple POS tag - $pos_i$;\n",
    "* a more detailed POS tag - $posplus_i$;\n",
    "* a lemma (i.e. a normalised word form) - $lemma_i$; and\n",
    "* dependeny tree information - $dep_i$.\n",
    "\n",
    "I.e. $$ \\boldsymbol w_i = (t_i, pos_i, posplus_i, lemma_i, dep_i) $$\n",
    "\n",
    "We have $ M $ entities: $$\\boldsymbol E = (e_0, e_1, ..., e_{M})$$ \n",
    "\n",
    "where $\\boldsymbol e_0 $ indicates \"no related entity\" or a \"null\" token. $M$ is not known but will be greater than 2 and less than a number of nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An occurrence is a set of consecutive tokens: $$ \\boldsymbol o_k = [\\boldsymbol w_i, \\boldsymbol w_{i+1}, ..., \\boldsymbol w_{i+L_{k}}] $$ where $L_k$ is the length of occurrence $k$ which begins at word index $i$.\n",
    "\n",
    "$$ \\boldsymbol W = [o_1, o_2, ..., o_K] $$ where there are $K$ total occurrences in the claim. However, we don't know $K$ for sure. \n",
    "\n",
    "We do know the number of nouns $N_{noun}$. And we know $1 \\leqslant K \\leqslant N_{noun}$. Also $M \\leqslant K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An entity can have:\n",
    "* a set of one or more occurrences;\n",
    "* a string representation - possibly equal to common text across the set of occurrences;\n",
    "* a number (e.g. be singular or plural).\n",
    "\n",
    "An entity may be though of as a class label that is applied to a word: $$ \\sum_{i=0}^M p(e_i | w) = 1 $$\n",
    "\n",
    "We know that $ p(e_0 | pos = {DET}) = p(e_0 | pos = {NOUN}) = 0 $, i.e. that determinants and nouns will be assigned to some entity. We also know $ p(e_0 | t = \";\") = p(e_0 | t = \":\") = p(e_0 | t = \".\") = 1$.\n",
    "\n",
    "Entities are primarily just groupings of word spans, wherein the grouping creates a discrete entity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{i=0}^M P(\\boldsymbol o_k | e_i) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decomposing using Bayes' Rule: \n",
    "\n",
    "$$ \\underset{\\boldsymbol e}{\\operatorname{argmax}} P(\\boldsymbol e | \\boldsymbol w) = {P(\\boldsymbol w | \\boldsymbol e) P(\\boldsymbol e)}/ P(\\boldsymbol w)$$ \n",
    "\n",
    "where we can ignore the denominator as we are looking for argmax: $$ \\underset{\\boldsymbol e}{\\operatorname{argmax}} P(\\boldsymbol e | \\boldsymbol w) = {P(\\boldsymbol w | \\boldsymbol e) P(\\boldsymbol e)}$$\n",
    "\n",
    "In other models $P(\\boldsymbol w | \\boldsymbol e)$ and $P(\\boldsymbol e)$ may be approximated by a product of transitions (e.g. as per a hidden markov model). However, we have dependencies across sets of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each determinant can only be linked to one of the nouns in front of it before the next determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by setting each noun as a separate entity? And marking the tokens that are not an entity? Or look at confident selections e.g. DET NOUN [:;.,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can maybe start with a binary classification: $\\boldsymbol e = [0,1]$? No, we can confidently apply a positive determination but our negative determination is unknown, i.e. a word that is not positively marked may still form part of an entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate $M$ by counting the number of \"a\"/\"an\" determinants + the number of multiple nouns.  \n",
    "\n",
    "Issue multiple nouns are often introduced by \"a X of Ys\".  \n",
    "\n",
    "Also we have \"at least one X\" and \"one or more Ys\" - these may not be introduced by \"a\" or \"an\" and \"at least one\" may be referred to again as \"the at least one\".  \n",
    "\n",
    "Can we use an estimate of number of determinants as a lower bound?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works fairly well for a lower bound / initial estimate.  \n",
    "\n",
    "We can cross check later for missing plural nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we model a sequential constraint? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word $w_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is our good algorithm\n",
    "\n",
    "# Start with all words relate to no entities\n",
    "p_all_e_word = dict()\n",
    "\n",
    "def check_start_phrase(token, doc):\n",
    "    \"\"\" Check for start of phrases 'at least one' and 'one or more' as determinant.\n",
    "    \n",
    "    Return true if located.\"\"\"\n",
    "    i = token.i\n",
    "    condition = (\n",
    "        doc[i:i+3].text.lower() == \"at least one\" or\n",
    "        doc[i:i+3].text.lower() == \"one or more\"\n",
    "    )\n",
    "    condition = condition and (doc[i-1].text.lower() != \"the\")\n",
    "    return condition\n",
    "\n",
    "def is_det(token, doc):\n",
    "    \"\"\" Wrapper function for determinant check.\"\"\"\n",
    "    # Add 'said' as custom determination\n",
    "    condition = (token.pos == DET or token.text == \"said\")\n",
    "    # Alternatively we can have the start phrases as above\n",
    "    condition = (condition or check_start_phrase(token, doc))\n",
    "    # Add check for 'a)' and 'a.' - this is not a det\n",
    "    condition = condition and (doc[token.i:token.i+2].text.lower() not in ['a)', 'a.'])\n",
    "    return condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristics:\n",
    "* \"for\" marks a non-entity [e_0=1]\n",
    "* \"DET X of ...\" [e_0=0]\n",
    "* \"in X with\" [e_0=1]\n",
    "* \"at least one\" / \"one or more\" [e_0=0]\n",
    "* lemma = \\[\"comprise\", \"have\", \"be\", \"include\"\\] [e_0=1]\n",
    "* \"where\" in token.text [e_0=1] (e.g. \"where or wherein\")\n",
    "* \"associated with\" [e_0=1]\n",
    "* \"configured/adapted to\" [e_0=0]\n",
    "\n",
    "Also watch out for \"each of the plurality of X\" or \"at least one of the plurality of X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import NUM\n",
    "\n",
    "# Could we change this to slice on a key? Probably\n",
    "def sliceodict(d, i):\n",
    "    \"\"\"Slice an ordered dict based on a passed index.\n",
    "    list[:i] for an ordered dict\n",
    "    \"\"\"\n",
    "    temp_dict = {k:v for j, (k,v) in enumerate(d.items()) if j < i}\n",
    "    return OrderedDict(sorted(temp_dict.items(), key=lambda t: t[1][0][0].i))\n",
    "\n",
    "# We want to set these if they are not already set\n",
    "def set_probability(token, p_all_e_word, entity, new_value):\n",
    "    \"\"\" Set probability value if not set already\"\"\"\n",
    "    if entity not in p_all_e_word[token].keys():\n",
    "        if sum([v for k, v in p_all_e_word[token]] + new_value) <= 1: \n",
    "            p_all_e_word[token][entity] = new_value\n",
    "    return p_all_e_word\n",
    "            \n",
    "\n",
    "def heuristics(token, doc, p_all_e_word):\n",
    "    \"\"\" Apply heuristics to mark entity probabilities\"\"\"\n",
    "    entity_stop_chars = [\"\\n\",\":\",\";\",\".\", \",\"]\n",
    "    # Set stop characters as non-entity\n",
    "    if token.text in entity_stop_chars:\n",
    "        p_all_e_word[token][0] = 1\n",
    "    \n",
    "    # Set noun as entity\n",
    "    if token.pos == NOUN and p_all_e_word[token].get(0, None) != 1:\n",
    "        p_all_e_word[token][0] = 0\n",
    "    \n",
    "    # 'for' is an entity boundary\n",
    "    if token.lemma_ == \"for\":\n",
    "        p_all_e_word[token][0] = 1\n",
    "    \n",
    "    # \"comprise\", \"have\", \"be\", \"include\" do not relate to an entity\n",
    "    if token.lemma_ in [\"comprise\", \"have\", \"be\", \"include\"]:\n",
    "        p_all_e_word[token][0] = 1\n",
    "    \n",
    "    # \"where\" and \"wherein\" do not relate to an entity\n",
    "    if \"where\" in token.lemma_:\n",
    "         p_all_e_word[token][0] = 1\n",
    "    \n",
    "    # Look ahead - check not at end\n",
    "    if token.i < (len(doc)-1):\n",
    "        \n",
    "        # \"configured/adapted to\" do not relate to an entity\n",
    "        if doc[token.i+1].lemma_ == \"to\" and token.lemma_ in [\"configure\", \"adapt\"]:\n",
    "            p_all_e_word[token][0] = 1\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 1\n",
    "    \n",
    "    if token.i < (len(doc)-2):\n",
    "        # Set DETs as entity\n",
    "        if (\n",
    "            token.pos == DET or token.text == \"said\"\n",
    "        ) and (\n",
    "            doc[token.i:token.i+2].text.lower() not in ['a)', 'a.']\n",
    "        ):\n",
    "            p_all_e_word[token][0] = 0\n",
    "            p_all_e_word[doc[token.i+1]][0] = 0\n",
    "            \n",
    "        # DET X of .. relates to an entity\n",
    "        if token.pos == DET and doc[token.i+2].lemma_ == \"of\":\n",
    "            p_all_e_word[token][0] = 0\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 0\n",
    "            # Set of\n",
    "            p_all_e_word[doc[token.i + 2]][0] = 0\n",
    "            # Set term after off\n",
    "            p_all_e_word[doc[token.i + 3]][0] = 0\n",
    "            \n",
    "        # \"in X with\" does not relate to an entity\n",
    "        if token.lemma_ == \"in\" and doc[token.i+2].lemma_ == \"with\":\n",
    "            p_all_e_word[token][0] = 1\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 1\n",
    "            p_all_e_word[doc[token.i + 2]][0] = 1\n",
    "            \n",
    "        # Associated with does not relate to an entity\n",
    "        if doc[token.i:token.i+2].text.lower() == \"associated with\":\n",
    "            p_all_e_word[token][0] = 1\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 1\n",
    "    \n",
    "    if token.i < (len(doc)-3):\n",
    "        # \"at least NUM\" / \"NUM or more\" relates to an entity\n",
    "        if doc[token.i:token.i + 2].text.lower() == \"at least\" and doc[token.i + 2].pos == NUM:\n",
    "            p_all_e_word[token][0] = 0\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 0\n",
    "            p_all_e_word[doc[token.i + 2]][0] = 0\n",
    "        if doc[token.i+1:token.i + 3].text.lower() == \"or more\" and token.pos == NUM:\n",
    "            p_all_e_word[token][0] = 0\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 0\n",
    "            p_all_e_word[doc[token.i + 2]][0] = 0\n",
    "    \n",
    "    return p_all_e_word\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm generally is:\n",
    "* Mark as entity or not based on rules;\n",
    "* Look back from DET or punct break [':',';',',','.'] - set as non-entity until noun is found;\n",
    "* Look at noun phrase chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp_docs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from difflib import SequenceMatcher # alternative when looking at string differences for matches\n",
    "# Try out with adding heuristics\n",
    "\n",
    "def extract_entities(doc):\n",
    "    \"\"\"Extract entities from a spaCy doc object.\"\"\"\n",
    "    # Start with all words relate to no entities\n",
    "    p_all_e_word = dict()\n",
    "      \n",
    "    for token in doc:\n",
    "        # Initialise probabilities\n",
    "        p_all_e_word[token] = dict()\n",
    "        \n",
    "    # This can be combined with first pass easily - similar checks\n",
    "    print(\"First pass - entity label heuristics\")\n",
    "    for token in doc:\n",
    "        p_all_e_word = heuristics(token, doc, p_all_e_word)\n",
    "        print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n')\n",
    "   \n",
    "    print(\"Second pass - look for DET ... NOUN groupings\") \n",
    "    # Second parse - take any DET ... NOUN <boundary> portions\n",
    "    last_break = 0\n",
    "    spans_to_match = list()\n",
    "    for token in doc:\n",
    "        # Look for hard end points or DET\n",
    "        if (p_all_e_word[token].get(0, None) == 1) or (token.pos == DET):\n",
    "            print(\"{0} is e_0=1 or DET - looking back\".format(token))\n",
    "            # Step back marking as e_0=1 until first NOUN      \n",
    "            for j in range(token.i-1, last_break, -1):\n",
    "                print(\"Step back token - {0} with pos - {1}\".format(doc[j], doc[j].pos))\n",
    "                if doc[j].pos != NOUN:\n",
    "                    print(\"Setting non-Noun\")\n",
    "                    p_all_e_word[doc[j]][0] = 1\n",
    "                else:\n",
    "                    last_break = j\n",
    "                    break\n",
    "                    \n",
    "        # Look at grouping from DET\n",
    "        if is_det(token, doc):\n",
    "            # Tweak for \"at least X\" and \"X or more\"\n",
    "            if (\n",
    "                doc[token.i:token.i + 2].text.lower() == \"at least\" and doc[token.i + 2].pos == NUM\n",
    "            ) or (\n",
    "                doc[token.i+1:token.i + 3].text.lower() == \"or more\" and token.pos == NUM\n",
    "            ):\n",
    "                #print(\"Head index set to {0}\".format())\n",
    "                head_index = doc[token.i+2].head.i\n",
    "            else: \n",
    "                head_index = token.head.i\n",
    "            possible_entity = True\n",
    "            # Step through intermediate tokens between current and head\n",
    "            for j in range(token.i, head_index):\n",
    "                # If head is outside of DET ... end_NOUN sequence\n",
    "                if doc[j].head.i < token.i and doc[j].head.i > head_index:\n",
    "                    # Check for nested portions\n",
    "                    possible_entity = False\n",
    "            if possible_entity:\n",
    "                for k in range(token.i, head_index + 1):\n",
    "                    p_all_e_word[doc[k]][0] = 0 \n",
    "        # Need to adapt the above for at least one ... X and one or more ... Xs - \"at\" > head > \"least\" > \"one\" > X\n",
    "        \n",
    "        # Look at plural nouns\n",
    "        if token.tag_ == \"NNS\":\n",
    "            print(\"Located plural noun: {0}\".format(token))\n",
    "            #Step back and mark as e_0=0 any preceding word that has the token as a head\n",
    "            for j in range(token.i-1, 0, -1):\n",
    "                print(doc[j], doc[j].head.i, p_all_e_word[doc[j]])\n",
    "                if p_all_e_word[doc[j]]:\n",
    "                    break\n",
    "                elif (\n",
    "                    doc[j].head.i == token.i\n",
    "                ):\n",
    "                    print(\"Setting {0} as e_0=0\".format(doc[j]))\n",
    "                    p_all_e_word[doc[j]][0] = 0\n",
    "    \n",
    "    for token in doc:\n",
    "        print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n') \n",
    "    \n",
    "    for token in doc:\n",
    "        if not p_all_e_word[token]:\n",
    "            print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n') \n",
    "    \n",
    "    print(\"Extracted possible occurrences:\\n\")\n",
    "    poss_occ = list()\n",
    "    for token in doc[1:]:\n",
    "        # If transition\n",
    "        if p_all_e_word[token].get(0, 0) == 0 and p_all_e_word[doc[token.i-1]].get(0, 1) == 1:\n",
    "            # Add consecutive e_0=0\n",
    "            for j in range(token.i, len(doc)+1):\n",
    "                if p_all_e_word[doc[j]].get(0, 1) != 0:\n",
    "                    poss_occ.append(doc[token.i:j])\n",
    "                    break\n",
    "\n",
    "    print(poss_occ)\n",
    "\n",
    "    # Matching occurrences\n",
    "    entity_dict = dict()\n",
    "    # Now group by unique\n",
    "    for entity in poss_occ:\n",
    "        np_start = entity.start\n",
    "        # Ignore the determinant \n",
    "        if doc[np_start].pos == DET:\n",
    "            np_start += 1\n",
    "        # Generate a string representation excluding the determinant\n",
    "        np_string = doc[np_start:entity.end].text.lower()                        \n",
    "        if np_string:\n",
    "            if np_string not in entity_dict.keys():\n",
    "                entity_dict[np_string] = list()          \n",
    "            entity_dict[np_string].append(entity)\n",
    "\n",
    "    print(doc)\n",
    "    # print(entity_dict)\n",
    "\n",
    "    # Quick function to sort entities by occurrence\n",
    "    # Need to sort the keys by the index of the first word in the first entry\n",
    "    ordered_entities = OrderedDict(sorted(entity_dict.items(), key=lambda t: t[1][0][0].i))\n",
    "\n",
    "    print(ordered_entities)\n",
    "\n",
    "    # Look for duplict entities and merge\n",
    "    new_o_e = ordered_entities.copy()\n",
    "    for i, (entity_string, occurrences) in enumerate(ordered_entities.items()):\n",
    "        # Check if first entry in occurrences begins with the\n",
    "        current_occurrence = occurrences[0]\n",
    "        if current_occurrence[0].lemma_ in [\"the\", \"each\"]:\n",
    "            print(\"Found entity '{0}' with incorrect antecedence\".format(current_occurrence))\n",
    "            possible_matches = list()\n",
    "            for previous_entity_string, previous_occurrences in sliceodict(ordered_entities, i).items():\n",
    "                first_entry = previous_occurrences[0]\n",
    "                # Check to see if head of occurrence with \"the\" agrees with head of previous occurrence\n",
    "                # print(first_entry[-1].text.lower(), first_entry[-1].tag_, \n",
    "                  # current_occurrence[-1].text.lower(), current_occurrence[-1].tag_)\n",
    "                if (\n",
    "                    first_entry[-1].text.lower() == current_occurrence[-1].text.lower()\n",
    "                ) and (\n",
    "                    first_entry[-1].tag == current_occurrence[-1].tag\n",
    "                ):\n",
    "                    # print(first_entry[0].head, first_entry[-1], first_entry[-1].tag_)\n",
    "                    print(\"Found possible match with {0}\". format(previous_entity_string))\n",
    "                \n",
    "                    # Need to check here for multiple term matches \n",
    "                    possible_matches.append(previous_entity_string)\n",
    "        \n",
    "            print(possible_matches)\n",
    "            if len(possible_matches) > 0:\n",
    "                if len(possible_matches) > 1:\n",
    "                    best_match = 0.0\n",
    "                    best_match_string = \"\"\n",
    "                    for match in possible_matches:\n",
    "                        s = SequenceMatcher(a=entity_string, b=match).quick_ratio()\n",
    "                        print(s)\n",
    "                        if s > best_match:\n",
    "                            best_match = s\n",
    "                            print(\"Best match = {0}\".format(best_match))\n",
    "                            best_match_string = match\n",
    "                    previous_entity_string = best_match_string\n",
    "                else:\n",
    "                    previous_entity_string = possible_matches[0]\n",
    "                # Merge entries in copy of dict\n",
    "                print(\"Selected previous entity = {0}\".format(previous_entity_string))\n",
    "                new_o_e[previous_entity_string] += occurrences\n",
    "                new_o_e.pop(entity_string)\n",
    "                \n",
    "    print(new_o_e)\n",
    "    return new_o_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error with:\n",
    "\"the side surfaces and the rear surface form an opening\" > 'rear surface form an opening'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test claims for specifio:\n",
    "  \n",
    "A method for a storage system, the storage system including a first controller, a second controller and a plurality of storage devices, each of the first and second controllers communicatively coupled to each one of the storage devices, the method comprising:\n",
    "starting a timer that expires after a first time period; and\n",
    "subsequent to starting the timer, transmitting a first message from the first controller to a memory element shared by the first and second controllers, the first message capable of notifying the second controller of an imminent failure of the first controller, wherein subsequent to transmitting the first message to the shared memory element and before or when the timer expires, the first controller becomes unavailable to facilitate access to the storage devices.\n",
    "\n",
    "A dynamic voltage scaling scheduling method for resource-sharing and hard real-time tasks, applicable for scheduling tasks in a delayed task set, comprising:\n",
    "determining a property of a task, and executing one of the following steps, when the task belongs to the delayed task set or the task does not belong to the task collection but a waiting time has exceeded a period of the task;\n",
    "when one task in the delayed task set requires for being executed, increasing a working voltage required for executing the task, removing the task from the delayed task set, and returning to the step of determining the property of the task;\n",
    "when one task in the delayed task set requires for sharing resources, setting the working voltage required by the task as a current working voltage or as a larger one in least upper bounds of all tasks requiring for sharing resources, and returning to the step of determining the property of the task; and\n",
    "when one task not belonging to the delayed task set exists, and the waiting time of the task has exceeded the period of the task, reducing the working voltage required for executing the task, adding the task in the delayed task set, and returning to the step of determining the property of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SequenceMatcher(a=\"one or more border areas\", b=\"the border area\").quick_ratio()\n",
    "# Maybe remove determinant phrase then look at match score > ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "system system\n",
    "apparatus apparatus\n",
    "system system\n",
    "application application\n",
    "offline offline\n",
    "experience experience\n",
    "application apparatus\n",
    "modify operation\n",
    "```\n",
    "A possible entity cross-check - first_entry[0].head = first_entry[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper - http://cogprints.org/5025/1/nrc-48727.pdf - suggests a two-phase process:\n",
    "* Generate a \"gazetteer\" (a list of named entities) - similar to our first stage of simple_entity_extraction method;\n",
    "* Disambiguate names in \"gazetteer\" (this is similar to our second stage of simple_entity_extraction method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "* Need to look for entities with different names to merge based on number agreement and head agreement and presence before use of the in claim. (e.g. \"An elongate container....the container\" or \"a plurality of notches....the respective notches\" - DONE\n",
    "* Also look for unassigned words between det and noun - mark as e_0=1 look for head = noun (two image storage regions).\n",
    "* Look an phrases such as \"an offline and an online experience\" - currently split as \"an offline\" and \"an online experience\" - need to merge to \"an offline experience\" and \"an online experience\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for spans between e_0=1 - these must contain an occurrence. If there is only one DET-NOUN (check NP using head) or X NNS (check again using NP head) - that must be an entity. (This is the second parse?)\n",
    "\n",
    "Can we look backwards from DET? Anything that is not a NOUN is e_0=1?\n",
    "\n",
    "Plurals need looking at:\n",
    "```\n",
    "user [{0: 0}]\n",
    "defined [{}]\n",
    "rules [{0: 0}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 \n",
      " SPACE 1 1 SP\n",
      "1 1 1 NUM 1 1 CD\n",
      "- 2 - SYM 10 3 SYM\n",
      "10 3 10 NUM 1 1 CD\n",
      ". 4 . PUNCT 1 1 .\n",
      "( 5 ( PUNCT 1 1 -LRB-\n",
      "canceled 6 cancel VERB 1 1 VBN\n",
      ") 7 ) PUNCT canceled 6 -RRB-\n",
      "\n",
      " 8 \n",
      " SPACE ) 7 SP\n"
     ]
    }
   ],
   "source": [
    "# Look at POS and head for each token\n",
    "for token in doc:\n",
    "    print(token.text, token.i, token.lemma_, token.pos_, token.head.text, token.head.i, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When matching what to do with 'the time scale' and 'the time scale display information' or 'a project' and:\n",
    "```\n",
    "A [{0: 0}]\n",
    "project [{}]\n",
    "information [{0: 0}]\n",
    "display [{0: 0}]\n",
    "device [{0: 0}]\n",
    ", [{0: 1}]\n",
    "comprising [{}]\n",
    ": [{0: 1}]\n",
    "```\n",
    "Only look for e_0 stretches of same number with matching pos and text? (Are we now getting to look at transitions?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are issues with \"(a)\" and \"a.\"\n",
    "\n",
    "Also \"response\" from \"in response\".\n",
    "\n",
    "Check det is not working for \"at least one\"\n",
    "\n",
    "We can iterate back from where e_0 = 1 - tokens between a last noun and determinant will be part of an entity. We can then match those across the claim. This is the simple entity finder but stepping back at [:;,.] as well as DET.  \n",
    "Pattern is:\n",
    "* If next step back is e_0=0;\n",
    "* If next e_0=0 is a check_det=True;\n",
    "* Fill in inbetween as e_0=0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another pattern is \"DET X FOR [phrase]\" - this is one entity? But contains references to other entities\n",
    "```\n",
    "a [{0: 0}]\n",
    "system [{0: 0}]\n",
    "for [{}]\n",
    "providing [{}]\n",
    "a [{0: 0}]\n",
    "plurality [{0: 0}]\n",
    "of [{}]\n",
    "football [{0: 0}]\n",
    "player [{0: 0}]\n",
    "types [{0: 0}]\n",
    "from [{}]\n",
    "which [{}]\n",
    "a [{0: 0}]\n",
    "football [{0: 0}]\n",
    "player [{0: 0}]\n",
    "type [{0: 0}]\n",
    "is [{}]\n",
    "selected \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a for clause as the whole entity string - e.g. \"A method for modeling electrical characteristics of cells having given circuit elements\" and \"a layout of cells having at least one cell\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Claims - Extracting Steps\n",
    "\n",
    "Let's try something similar for method steps. This may give us some clues for \"comprising\" X, Y, Z structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive algorithm:\n",
    "* Look for a comprising relating to a method.\n",
    "* Look for VERBs following at least one of ['\\n', ';', ','] after the comprising colon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. A game system comprising:\n",
      "an\n",
      "system\n",
      "\n",
      "1. A server for providing affiliate store information\n",
      "providing\n",
      "\n",
      "1. A cover for a mobile device,\n",
      "cover\n",
      "\n",
      "1. A dynamic voltage scaling scheduling method for\n",
      "task\n",
      "\n",
      "1. A method of forming conductive traces on\n",
      "forming\n",
      "\n",
      "1. A method for a storage system,\n",
      "method\n",
      "\n",
      "1. A method comprising:\n",
      "initiating a\n",
      "method\n",
      "\n",
      "1. A power-saving method for a\n",
      "terminal\n",
      "\n",
      "1-10. (canceled)\n",
      "\n",
      "\n",
      "1. A system for automatically replacing problematic media\n",
      "files\n"
     ]
    }
   ],
   "source": [
    "for doc in nlp_docs:\n",
    "    print(doc[0:10])\n",
    "    for token in doc:\n",
    "        if token.lemma_ == \"comprise\":\n",
    "            print(token.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't really rely on a shallow naive use of the dependency pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 \n",
      " SPACE 1 1 SP  []\n",
      "1 1 1 PUNCT 1 1 LS ROOT [(\n",
      ", ''), (., 'punct')]\n",
      ". 2 . PUNCT 1 1 . punct []\n",
      "A 3 a DET method 7 DT det []\n",
      "power 4 power NOUN saving 6 NN npadvmod []\n",
      "- 5 - PUNCT saving 6 HYPH punct []\n",
      "saving 6 save VERB method 7 VBG amod [(power, 'npadvmod'), (-, 'punct')]\n",
      "method 7 method NOUN method 7 NN ROOT [(A, 'det'), (saving, 'amod'), (for, 'prep'), (., 'punct')]\n",
      "for 8 for ADP method 7 IN prep [(terminal, 'pobj')]\n",
      "a 9 a DET terminal 11 DT det []\n",
      "mobile 10 mobile ADJ terminal 11 JJ amod []\n",
      "terminal 11 terminal NOUN for 8 NN pobj [(a, 'det'), (mobile, 'amod'), (,, 'punct'), (comprising, 'acl')]\n",
      ", 12 , PUNCT terminal 11 , punct []\n",
      "comprising 13 comprise VERB terminal 11 VBG acl [(steps, 'dobj')]\n",
      "the 14 the DET steps 15 DT det []\n",
      "steps 15 step NOUN comprising 13 NNS dobj [(the, 'det'), (of, 'prep')]\n",
      "of 16 of ADP steps 15 IN prep [(:, 'punct'), (monitoring, 'acl')]\n",
      ": 17 : PUNCT of 16 : punct [(\n",
      ", '')]\n",
      "\n",
      " 18 \n",
      " SPACE : 17 SP  []\n",
      "monitoring 19 monitoring NOUN of 16 NN acl [(,, 'punct'), (by, 'prep')]\n",
      ", 20 , PUNCT monitoring 19 , punct []\n",
      "by 21 by ADP monitoring 19 IN prep [(terminal, 'pobj')]\n",
      "the 22 the DET terminal 24 DT det []\n",
      "mobile 23 mobile ADJ terminal 24 JJ amod []\n",
      "terminal 24 terminal NOUN by 21 NN pobj [(the, 'det'), (mobile, 'amod'), (,, 'punct'), (services, 'appos')]\n",
      ", 25 , PUNCT terminal 24 , punct []\n",
      "data 26 datum NOUN services 27 NNS compound []\n",
      "services 27 service NOUN terminal 24 NNS appos [(data, 'compound'), (in, 'prep'), (;, 'punct'), (triggering, 'acl')]\n",
      "in 28 in ADP services 27 IN prep [(accordance, 'pobj')]\n",
      "accordance 29 accordance NOUN in 28 NN pobj [(with, 'prep')]\n",
      "with 30 with ADP accordance 29 IN prep [(strategy, 'pobj')]\n",
      "a 31 a DET strategy 33 DT det []\n",
      "monitoring 32 monitoring NOUN strategy 33 NN compound []\n",
      "strategy 33 strategy NOUN with 30 NN pobj [(a, 'det'), (monitoring, 'compound'), (for, 'prep')]\n",
      "for 34 for ADP strategy 33 IN prep [(state, 'pobj')]\n",
      "a 35 a DET state 41 DT det []\n",
      "data 36 data NOUN service 37 NN compound []\n",
      "service 37 service NOUN state 41 NN compound [(data, 'compound')]\n",
      "power 38 power NOUN saving 40 NN compound []\n",
      "- 39 - PUNCT saving 40 HYPH punct []\n",
      "saving 40 save VERB state 41 VBG compound [(power, 'compound'), (-, 'punct')]\n",
      "state 41 state NOUN for 34 NN pobj [(a, 'det'), (service, 'compound'), (saving, 'compound')]\n",
      "; 42 ; PUNCT services 27 : punct [(\n",
      ", '')]\n",
      "\n",
      " 43 \n",
      " SPACE ; 42 SP  []\n",
      "triggering 44 trigger VERB services 27 VBG acl [(operation, 'dobj')]\n",
      "a 45 a DET operation 51 DT det []\n",
      "data 46 data NOUN service 47 NN compound []\n",
      "service 47 service NOUN operation 51 NN compound [(data, 'compound')]\n",
      "power 48 power NOUN saving 50 NN compound []\n",
      "- 49 - PUNCT saving 50 HYPH punct []\n",
      "saving 50 save VERB operation 51 VBG amod [(power, 'compound'), (-, 'punct')]\n",
      "operation 51 operation NOUN triggering 44 NN dobj [(a, 'det'), (service, 'compound'), (saving, 'amod'), (met, 'advcl'), (;, 'punct'), (and, 'cc'), (performing, 'conj')]\n",
      "when 52 when ADV met 62 WRB advmod||conj []\n",
      "a 53 a DET condition 55 DT det []\n",
      "triggering 54 trigger VERB condition 55 VBG amod []\n",
      "condition 55 condition NOUN met 62 NN nsubjpass [(a, 'det'), (triggering, 'amod'), (set, 'acl')]\n",
      "set 56 set VERB condition 55 VBN acl [(in, 'prep')]\n",
      "in 57 in ADP set 56 IN prep [(strategy, 'pobj')]\n",
      "the 58 the DET strategy 60 DT det []\n",
      "monitoring 59 monitoring NOUN strategy 60 NN compound []\n",
      "strategy 60 strategy NOUN in 57 NN pobj [(the, 'det'), (monitoring, 'compound')]\n",
      "is 61 be VERB met 62 VBZ auxpass []\n",
      "met 62 meet VERB operation 51 VBN advcl [(when, 'advmod||conj'), (condition, 'nsubjpass'), (is, 'auxpass')]\n",
      "; 63 ; PUNCT operation 51 : punct []\n",
      "and 64 and CCONJ operation 51 CC cc [(\n",
      ", '')]\n",
      "\n",
      " 65 \n",
      " SPACE and 64 SP  []\n",
      "performing 66 perform VERB operation 51 VBG conj [(,, 'punct'), (by, 'prep')]\n",
      ", 67 , PUNCT performing 66 , punct []\n",
      "by 68 by ADP performing 66 IN prep [(terminal, 'pobj')]\n",
      "the 69 the DET terminal 71 DT det []\n",
      "mobile 70 mobile ADJ terminal 71 JJ amod []\n",
      "terminal 71 terminal NOUN by 68 NN pobj [(the, 'det'), (mobile, 'amod'), (,, 'punct'), (operation, 'appos')]\n",
      ", 72 , PUNCT terminal 71 , punct []\n",
      "the 73 the DET operation 79 DT det []\n",
      "data 74 data NOUN service 75 NN compound []\n",
      "service 75 service NOUN operation 79 NN compound [(data, 'compound')]\n",
      "power 76 power NOUN saving 78 NN compound []\n",
      "- 77 - PUNCT saving 78 HYPH punct []\n",
      "saving 78 save VERB operation 79 VBG compound [(power, 'compound'), (-, 'punct')]\n",
      "operation 79 operation NOUN terminal 71 NN appos [(the, 'det'), (service, 'compound'), (saving, 'compound')]\n",
      ". 80 . PUNCT method 7 . punct [(\n",
      "\n",
      ", '')]\n",
      "\n",
      "\n",
      " 81 \n",
      "\n",
      " SPACE . 80 SP  []\n"
     ]
    }
   ],
   "source": [
    "# Look at POS and head for each token\n",
    "for token in nlp_docs[7]:\n",
    "    print(\n",
    "        token.text, token.i, token.lemma_, token.pos_, \n",
    "        token.head.text, token.head.i, token.tag_, token.dep_,\n",
    "        [(c, c.dep_) for c in token.children]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commas may be used to separate steps if spans have no internal commas? (But I sometimes write claims with comma-separated steps (e.g. substeps) that have clauses.)  \n",
    "\n",
    "Look for colon that has comprising as head then look for (semi-colon) PUNCT that have colon as head (probably the same as just finding the semi-colons!\n",
    "\n",
    "Sometimes errors in POS tagging - \n",
    "```\n",
    "monitoring 19 monitoring NOUN of 16 NN acl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 \n",
      " SPACE A 1 SP []\n",
      "A 1 a DET method 2 DT [\n",
      "]\n",
      "method 2 method NOUN method 2 NN [A, comprising, .]\n",
      "comprising 3 comprise VERB method 2 VBG [:, forming]\n",
      ": 4 : PUNCT comprising 3 : []\n",
      "forming 5 form VERB comprising 3 VBG [;, selecting]\n",
      "; 6 ; PUNCT forming 5 : []\n",
      "selecting 7 select VERB forming 5 VBG [;, and, having]\n",
      "; 8 ; PUNCT selecting 7 : []\n",
      "and 9 and CCONJ selecting 7 CC []\n",
      "having 10 have VERB selecting 7 VBG []\n",
      ". 11 . PUNCT method 2 . [\n",
      "]\n",
      "\n",
      " 12 \n",
      " SPACE . 11 SP []\n"
     ]
    }
   ],
   "source": [
    "test_doc = nlp(\"\"\"\n",
    "A method comprising: forming; selecting; and having.\n",
    "\"\"\")\n",
    "for token in test_doc:\n",
    "    print(\n",
    "        token.text, token.i, token.lemma_, token.pos_, \n",
    "        token.head.text, token.head.i, token.tag_, list(token.children)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1. A method for a storage system, the storage system including a first controller, a second controller and a plurality of storage devices, each of the first and second controllers communicatively coupled to each one of the storage devices, the method comprising:\n",
       "starting a timer that expires after a first time period; and\n",
       "subsequent to starting the timer, transmitting a first message from the first controller to a memory element shared by the first and second controllers, the first message capable of notifying the second controller of an imminent failure of the first controller, wherein subsequent to transmitting the first message to the shared memory element and before or when the timer expires, the first controller becomes unavailable to facilitate access to the storage devices.\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import PUNCT, VERB\n",
    "\n",
    "def extract_steps(doc):\n",
    "    \"\"\" Extract steps of a method claim from a spaCy doc object.\"\"\"\n",
    "    step_boundaries = list()\n",
    "    \n",
    "    # Alternative to below is to look at head of \"method\" token in claim\n",
    "    for t1 in doc:\n",
    "        if t1.lemma_ in [\"comprise\"]:\n",
    "            # Scan ahead for colon \n",
    "            for t2 in doc[t1.i+1:]:\n",
    "                if t2.lemma_ == \":\":\n",
    "                    print(\"Colon found at {0} (text='{1}')\".format(t2.i, t2))\n",
    "                    step_boundaries.append(t2)\n",
    "                    # Scan ahead to find semi-colons associated with colon\n",
    "                    for t3 in doc[t2.i+1:]:\n",
    "                        if t3.pos == PUNCT and t3.tag_ in [\":\", \".\"]:\n",
    "                            step_boundaries.append(t3)\n",
    "                    break\n",
    "            break\n",
    "    \n",
    "    print(\"Step boundaries are {0}\".format(step_boundaries))\n",
    "    \n",
    "    # Find first verb after each in set [colon, semi-colon]\n",
    "    step_verbs = list()\n",
    "    for sb in step_boundaries[:-1]:\n",
    "        for t1 in doc[sb.i+1:]:\n",
    "            if t1.pos == VERB and t1.tag_ == \"VBG\":\n",
    "                # sb is previous step boundary - we want next step boundary\n",
    "                step_verbs.append(t1)\n",
    "                break  \n",
    "    \n",
    "    print(\"Step verbs are {0}\".format(step_verbs))\n",
    "    \n",
    "    steps = list()\n",
    "    # Tada set of method steps\n",
    "    for sv, sb in zip(step_verbs, step_boundaries[1:]):\n",
    "        print(\"Step verb is {0} with lemma {1}\".format(sv, sv.lemma_))\n",
    "        print(\"Step text is {0}\".format(doc[sv.i:sb.i].text))\n",
    "        steps.append((sv, sb))\n",
    "        \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pass - entity label heuristics\n",
      "\n",
      " [{0: 1}]\n",
      "1 [{}]\n",
      ". [{0: 1}]\n",
      "A [{0: 0}]\n",
      "system [{0: 0}]\n",
      "for [{0: 1}]\n",
      "automatically [{}]\n",
      "replacing [{}]\n",
      "problematic [{}]\n",
      "media [{0: 0}]\n",
      "files [{0: 0}]\n",
      "comprising [{0: 1}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "a [{0: 0}]\n",
      "first [{0: 0}]\n",
      "media [{0: 0}]\n",
      "store [{0: 0}]\n",
      "configured [{0: 1}]\n",
      "to [{0: 1}]\n",
      "store [{}]\n",
      "a [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{0: 0}]\n",
      "digitally [{0: 0}]\n",
      "encoded [{}]\n",
      "local [{}]\n",
      "media [{0: 0}]\n",
      "files [{0: 0}]\n",
      "; [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "a [{0: 0}]\n",
      "second [{0: 0}]\n",
      "media [{0: 0}]\n",
      "store [{0: 0}]\n",
      "configured [{0: 1}]\n",
      "to [{0: 1}]\n",
      "store [{}]\n",
      "a [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{0: 0}]\n",
      "digitally [{0: 0}]\n",
      "encoded [{}]\n",
      "source [{0: 0}]\n",
      "media [{0: 0}]\n",
      "files [{0: 0}]\n",
      "; [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "a [{0: 0}]\n",
      "media [{0: 0}]\n",
      "diagnostic [{}]\n",
      "engine [{0: 0}]\n",
      "configured [{0: 1}]\n",
      "to [{0: 1}]\n",
      "identify [{}]\n",
      "whether [{}]\n",
      "problems [{0: 0}]\n",
      "exist [{}]\n",
      "within [{}]\n",
      "one [{}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "media [{0: 0}]\n",
      "files [{0: 0}]\n",
      "located [{}]\n",
      "in [{}]\n",
      "the [{0: 0}]\n",
      "first [{0: 0}]\n",
      "media [{0: 0}]\n",
      "store [{0: 0}]\n",
      ", [{0: 1}]\n",
      "wherein [{0: 1}]\n",
      "when [{}]\n",
      "a [{0: 0}]\n",
      "problem [{0: 0}]\n",
      "is [{0: 1}]\n",
      "identified [{}]\n",
      ", [{0: 1}]\n",
      "the [{0: 0}]\n",
      "associated [{0: 0}]\n",
      "file [{0: 0}]\n",
      "having [{0: 1}]\n",
      "the [{0: 0}]\n",
      "problem [{0: 0}]\n",
      "is [{0: 1}]\n",
      "able [{}]\n",
      "to [{}]\n",
      "be [{0: 1}]\n",
      "referred [{}]\n",
      "to [{}]\n",
      "as [{}]\n",
      "a [{0: 0}]\n",
      "problematic [{0: 0}]\n",
      "file [{0: 0}]\n",
      ", [{0: 1}]\n",
      "wherein [{0: 1}]\n",
      "the [{0: 0}]\n",
      "media [{0: 0}]\n",
      "diagnostic [{}]\n",
      "engine [{0: 0}]\n",
      "is [{0: 1}]\n",
      "configured [{0: 1}]\n",
      "to [{0: 1}]\n",
      "identify [{}]\n",
      "whether [{}]\n",
      "problems [{0: 0}]\n",
      "exist [{}]\n",
      "based [{}]\n",
      "upon [{}]\n",
      "at [{0: 0}]\n",
      "least [{0: 0}]\n",
      "one [{0: 0}]\n",
      "of [{}]\n",
      "a [{0: 0}]\n",
      "user [{0: 0}]\n",
      "input [{0: 0}]\n",
      "and [{}]\n",
      "a [{0: 0}]\n",
      "software [{0: 0}]\n",
      "based [{}]\n",
      "error [{0: 0}]\n",
      "detection [{0: 0}]\n",
      "algorithm [{0: 0}]\n",
      "determination [{0: 0}]\n",
      "; [{0: 1}]\n",
      "and [{}]\n",
      "\n",
      " [{0: 1}]\n",
      "a [{0: 0}]\n",
      "media [{0: 0}]\n",
      "replacement [{0: 0}]\n",
      "engine [{0: 0}]\n",
      "configured [{0: 1}]\n",
      "to [{0: 1}]\n",
      "replace [{}]\n",
      "a [{0: 0}]\n",
      "problematic [{0: 0}]\n",
      "file [{0: 0}]\n",
      "in [{}]\n",
      "the [{0: 0}]\n",
      "first [{0: 0}]\n",
      "media [{0: 0}]\n",
      "store [{0: 0}]\n",
      "with [{}]\n",
      "a [{0: 0}]\n",
      "copy [{0: 0}]\n",
      "of [{0: 0}]\n",
      "a [{0: 0}]\n",
      "corresponding [{0: 0}]\n",
      "media [{0: 0}]\n",
      "file [{0: 0}]\n",
      "from [{}]\n",
      "the [{0: 0}]\n",
      "second [{0: 0}]\n",
      "media [{0: 0}]\n",
      "store [{0: 0}]\n",
      ". [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "Second pass - look for DET ... NOUN groupings\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      ". is e_0=1 or DET - looking back\n",
      "Step back token - 1 with pos - 95\n",
      "Setting non-Noun\n",
      "A is e_0=1 or DET - looking back\n",
      "Step back token - . with pos - 95\n",
      "Setting non-Noun\n",
      "Step back token - 1 with pos - 95\n",
      "Setting non-Noun\n",
      "for is e_0=1 or DET - looking back\n",
      "Step back token - system with pos - 90\n",
      "Located plural noun: media\n",
      "problematic 10 {}\n",
      "replacing 5 {}\n",
      "automatically 7 {}\n",
      "for 4 {0: 1}\n",
      "Located plural noun: files\n",
      "media 10 {0: 0}\n",
      "comprising is e_0=1 or DET - looking back\n",
      "Step back token - files with pos - 90\n",
      ": is e_0=1 or DET - looking back\n",
      "Step back token - comprising with pos - 98\n",
      "Setting non-Noun\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      "Step back token - : with pos - 95\n",
      "Setting non-Noun\n",
      "Step back token - comprising with pos - 98\n",
      "Setting non-Noun\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - \n",
      " with pos - 101\n",
      "Setting non-Noun\n",
      "Step back token - : with pos - 95\n",
      "Setting non-Noun\n",
      "Step back token - comprising with pos - 98\n",
      "Setting non-Noun\n",
      "configured is e_0=1 or DET - looking back\n",
      "Step back token - store with pos - 90\n",
      "to is e_0=1 or DET - looking back\n",
      "Step back token - configured with pos - 98\n",
      "Setting non-Noun\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - store with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - to with pos - 92\n",
      "Setting non-Noun\n",
      "Step back token - configured with pos - 98\n",
      "Setting non-Noun\n",
      "Located plural noun: files\n",
      "media 28 {0: 0}\n",
      "; is e_0=1 or DET - looking back\n",
      "Step back token - files with pos - 90\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - \n",
      " with pos - 101\n",
      "Setting non-Noun\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "configured is e_0=1 or DET - looking back\n",
      "Step back token - store with pos - 90\n",
      "to is e_0=1 or DET - looking back\n",
      "Step back token - configured with pos - 98\n",
      "Setting non-Noun\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - store with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - to with pos - 92\n",
      "Setting non-Noun\n",
      "Step back token - configured with pos - 98\n",
      "Setting non-Noun\n",
      "Located plural noun: files\n",
      "media 45 {0: 0}\n",
      "; is e_0=1 or DET - looking back\n",
      "Step back token - files with pos - 90\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - \n",
      " with pos - 101\n",
      "Setting non-Noun\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "configured is e_0=1 or DET - looking back\n",
      "Step back token - engine with pos - 90\n",
      "to is e_0=1 or DET - looking back\n",
      "Step back token - configured with pos - 98\n",
      "Setting non-Noun\n",
      "Located plural noun: problems\n",
      "whether 57 {}\n",
      "identify 52 {}\n",
      "to 54 {0: 1}\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - of with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - one with pos - 91\n",
      "Setting non-Noun\n",
      "Step back token - within with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - exist with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - problems with pos - 90\n",
      "Located plural noun: media\n",
      "the 63 {0: 0}\n",
      "Located plural noun: files\n",
      "media 63 {0: 0}\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - in with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - located with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - files with pos - 90\n",
      ", is e_0=1 or DET - looking back\n",
      "Step back token - store with pos - 90\n",
      "wherein is e_0=1 or DET - looking back\n",
      "Step back token - , with pos - 95\n",
      "Setting non-Noun\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - when with pos - 84\n",
      "Setting non-Noun\n",
      "Step back token - wherein with pos - 87\n",
      "Setting non-Noun\n",
      "Step back token - , with pos - 95\n",
      "Setting non-Noun\n",
      "is is e_0=1 or DET - looking back\n",
      "Step back token - problem with pos - 90\n",
      ", is e_0=1 or DET - looking back\n",
      "Step back token - identified with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - is with pos - 98\n",
      "Setting non-Noun\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - , with pos - 95\n",
      "Setting non-Noun\n",
      "Step back token - identified with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - is with pos - 98\n",
      "Setting non-Noun\n",
      "having is e_0=1 or DET - looking back\n",
      "Step back token - file with pos - 90\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - having with pos - 98\n",
      "Setting non-Noun\n",
      "is is e_0=1 or DET - looking back\n",
      "Step back token - problem with pos - 90\n",
      "be is e_0=1 or DET - looking back\n",
      "Step back token - to with pos - 92\n",
      "Setting non-Noun\n",
      "Step back token - able with pos - 82\n",
      "Setting non-Noun\n",
      "Step back token - is with pos - 98\n",
      "Setting non-Noun\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - as with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - to with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - referred with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - be with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - to with pos - 92\n",
      "Setting non-Noun\n",
      "Step back token - able with pos - 82\n",
      "Setting non-Noun\n",
      "Step back token - is with pos - 98\n",
      "Setting non-Noun\n",
      ", is e_0=1 or DET - looking back\n",
      "Step back token - file with pos - 90\n",
      "wherein is e_0=1 or DET - looking back\n",
      "Step back token - , with pos - 95\n",
      "Setting non-Noun\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - wherein with pos - 84\n",
      "Setting non-Noun\n",
      "Step back token - , with pos - 95\n",
      "Setting non-Noun\n",
      "Located plural noun: media\n",
      "the 99 {0: 0}\n",
      "is is e_0=1 or DET - looking back\n",
      "Step back token - engine with pos - 90\n",
      "configured is e_0=1 or DET - looking back\n",
      "Step back token - is with pos - 98\n",
      "Setting non-Noun\n",
      "to is e_0=1 or DET - looking back\n",
      "Step back token - configured with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - is with pos - 98\n",
      "Setting non-Noun\n",
      "Located plural noun: problems\n",
      "whether 106 {}\n",
      "identify 101 {}\n",
      "to 103 {0: 1}\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - of with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - one with pos - 91\n",
      "Setting non-Noun\n",
      "Step back token - least with pos - 84\n",
      "Setting non-Noun\n",
      "Step back token - at with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - upon with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - based with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - exist with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - problems with pos - 90\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - and with pos - 87\n",
      "Setting non-Noun\n",
      "Step back token - input with pos - 90\n",
      "; is e_0=1 or DET - looking back\n",
      "Step back token - determination with pos - 90\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      "Step back token - and with pos - 87\n",
      "Setting non-Noun\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - \n",
      " with pos - 101\n",
      "Setting non-Noun\n",
      "Step back token - and with pos - 87\n",
      "Setting non-Noun\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "configured is e_0=1 or DET - looking back\n",
      "Step back token - engine with pos - 90\n",
      "to is e_0=1 or DET - looking back\n",
      "Step back token - configured with pos - 98\n",
      "Setting non-Noun\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - replace with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - to with pos - 92\n",
      "Setting non-Noun\n",
      "Step back token - configured with pos - 98\n",
      "Setting non-Noun\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - in with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - file with pos - 90\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - with with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - store with pos - 90\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - of with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - copy with pos - 90\n",
      "Located plural noun: media\n",
      "corresponding 149 {0: 0}\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - from with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - file with pos - 90\n",
      ". is e_0=1 or DET - looking back\n",
      "Step back token - store with pos - 90\n",
      "\n",
      " [{0: 1}]\n",
      "1 [{0: 1}]\n",
      ". [{0: 1}]\n",
      "A [{0: 0}]\n",
      "system [{0: 0}]\n",
      "for [{0: 1}]\n",
      "automatically [{}]\n",
      "replacing [{}]\n",
      "problematic [{}]\n",
      "media [{0: 0}]\n",
      "files [{0: 0}]\n",
      "comprising [{0: 1}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "a [{0: 0}]\n",
      "first [{0: 0}]\n",
      "media [{0: 0}]\n",
      "store [{0: 0}]\n",
      "configured [{0: 1}]\n",
      "to [{0: 1}]\n",
      "store [{0: 1}]\n",
      "a [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{0: 0}]\n",
      "digitally [{0: 0}]\n",
      "encoded [{}]\n",
      "local [{}]\n",
      "media [{0: 0}]\n",
      "files [{0: 0}]\n",
      "; [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "a [{0: 0}]\n",
      "second [{0: 0}]\n",
      "media [{0: 0}]\n",
      "store [{0: 0}]\n",
      "configured [{0: 1}]\n",
      "to [{0: 1}]\n",
      "store [{0: 1}]\n",
      "a [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{0: 0}]\n",
      "digitally [{0: 0}]\n",
      "encoded [{}]\n",
      "source [{0: 0}]\n",
      "media [{0: 0}]\n",
      "files [{0: 0}]\n",
      "; [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "a [{0: 0}]\n",
      "media [{0: 0}]\n",
      "diagnostic [{0: 0}]\n",
      "engine [{0: 0}]\n",
      "configured [{0: 1}]\n",
      "to [{0: 1}]\n",
      "identify [{}]\n",
      "whether [{}]\n",
      "problems [{0: 0}]\n",
      "exist [{0: 1}]\n",
      "within [{0: 1}]\n",
      "one [{0: 1}]\n",
      "of [{0: 1}]\n",
      "the [{0: 0}]\n",
      "media [{0: 0}]\n",
      "files [{0: 0}]\n",
      "located [{0: 1}]\n",
      "in [{0: 1}]\n",
      "the [{0: 0}]\n",
      "first [{0: 0}]\n",
      "media [{0: 0}]\n",
      "store [{0: 0}]\n",
      ", [{0: 1}]\n",
      "wherein [{0: 1}]\n",
      "when [{0: 1}]\n",
      "a [{0: 0}]\n",
      "problem [{0: 0}]\n",
      "is [{0: 1}]\n",
      "identified [{0: 1}]\n",
      ", [{0: 1}]\n",
      "the [{0: 0}]\n",
      "associated [{0: 0}]\n",
      "file [{0: 0}]\n",
      "having [{0: 1}]\n",
      "the [{0: 0}]\n",
      "problem [{0: 0}]\n",
      "is [{0: 1}]\n",
      "able [{0: 1}]\n",
      "to [{0: 1}]\n",
      "be [{0: 1}]\n",
      "referred [{0: 1}]\n",
      "to [{0: 1}]\n",
      "as [{0: 1}]\n",
      "a [{0: 0}]\n",
      "problematic [{0: 0}]\n",
      "file [{0: 0}]\n",
      ", [{0: 1}]\n",
      "wherein [{0: 1}]\n",
      "the [{0: 0}]\n",
      "media [{0: 0}]\n",
      "diagnostic [{0: 0}]\n",
      "engine [{0: 0}]\n",
      "is [{0: 1}]\n",
      "configured [{0: 1}]\n",
      "to [{0: 1}]\n",
      "identify [{}]\n",
      "whether [{}]\n",
      "problems [{0: 0}]\n",
      "exist [{0: 1}]\n",
      "based [{0: 1}]\n",
      "upon [{0: 1}]\n",
      "at [{0: 1}]\n",
      "least [{0: 1}]\n",
      "one [{0: 1}]\n",
      "of [{0: 1}]\n",
      "a [{0: 0}]\n",
      "user [{0: 0}]\n",
      "input [{0: 0}]\n",
      "and [{0: 1}]\n",
      "a [{0: 0}]\n",
      "software [{0: 0}]\n",
      "based [{0: 0}]\n",
      "error [{0: 0}]\n",
      "detection [{0: 0}]\n",
      "algorithm [{0: 0}]\n",
      "determination [{0: 0}]\n",
      "; [{0: 1}]\n",
      "and [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "a [{0: 0}]\n",
      "media [{0: 0}]\n",
      "replacement [{0: 0}]\n",
      "engine [{0: 0}]\n",
      "configured [{0: 1}]\n",
      "to [{0: 1}]\n",
      "replace [{0: 1}]\n",
      "a [{0: 0}]\n",
      "problematic [{0: 0}]\n",
      "file [{0: 0}]\n",
      "in [{0: 1}]\n",
      "the [{0: 0}]\n",
      "first [{0: 0}]\n",
      "media [{0: 0}]\n",
      "store [{0: 0}]\n",
      "with [{0: 1}]\n",
      "a [{0: 0}]\n",
      "copy [{0: 0}]\n",
      "of [{0: 1}]\n",
      "a [{0: 0}]\n",
      "corresponding [{0: 0}]\n",
      "media [{0: 0}]\n",
      "file [{0: 0}]\n",
      "from [{0: 1}]\n",
      "the [{0: 0}]\n",
      "second [{0: 0}]\n",
      "media [{0: 0}]\n",
      "store [{0: 0}]\n",
      ". [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "automatically [{}]\n",
      "replacing [{}]\n",
      "problematic [{}]\n",
      "encoded [{}]\n",
      "local [{}]\n",
      "encoded [{}]\n",
      "identify [{}]\n",
      "whether [{}]\n",
      "identify [{}]\n",
      "whether [{}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "Extracted possible occurrences:\n",
      "\n",
      "[A system, , , , media files, a first media store, a plurality of digitally, , media files, a second media store, a plurality of digitally, source media files, a media diagnostic engine, , , problems, the media files, the first media store, a problem, the associated file, the problem, a problematic file, the media diagnostic engine, , , problems, a user input, a software based error detection algorithm determination, a media replacement engine, a problematic file, the first media store, a copy, a corresponding media file, the second media store, ]\n",
      "\n",
      "1. A system for automatically replacing problematic media files comprising:\n",
      "a first media store configured to store a plurality of digitally encoded local media files;\n",
      "a second media store configured to store a plurality of digitally encoded source media files;\n",
      "a media diagnostic engine configured to identify whether problems exist within one of the media files located in the first media store, wherein when a problem is identified, the associated file having the problem is able to be referred to as a problematic file, wherein the media diagnostic engine is configured to identify whether problems exist based upon at least one of a user input and a software based error detection algorithm determination; and\n",
      "a media replacement engine configured to replace a problematic file in the first media store with a copy of a corresponding media file from the second media store.\n",
      "\n",
      "\n",
      "OrderedDict([('system', [A system]), ('media files', [media files, media files, the media files]), ('first media store', [a first media store, the first media store, the first media store]), ('plurality of digitally', [a plurality of digitally, a plurality of digitally]), ('second media store', [a second media store, the second media store]), ('source media files', [source media files]), ('media diagnostic engine', [a media diagnostic engine, the media diagnostic engine]), ('problems', [problems, problems]), ('problem', [a problem, the problem]), ('associated file', [the associated file]), ('problematic file', [a problematic file, a problematic file]), ('user input', [a user input]), ('software based error detection algorithm determination', [a software based error detection algorithm determination]), ('media replacement engine', [a media replacement engine]), ('copy', [a copy]), ('corresponding media file', [a corresponding media file])])\n",
      "Found entity 'the associated file' with incorrect antecedence\n",
      "[]\n",
      "OrderedDict([('system', [A system]), ('media files', [media files, media files, the media files]), ('first media store', [a first media store, the first media store, the first media store]), ('plurality of digitally', [a plurality of digitally, a plurality of digitally]), ('second media store', [a second media store, the second media store]), ('source media files', [source media files]), ('media diagnostic engine', [a media diagnostic engine, the media diagnostic engine]), ('problems', [problems, problems]), ('problem', [a problem, the problem]), ('associated file', [the associated file]), ('problematic file', [a problematic file, a problematic file]), ('user input', [a user input]), ('software based error detection algorithm determination', [a software based error detection algorithm determination]), ('media replacement engine', [a media replacement engine]), ('copy', [a copy]), ('corresponding media file', [a corresponding media file])])\n",
      "\n",
      "Claim is not method claim\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_docs[9]\n",
    "\n",
    "entity_dict = extract_entities(doc)\n",
    "\n",
    "if \"method\" in list(entity_dict.keys())[0]:\n",
    "    print(\"\\nClaim is method claim\")\n",
    "    steps = extract_steps(doc)\n",
    "    print(\"\\n\", steps)\n",
    "else:\n",
    "    print(\"\\nClaim is not method claim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn into a flow chart we can use:\n",
    "* https://pygraphviz.github.io/examples.html\n",
    "* https://www.sharelatex.com/blog/2013/08/29/tikz-series-pt3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues:\n",
    "* Method claims that have a system comprising in the pre-amble (these are generally bad for US though).\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Extracted Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Wrapper Class\n",
    "\n",
    "It may be easier to deal with entities if we create a python class to model them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Entity:\n",
    "    \"\"\" Abstract object for instantiating entities. \n",
    "    \n",
    "    Attributes:\n",
    "        ref_num - int representing associated reference number (maybe a list?)\n",
    "        parent (? - or get from navigating children)\n",
    "        children\n",
    "        limitations\n",
    "        essential - T or F (optional = F)\n",
    "        number - (default = 1, >1 = plurality) \n",
    "        order - if in a set of children where it comes in the claim\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, string_name, occurrences=[]):\n",
    "        \"\"\" Initialise object. \"\"\"\n",
    "        self.name = string_name\n",
    "        self.occurrences = occurrences\n",
    "        self.children = list()\n",
    "        self.limitations = list()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"<Entity - name: {n}; \" \n",
    "            \"occurrences: {o}; \" \n",
    "            \"children: {c}; \" \n",
    "            \"limitations: {l}\"\n",
    "        ).format(\n",
    "            n = self.name,\n",
    "            o = self.occurrences,\n",
    "            c = self.children,\n",
    "            l = self.limitations\n",
    "        )\n",
    "        \n",
    "    def add_occurrence(self, spacy_span):\n",
    "        \"\"\" Add an occurrence in the form of a spaCy span\"\"\"\n",
    "        self.occurrences.append(spacy_span)\n",
    "        return self.occurrences\n",
    "    \n",
    "    @property\n",
    "    def first_occurrence(self):\n",
    "        \"\"\" Return starting index of first occurrence.\"\"\"\n",
    "        return min([o[0].i for o in self.occurrences])\n",
    "        \n",
    "    def add_child(self, child):\n",
    "        \"\"\" Add a child entity.\n",
    "        \n",
    "        child: an object of the same class\n",
    "        \"\"\"\n",
    "        # if string convert to ClaimFeature\n",
    "        if isinstance(child, str):\n",
    "            child = type(self)(child)\n",
    "        assert isinstance(child, type(self))\n",
    "        self.children.append(child)\n",
    "        return self.children\n",
    "    \n",
    "    def remove_child(self, child):\n",
    "        \"\"\" Remove a child entity.\n",
    "        \n",
    "        child: a child object to remove\n",
    "        \"\"\"\n",
    "        self.children.remove(child)\n",
    "        return self.children\n",
    "    \n",
    "    def add_limitation(self, limitation):\n",
    "        \"\"\" Add a limitation.\n",
    "        \n",
    "        limitation: a Limitation object\n",
    "        \"\"\"\n",
    "        if isinstance(limitation, str):\n",
    "            limitation = Limitation(limitation)\n",
    "        assert isinstance(limitation, Limitation)\n",
    "        self.limitations.append(limitation)\n",
    "        return self.limitations\n",
    "    \n",
    "    def remove_limitation(self, limitation):\n",
    "        \"\"\" Remove a limitation.\n",
    "        \n",
    "       limitation: a Limitation object\n",
    "        \"\"\"\n",
    "        self.limitations.remove(limitation)\n",
    "        return self.limitations\n",
    "    \n",
    "    def prettyprint(self, object_str_single, object_str_plural, tabs=0):\n",
    "        \"\"\" Pretty print a representation of feature with\n",
    "        children and limitations.\n",
    "        \n",
    "        object_str_single: string to call feature instantiation \n",
    "        object_str_plural: string to call feature instantiation (plural)\n",
    "        \"\"\"\n",
    "        tabtext = \"\\t\"*tabs\n",
    "        print(\"{0}{1}: {2}\\n\".format(tabtext, object_str_single, self.__repr__()))\n",
    "        \n",
    "        if self.limitations:\n",
    "            tabs = tabs + 1\n",
    "            tabtext = \"\\t\"*tabs\n",
    "            print(\"{0}Limitations:\\n\".format(tabtext))\n",
    "            for i, limitation in enumerate(self.limitations):\n",
    "                print(\"\\t{0}{1} - {2}\\n\".format(tabtext, i, limitation.__repr__()))\n",
    "        \n",
    "        if self.children:\n",
    "            tabs = tabs + 1\n",
    "            print(\"{0}Child {1}:\\n\".format(tabtext, object_str_plural))\n",
    "            for i, child in enumerate(self.children):\n",
    "                child.prettyprint(tabs=tabs)\n",
    "                \n",
    "class Claim:\n",
    "    \"\"\" Abstract object to represent a claim. \n",
    "    \n",
    "    Attributes:\n",
    "        entities\n",
    "    \"\"\"\n",
    "    # Merge this later with our claim object\n",
    "    def __init__(self, entities=[]):\n",
    "        \"\"\" Initialise object.\n",
    "        \n",
    "        features - list of instantiated Entity objects\n",
    "        \"\"\"\n",
    "        self.entities = entities\n",
    "        \n",
    "    def from_entity_dict(self, ed):\n",
    "        \"\"\" Populate claim entities from a dictionary ed.\"\"\"\n",
    "        for k, v in ed.items():\n",
    "            self.entities.append(\n",
    "                Entity(\n",
    "                    string_name=k,\n",
    "                    occurrences=v\n",
    "                )\n",
    "            )\n",
    "        return self.entities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Entity - name: system; occurrences: [A system]; children: []; limitations: [],\n",
       " <Entity - name: media files; occurrences: [media files, media files, the media files]; children: []; limitations: [],\n",
       " <Entity - name: first media store; occurrences: [a first media store, the first media store, the first media store]; children: []; limitations: [],\n",
       " <Entity - name: plurality of digitally; occurrences: [a plurality of digitally, a plurality of digitally]; children: []; limitations: [],\n",
       " <Entity - name: second media store; occurrences: [a second media store, the second media store]; children: []; limitations: [],\n",
       " <Entity - name: source media files; occurrences: [source media files]; children: []; limitations: [],\n",
       " <Entity - name: media diagnostic engine; occurrences: [a media diagnostic engine, the media diagnostic engine]; children: []; limitations: [],\n",
       " <Entity - name: problems; occurrences: [problems, problems]; children: []; limitations: [],\n",
       " <Entity - name: problem; occurrences: [a problem, the problem]; children: []; limitations: [],\n",
       " <Entity - name: associated file; occurrences: [the associated file]; children: []; limitations: [],\n",
       " <Entity - name: problematic file; occurrences: [a problematic file, a problematic file]; children: []; limitations: [],\n",
       " <Entity - name: user input; occurrences: [a user input]; children: []; limitations: [],\n",
       " <Entity - name: software based error detection algorithm determination; occurrences: [a software based error detection algorithm determination]; children: []; limitations: [],\n",
       " <Entity - name: media replacement engine; occurrences: [a media replacement engine]; children: []; limitations: [],\n",
       " <Entity - name: copy; occurrences: [a copy]; children: []; limitations: [],\n",
       " <Entity - name: corresponding media file; occurrences: [a corresponding media file]; children: []; limitations: []]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim1 = Claim()\n",
    "claim1.from_entity_dict(entity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May not be able to use dependency information from spaCy for looking at structure - link between \"comprising\" and subsequent features appear lost over long claim text.  \n",
    "\n",
    "Head of verb will give you the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system 3\n",
      "media files 9\n",
      "first media store 14\n",
      "plurality of digitally 21\n",
      "second media store 31\n",
      "source media files 43\n",
      "media diagnostic engine 48\n",
      "problems 56\n",
      "problem 73\n",
      "associated file 78\n",
      "problematic file 91\n",
      "user input 113\n",
      "software based error detection algorithm determination 117\n",
      "media replacement engine 127\n",
      "copy 143\n",
      "corresponding media file 146\n"
     ]
    }
   ],
   "source": [
    "for entity in claim1.entities:\n",
    "    print(entity.name, entity.first_occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------\n",
      " [{'endindex': 30, 'text': '\\n1. A game system comprising:\\n', 'startindex': 0}, {'endindex': 89, 'text': 'an interactive apparatus having a communication system; and', 'startindex': 30}, {'endindex': 403, 'text': '\\nan interactive application, the interactive application and the interactive apparatus are independently operable to provide an offline and an online experience, wherein at least one of the interactive application and interactive apparatus is configured modify its operation based on the experience of the other.\\n\\n', 'startindex': 89}]\n",
      "\n",
      "------\n",
      " [{'endindex': 68, 'text': '\\n1. A server for providing affiliate store information, comprising:\\n', 'startindex': 0}, {'endindex': 271, 'text': 'a receiver for collecting transaction count information relating to a number of transactions made in each affiliate store and receiving search condition information from a mobile communication terminal;\\n', 'startindex': 68}, {'endindex': 424, 'text': 'a database for storing affiliate store information generated based on the collected transaction count information according to a predetermined criteria;\\n', 'startindex': 271}, {'endindex': 662, 'text': 'a generator for extracting the affiliate store information of at least one affiliate store from the database according to the search condition information and generating a search result using the extracted affiliate store information; and', 'startindex': 424}, {'endindex': 751, 'text': '\\na transmitter for transmitting the search result to the mobile communication terminal.\\n\\n', 'startindex': 662}]\n",
      "\n",
      "------\n",
      " [{'endindex': 45, 'text': '\\n1. A cover for a mobile device, comprising:\\n', 'startindex': 0}, {'endindex': 150, 'text': 'side surfaces configured to be adjacent at least a portion one or more side surfaces of a mobile device;\\n', 'startindex': 45}, {'endindex': 395, 'text': 'a rear surface configured to be adjacent at least a portion of a rear surface of the mobile device and connected to the side surfaces, the side surfaces and the rear surface form an opening that receives at least a portion of the mobile device;\\n', 'startindex': 150}, {'endindex': 461, 'text': 'a connector configured to connect to a port of the mobile device;\\n', 'startindex': 395}, {'endindex': 529, 'text': 'a circuit that connects the connector to a communication module; and', 'startindex': 461}, {'endindex': 694, 'text': '\\nthe communication module configured to execute transactions with contactless devices and communicate with the mobile device through the circuit and the connector.\\n\\n', 'startindex': 529}]\n",
      "\n",
      "------\n",
      " [{'endindex': 162, 'text': '\\n1. A dynamic voltage scaling scheduling method for resource-sharing and hard real-time tasks, applicable for scheduling tasks in a delayed task set, comprising:\\n', 'startindex': 0}, {'endindex': 389, 'text': 'determining a property of a task, and executing one of the following steps, when the task belongs to the delayed task set or the task does not belong to the task collection but a waiting time has exceeded a period of the task;\\n', 'startindex': 162}, {'endindex': 630, 'text': 'when one task in the delayed task set requires for being executed, increasing a working voltage required for executing the task, removing the task from the delayed task set, and returning to the step of determining the property of the task;\\n', 'startindex': 389}, {'endindex': 935, 'text': 'when one task in the delayed task set requires for sharing resources, setting the working voltage required by the task as a current working voltage or as a larger one in least upper bounds of all tasks requiring for sharing resources, and returning to the step of determining the property of the task; and', 'startindex': 630}, {'endindex': 1237, 'text': '\\nwhen one task not belonging to the delayed task set exists, and the waiting time of the task has exceeded the period of the task, reducing the working voltage required for executing the task, adding the task in the delayed task set, and returning to the step of determining the property of the task.\\n\\n', 'startindex': 935}]\n",
      "\n",
      "------\n",
      " [{'endindex': 79, 'text': '\\n1. A method of forming conductive traces on a touch sensor panel, comprising:\\n', 'startindex': 0}, {'endindex': 446, 'text': 'forming and patterning a stackup of a first conductive material over a substrate in one or more border areas of the touch sensor panel to create one or more traces having widths maximized so that the one or more traces and any separation areas between the traces in any particular portion of the border area occupy a full width of that portion of the border area; and', 'startindex': 79}, {'endindex': 672, 'text': '\\nforming and patterning a layer of a second conductive material over the substrate to create one or more rows, each row coupled to a different trace, the rows forming part of a plurality of sensors on the touch sensor panel.\\n\\n', 'startindex': 446}]\n",
      "\n",
      "------\n",
      " [{'endindex': 264, 'text': '\\n1. A method for a storage system, the storage system including a first controller, a second controller and a plurality of storage devices, each of the first and second controllers communicatively coupled to each one of the storage devices, the method comprising:\\n', 'startindex': 0}, {'endindex': 324, 'text': 'starting a timer that expires after a first time period; and', 'startindex': 264}, {'endindex': 794, 'text': '\\nsubsequent to starting the timer, transmitting a first message from the first controller to a memory element shared by the first and second controllers, the first message capable of notifying the second controller of an imminent failure of the first controller, wherein subsequent to transmitting the first message to the shared memory element and before or when the timer expires, the first controller becomes unavailable to facilitate access to the storage devices.\\n\\n', 'startindex': 324}]\n",
      "\n",
      "------\n",
      " [{'endindex': 25, 'text': '\\n1. A method comprising:\\n', 'startindex': 0}, {'endindex': 188, 'text': 'initiating a first interaction of a payment cardholder computer system executing a browser application with an item of content associated with a merchant Website;\\n', 'startindex': 25}, {'endindex': 399, 'text': 'before completion of the first interaction, initiating a proxy interaction of the payment cardholder computer system executing a browser application with an item of content associated with the merchant Website;\\n', 'startindex': 188}, {'endindex': 479, 'text': 'communicating with a payment card network connected to the merchant Website; and', 'startindex': 399}, {'endindex': 596, 'text': '\\nusing the proxy interaction to determine whether the merchant Website is registered with the payment card network.\\n\\n', 'startindex': 479}]\n",
      "\n",
      "------\n",
      " [{'endindex': 74, 'text': '\\n1. A power-saving method for a mobile terminal, comprising the steps of:\\n', 'startindex': 0}, {'endindex': 204, 'text': 'monitoring, by the mobile terminal, data services in accordance with a monitoring strategy for a data service power-saving state;\\n', 'startindex': 74}, {'endindex': 323, 'text': 'triggering a data service power-saving operation when a triggering condition set in the monitoring strategy is met; and', 'startindex': 204}, {'endindex': 402, 'text': '\\nperforming, by the mobile terminal, the data service power-saving operation.\\n\\n', 'startindex': 323}]\n",
      "\n",
      "------\n",
      " []\n",
      "\n",
      "------\n",
      " [{'endindex': 77, 'text': '\\n1. A system for automatically replacing problematic media files comprising:\\n', 'startindex': 0}, {'endindex': 169, 'text': 'a first media store configured to store a plurality of digitally encoded local media files;\\n', 'startindex': 77}, {'endindex': 263, 'text': 'a second media store configured to store a plurality of digitally encoded source media files;\\n', 'startindex': 169}, {'endindex': 715, 'text': 'a media diagnostic engine configured to identify whether problems exist within one of the media files located in the first media store, wherein when a problem is identified, the associated file having the problem is able to be referred to as a problematic file, wherein the media diagnostic engine is configured to identify whether problems exist based upon at least one of a user input and a software based error detection algorithm determination; and', 'startindex': 263}, {'endindex': 881, 'text': '\\na media replacement engine configured to replace a problematic file in the first media store with a copy of a corresponding media file from the second media store.\\n\\n', 'startindex': 715}]\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    claim1 = d.claimset.get_claim(1)\n",
    "    print(\"\\n------\\n\", claim1.split_into_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_relationships(doc):\n",
    "    \"\"\" Extract relationships between claim entities from a spaCy doc object.\"\"\"\n",
    "    \n",
    "    entity_dict = extract_entities(doc)\n",
    "    \n",
    "    entity_boundaries = list()\n",
    "    \n",
    "    # Alternative to below is to look at head of first entity in claim (but does not reliably point to comprising)\n",
    "    for t1 in doc:\n",
    "        if t1.lemma_ in [\"comprise\"]:\n",
    "            # Scan ahead for colon \n",
    "            for t2 in doc[t1.i+1:]:\n",
    "                if t2.lemma_ == \":\":\n",
    "                    print(\"Colon found at {0} (text='{1}')\".format(t2.i, t2))\n",
    "                    step_boundaries.append(t2)\n",
    "                    # Scan ahead to find semi-colons associated with colon\n",
    "                    for t3 in doc[t2.i+1:]:\n",
    "                        if t3.pos == PUNCT and t3.tag_ in [\":\", \".\"]:\n",
    "                            step_boundaries.append(t3)\n",
    "                    break\n",
    "            break\n",
    "    \n",
    "    print(\"Step boundaries are {0}\".format(step_boundaries))\n",
    "    \n",
    "    # Find first entity with DET != the after each in set [colon, semi-colon]\n",
    "    component = list()\n",
    "    for sb in step_boundaries[:-1]:\n",
    "        for t1 in doc[sb.i+1:]:\n",
    "            if t1.pos == VERB and t1.tag_ == \"VBG\":\n",
    "                # sb is previous step boundary - we want next step boundary\n",
    "                step_verbs.append(t1)\n",
    "                break  \n",
    "    \n",
    "    print(\"Step verbs are {0}\".format(step_verbs))\n",
    "    \n",
    "    steps = list()\n",
    "    # Tada set of method steps\n",
    "    for sv, sb in zip(step_verbs, step_boundaries[1:]):\n",
    "        print(\"Step verb is {0} with lemma {1}\".format(sv, sv.lemma_))\n",
    "        print(\"Step text is {0}\".format(doc[sv.i:sb.i].text))\n",
    "        steps.append((sv, sb))\n",
    "        \n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity system has relationship comprising\n",
      "Entity apparatus has relationship having\n"
     ]
    }
   ],
   "source": [
    "# Look for [\"comprise\", \"have\", \"be\", \"include\"]\n",
    "for token in doc:\n",
    "    if token.lemma_ in [\"comprise\", \"have\", \"include\"]:\n",
    "        print(\"Entity {0} has relationship {1}\".format(token.head.text, token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
