{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Claims - Keras CNN + LSTM + Embedding\n",
    "\n",
    "In this post we will see if we can build some classifiers to predict a first level patent classification from the claim text.\n",
    "\n",
    "In particular, here we will look at applying a CNN as part of the deep learning stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"raw_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n1. A detector for atrial fibrillation or flutter (AF) comprising: \\nan impedance measuring unit comprising a measuring input, to which an atrial electrode line having an electrode for a unipolar measurement of an impedance in an atrium is connected and is implemented to generate an atrial impedance signal, obtained in a unipolar manner, in such a way that an impedance signal for each atrial cycle, comprising an atrial contraction and a following relaxation of said atrium, comprises multiple impedance values detected at different instants within a particular atrial cycle; \\nsaid impedance measuring unit comprising a signal input, via which a ventricle signal is to be supplied to said detector, which reflects instants of ventricular contractions in chronological assignment to said impedance signal; \\nan analysis unit configured to average multiple sequential impedance signal sections of a unipolar atrial impedance signal, which are each delimited by two sequential ventricular contractions, with one another, and to determine a maximum amplitude of an averaged unipolar atrial impedance signal section, \\nsaid analysis unit configured to compare said maximum amplitude to a comparison value, and if said maximum amplitude of said averaged unipolar atrial impedance signal is less than said comparison value, generate an AF suspicion signal. \\n\\n',\n",
       " 'A')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a play with the Keras text tokenizer (as per here - https://keras.io/preprocessing/text/#tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "docs = [d[0] for d in data]\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_claims = t.texts_to_matrix(docs, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11238, 26142)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_claims.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much faster than my old methods! But hey, I learnt some stuff about tokenisation.  \n",
    "\n",
    "We can use the \"num_words\" parameter as passed into the Tokenizer to restrict to the top n words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our classes are now a matrix of (11238, 8)\n",
      "Our claims are now a matrix of (11238, 5000)\n"
     ]
    }
   ],
   "source": [
    "# LSTM and CNN for sequence classification on claim data\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "Y_class = [d[1] for d in data]\n",
    "\n",
    "# encode class values as integers\n",
    "label_e = LabelEncoder()\n",
    "label_e.fit(Y_class)\n",
    "encoded_Y = label_e.transform(Y_class)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "Y_data = to_categorical(encoded_Y)\n",
    "print(\"Our classes are now a matrix of {0}\".format(Y_data.shape))\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 9\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# Initialise tokenizer\n",
    "top_words = 5000\n",
    "tokenizer = Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(docs)\n",
    "X_data = tokenizer.texts_to_matrix(docs, mode='tfidf')\n",
    "print(\"Our claims are now a matrix of {0}\".format(X_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 different classes\n"
     ]
    }
   ],
   "source": [
    "no_classes = Y_data.shape[1]\n",
    "print(\"There are {0} different classes\".format(no_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 808       \n",
      "=================================================================\n",
      "Total params: 217,112\n",
      "Trainable params: 217,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8990 samples, validate on 2248 samples\n",
      "Epoch 1/20\n",
      "8990/8990 [==============================] - 339s - loss: 1.7927 - acc: 0.2919 - val_loss: 1.7300 - val_acc: 0.2945\n",
      "Epoch 2/20\n",
      "8990/8990 [==============================] - 335s - loss: 1.7633 - acc: 0.2920 - val_loss: 1.7357 - val_acc: 0.2945\n",
      "Epoch 3/20\n",
      "8990/8990 [==============================] - 333s - loss: 1.7629 - acc: 0.2972 - val_loss: 1.7334 - val_acc: 0.2945\n",
      "Epoch 4/20\n",
      "8990/8990 [==============================] - 333s - loss: 1.7610 - acc: 0.2941 - val_loss: 1.7302 - val_acc: 0.2949\n",
      "Epoch 5/20\n",
      "8990/8990 [==============================] - 333s - loss: 1.7591 - acc: 0.2981 - val_loss: 1.7416 - val_acc: 0.2940\n",
      "Epoch 6/20\n",
      "8990/8990 [==============================] - 334s - loss: 1.7593 - acc: 0.2989 - val_loss: 1.7358 - val_acc: 0.2940\n",
      "Epoch 7/20\n",
      "8990/8990 [==============================] - 334s - loss: 1.7577 - acc: 0.2982 - val_loss: 1.7334 - val_acc: 0.2940\n",
      "Epoch 8/20\n",
      "8990/8990 [==============================] - 334s - loss: 1.7561 - acc: 0.2993 - val_loss: 1.7338 - val_acc: 0.2940\n",
      "Epoch 9/20\n",
      "8990/8990 [==============================] - 335s - loss: 1.7549 - acc: 0.2994 - val_loss: 1.7341 - val_acc: 0.2949\n",
      "Epoch 10/20\n",
      "8990/8990 [==============================] - 334s - loss: 1.7553 - acc: 0.2967 - val_loss: 1.7272 - val_acc: 0.2949\n",
      "Epoch 11/20\n",
      "8990/8990 [==============================] - 334s - loss: 1.7549 - acc: 0.2949 - val_loss: 1.7280 - val_acc: 0.2949\n",
      "Epoch 12/20\n",
      "8990/8990 [==============================] - 334s - loss: 1.7535 - acc: 0.2998 - val_loss: 1.7342 - val_acc: 0.2949\n",
      "Epoch 13/20\n",
      "8990/8990 [==============================] - 334s - loss: 1.7541 - acc: 0.2993 - val_loss: 1.7332 - val_acc: 0.2949\n",
      "Epoch 14/20\n",
      "8990/8990 [==============================] - 347s - loss: 1.7535 - acc: 0.2967 - val_loss: 1.7349 - val_acc: 0.2945\n",
      "Epoch 15/20\n",
      "8990/8990 [==============================] - 338s - loss: 1.7529 - acc: 0.2997 - val_loss: 1.7306 - val_acc: 0.2949\n",
      "Epoch 16/20\n",
      "8990/8990 [==============================] - 334s - loss: 1.7523 - acc: 0.3006 - val_loss: 1.7405 - val_acc: 0.2949\n",
      "Epoch 17/20\n",
      "8990/8990 [==============================] - 335s - loss: 1.7529 - acc: 0.3001 - val_loss: 1.7346 - val_acc: 0.2949\n",
      "Epoch 18/20\n",
      "8990/8990 [==============================] - 341s - loss: 1.7518 - acc: 0.3003 - val_loss: 1.7298 - val_acc: 0.2949\n",
      "Epoch 19/20\n",
      "8990/8990 [==============================] - 331s - loss: 1.7521 - acc: 0.3007 - val_loss: 1.7337 - val_acc: 0.2945\n",
      "Epoch 20/20\n",
      "8990/8990 [==============================] - 352s - loss: 1.7515 - acc: 0.3012 - val_loss: 1.7357 - val_acc: 0.2949\n",
      "Accuracy: 29.49%\n"
     ]
    }
   ],
   "source": [
    "# split the data into training (80%) and testing (20%)\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(X_data, Y_data, test_size=0.2, random_state=seed)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_claim_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_claim_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_claim_length)\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_claim_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b24ec54a2c6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv_lstm.hd5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"conv_lstm.hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that this approach doesn't seem to work. Likely because there are limited patterns at the word level to be detected by a CNN.  \n",
    "\n",
    "This would likely work better at the character level."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
