{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Entity Extraction\n",
    "\n",
    "In this notebook we will be looking at using spaCy (https://spacy.io/) to populate object models from patent claim data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's import spaCy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference here are some common object POS patterns as extracted from a patent specification using the reference numeral as an end point.\n",
    "```\n",
    "[('<DET><NOUN><NUM>', 63),\n",
    " ('<DET><NOUN><NOUN><NUM>', 50),\n",
    " ('<DET><VERB><NOUN><NUM>', 48),\n",
    " ('<DET><ADJ><NOUN><NUM>', 39),\n",
    " ('<DET><NOUN><NOUN><NOUN><NUM>', 35),\n",
    " ('<DET><ADJ><ADJ><NOUN><NOUN><NUM>', 14),\n",
    " ('<DET><NOUN><PUNCT><VERB><NOUN><NUM>', 8),\n",
    " ('<DET><ADJ><NOUN><NOUN><NUM>', 6),\n",
    " ('<DET><ADJ><CCONJ><ADJ><ADJ><NOUN><NOUN><NUM>', 4),\n",
    " ('<DET><NOUN><NOUN><NOUN><NOUN><NUM>', 3),\n",
    " ('<DET><NOUN><ADP><NOUN><NOUN><NUM>', 3),\n",
    " ('<DET><ADJ><CCONJ><ADJ><NOUN><NUM>', 3),\n",
    " ('<DET><NOUN><ADP><NOUN><NUM>', 3),\n",
    " ('<DET><NOUN><VERB><NOUN><NUM>', 2),\n",
    " ('<DET><NOUN><ADV><CCONJ><ADJ><NOUN><NUM>', 1),\n",
    " ('<DET><ADJ><VERB><NUM><PUNCT><NUM><ADP><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><ADP><ADV><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><ADV><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><ADV><VERB><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><VERB><NOUN><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><PUNCT><NOUN><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><VERB><ADP><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><ADP><ADJ><ADJ><ADJ><NOUN><NOUN><NUM>', 1),\n",
    " ('<DET><ADJ><NOUN><PUNCT><NOUN><PUNCT><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><PUNCT><NOUN><PUNCT><NOUN><PUNCT><VERB><NOUN><NUM>', 1),\n",
    " ('<DET><VERB><NOUN><ADV><CCONJ><ADJ><NOUN><NUM>', 1),\n",
    " ('<DET><NOUN><ADP><ADJ><NOUN><NUM>', 1)]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some initial functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import DET, NOUN, CCONJ\n",
    "\n",
    "def simple_spacy_entity_finder(doc):\n",
    "    \"\"\" Find entities with reference numerals using POS data.\"\"\"\n",
    "    entity_list = list()\n",
    "    record = False\n",
    "    # Generate a list of tokens so we can iterate backwards through it\n",
    "    enum_doc_list = list(enumerate(doc))\n",
    "    last_end = 0\n",
    "    # Add indices\n",
    "    for i, word in enum_doc_list:\n",
    "        if word.pos == DET and not record:\n",
    "            # Start recording and record start index\n",
    "            record = True\n",
    "            start_index = i\n",
    "        else:        \n",
    "            if (word.pos == DET or word.pos == CCONJ or word.lemma_ == \";\") and record:\n",
    "                # Step back until last noun is found\n",
    "                for j, bword in reversed(enum_doc_list[last_end:i]):\n",
    "                    if bword.pos == NOUN:\n",
    "                        # Add np_chunk to buffer\n",
    "                        entity_list.append(doc[start_index:j+1])\n",
    "                        last_end = j\n",
    "                        break       \n",
    "                if word.pos == DET:\n",
    "                    # Set new start index\n",
    "                    record = True\n",
    "                    start_index = i\n",
    "                else:\n",
    "                    record = False\n",
    "    \n",
    "    entity_dict = dict()\n",
    "    # Now group by unique\n",
    "    for entity in entity_list:\n",
    "        \n",
    "        np_start = entity.start\n",
    "        # Ignore the determinant \n",
    "        if doc[np_start].pos == DET:\n",
    "            np_start += 1\n",
    "        # Generate a string representation excluding the determinant\n",
    "        np_string = doc[np_start:entity.end].text.lower()\n",
    "                                \n",
    "        if np_string not in entity_dict.keys():\n",
    "            entity_dict[np_string] = list()          \n",
    "        entity_dict[np_string].append(entity)\n",
    "    \n",
    "    return entity_list, entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import DET, NOUN\n",
    "\n",
    "def check_ant(doc, entity_dict):\n",
    "    \"\"\" Check antecedence - attempt to merge entries with incorrect antecedence.\"\"\"\n",
    "    \n",
    "    issue_keys_a = list()\n",
    "    issue_keys_the = list()\n",
    "    \n",
    "    # Look for entries with antecedence issues\n",
    "    for key in entity_dict:\n",
    "        entities = entity_dict[key]\n",
    "        # Check if first entry begins with \"a\" - flag if doesn't\n",
    "        first_entry = entities[0]\n",
    "        if first_entry[0].pos == DET and first_entry[0].lemma_ != \"a\" and first_entry[0].lemma_ != \"an\":\n",
    "            issue_keys_a.append(key)\n",
    "        \n",
    "        # If more than one entry check subsequent entries start with \"the\" - flag if don't\n",
    "        if len(entities) > 1:\n",
    "            for entity in entities[1:]:\n",
    "                if entity[0].pos == DET and entity[0].lemma_ != \"the\":\n",
    "                    issue_keys_the.append(key)\n",
    "    \n",
    "    return issue_keys_a, issue_keys_the\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def look_for_existing(doc, entity_dict):\n",
    "    \"\"\" Look for previously existing versions of problem keys.\"\"\"\n",
    "    # If more than one entry check subsequent entries start with \"the\" - flag if don't\n",
    "    issue_keys_a = list()\n",
    "    for key in entity_dict:\n",
    "        entities = entity_dict[key]\n",
    "        # Check if first entry begins with \"a\" - flag if doesn't\n",
    "        first_entry = entities[0]\n",
    "        if first_entry[0].pos == DET and first_entry[0].lemma_ != \"a\" and first_entry[0].lemma_ != \"an\":\n",
    "            issue_keys_a.append(key)\n",
    "    \n",
    "    for pkey in issue_keys_a:\n",
    "        problem_entities = entity_dict[pkey]\n",
    "        # i.e. list of two longer oblong spans\n",
    "        # Can we just work with the key initially?\n",
    "        for key in entity_dict.keys():\n",
    "            if len(pkey) > len(key) and key in pkey:\n",
    "                print(key, pkey)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We now need to collate and create a set of entities\n",
    "def get_entity_set(entity_list):\n",
    "    \"\"\" Get a set of unique entity n-grams from a list of entities.\"\"\"\n",
    "    ngram_list = list()\n",
    "    for entity in entity_list:\n",
    "        ngram_list.append(\" \".join([word for word, pos in entity if (pos != 'DET')]))\n",
    "    return set(ngram_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing on Other Patent Data\n",
    "\n",
    "Lets test on different patent claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554570 records located.\n",
      "10 records sampled.\n"
     ]
    }
   ],
   "source": [
    "# Generate or create some test claim sets for analysis\n",
    "\n",
    "# (Looks like we can't pickle and load spaCy objects)\n",
    "from patentdata.corpus import USPublications\n",
    "\n",
    "pubs = USPublications(\"/media/SAMSUNG1/Patent_Downloads\")\n",
    "filegenerator = pubs.patentdoc_generator(['G', '06'], sample_size=10)\n",
    "docs = list(filegenerator)\n",
    "ent_from_claims = list()\n",
    "nlp_docs = list()\n",
    "for doc in docs:\n",
    "    nlp_doc = nlp(doc.claimset.get_claim(1).text)\n",
    "    entity_list, entity_dict = simple_spacy_entity_finder(nlp_doc)\n",
    "    nlp_docs.append(nlp_doc)\n",
    "    ent_from_claims.append(entity_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'communication system': [a communication system],\n",
       " 'experience': [the experience],\n",
       " 'game system': [A game system],\n",
       " 'interactive apparatus': [an interactive apparatus,\n",
       "  the interactive apparatus],\n",
       " 'interactive application': [an interactive application,\n",
       "  the interactive application,\n",
       "  the interactive application],\n",
       " 'offline': [an offline],\n",
       " 'online experience': [an online experience]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_from_claims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These terms are not explicitly introduced using 'a/an X':\n",
      " ['experience'] \n",
      "\n",
      "These terms do not use 'the' yet occur previously:\n",
      " [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ika, ikt = check_ant(nlp_docs[0], ent_from_claims[0])\n",
    "print(\"These terms are not explicitly introduced using 'a/an X':\\n\", ika, \"\\n\")\n",
    "\n",
    "print(\"These terms do not use 'the' yet occur previously:\\n\", ikt, \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1. A game system comprising:\n",
       "an interactive apparatus having a communication system; and\n",
       "an interactive application, the interactive application and the interactive apparatus are independently operable to provide an offline and an online experience, wherein at least one of the interactive application and interactive apparatus is configured modify its operation based on the experience of the other.\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. A game system comprising:\n",
      "an interactive apparatus having a communication system; and\n",
      "an interactive application, the interactive application and the interactive apparatus are independently operable to provide an offline and an online experience, wherein at least one of the interactive application and interactive apparatus is configured modify its operation based on the experience of the other.\n",
      "\n",
      " \n",
      "\n",
      "{'interactive apparatus': [an interactive apparatus, the interactive apparatus], 'game system': [A game system], 'communication system': [a communication system], 'offline': [an offline], 'interactive application': [an interactive application, the interactive application, the interactive application], 'experience': [the experience], 'online experience': [an online experience]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A server for providing affiliate store information, comprising:\n",
      "a receiver for collecting transaction count information relating to a number of transactions made in each affiliate store and receiving search condition information from a mobile communication terminal;\n",
      "a database for storing affiliate store information generated based on the collected transaction count information according to a predetermined criteria;\n",
      "a generator for extracting the affiliate store information of at least one affiliate store from the database according to the search condition information and generating a search result using the extracted affiliate store information; and\n",
      "a transmitter for transmitting the search result to the mobile communication terminal.\n",
      "\n",
      " \n",
      "\n",
      "{'predetermined criteria': [a predetermined criteria], 'mobile communication terminal': [a mobile communication terminal], 'receiver for collecting transaction count information': [a receiver for collecting transaction count information], 'generator': [a generator], 'affiliate store': [each affiliate store], 'server for providing affiliate store information': [A server for providing affiliate store information], 'collected transaction count information': [the collected transaction count information], 'database for storing affiliate store information': [a database for storing affiliate store information], 'extracted affiliate store information': [the extracted affiliate store information], 'transmitter': [a transmitter], 'search result': [a search result, the search result], 'number of transactions': [a number of transactions], 'database': [the database], 'affiliate store information of at least one affiliate store': [the affiliate store information of at least one affiliate store], 'search condition information': [the search condition information]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A cover for a mobile device, comprising:\n",
      "side surfaces configured to be adjacent at least a portion one or more side surfaces of a mobile device;\n",
      "a rear surface configured to be adjacent at least a portion of a rear surface of the mobile device and connected to the side surfaces, the side surfaces and the rear surface form an opening that receives at least a portion of the mobile device;\n",
      "a connector configured to connect to a port of the mobile device;\n",
      "a circuit that connects the connector to a communication module; and\n",
      "the communication module configured to execute transactions with contactless devices and communicate with the mobile device through the circuit and the connector.\n",
      "\n",
      " \n",
      "\n",
      "{'portion': [a portion, a portion, a portion], 'rear surface': [a rear surface, a rear surface], 'cover': [A cover], 'port': [a port], 'opening': [an opening], 'circuit': [a circuit, the circuit], 'rear surface form': [the rear surface form], 'communication module': [a communication module], 'mobile device, comprising:\\nside surfaces': [a mobile device, comprising:\n",
      "side surfaces], 'communication module configured to execute transactions with contactless devices': [the communication module configured to execute transactions with contactless devices], 'mobile device': [a mobile device, the mobile device, the mobile device, the mobile device, the mobile device], 'side surfaces': [the side surfaces, the side surfaces], 'connector': [a connector, the connector]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A dynamic voltage scaling scheduling method for resource-sharing and hard real-time tasks, applicable for scheduling tasks in a delayed task set, comprising:\n",
      "determining a property of a task, and executing one of the following steps, when the task belongs to the delayed task set or the task does not belong to the task collection but a waiting time has exceeded a period of the task;\n",
      "when one task in the delayed task set requires for being executed, increasing a working voltage required for executing the task, removing the task from the delayed task set, and returning to the step of determining the property of the task;\n",
      "when one task in the delayed task set requires for sharing resources, setting the working voltage required by the task as a current working voltage or as a larger one in least upper bounds of all tasks requiring for sharing resources, and returning to the step of determining the property of the task; and\n",
      "when one task not belonging to the delayed task set exists, and the waiting time of the task has exceeded the period of the task, reducing the working voltage required for executing the task, adding the task in the delayed task set, and returning to the step of determining the property of the task.\n",
      "\n",
      " \n",
      "\n",
      "{'delayed task set requires for sharing resources': [the delayed task set requires for sharing resources], 'waiting time': [a waiting time, the waiting time], 'larger one in least upper bounds': [a larger one in least upper bounds], 'step': [the step, the step, the step], 'dynamic voltage scaling scheduling method for resource-sharing': [A dynamic voltage scaling scheduling method for resource-sharing], 'task': [a task, the task, the task, the task, the task, the task, the task, the task, the task, the task, the task, the task, the task], 'delayed task': [a delayed task, the delayed task, the delayed task, the delayed task, the delayed task], 'property': [a property, the property, the property, the property], 'working voltage': [a working voltage, the working voltage, the working voltage], 'delayed task set': [the delayed task set], 'current working voltage': [a current working voltage], 'task collection': [the task collection], 'following steps': [the following steps], 'tasks requiring for sharing resources': [all tasks requiring for sharing resources], 'period': [a period, the period]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A method of forming conductive traces on a touch sensor panel, comprising:\n",
      "forming and patterning a stackup of a first conductive material over a substrate in one or more border areas of the touch sensor panel to create one or more traces having widths maximized so that the one or more traces and any separation areas between the traces in any particular portion of the border area occupy a full width of that portion of the border area; and\n",
      "forming and patterning a layer of a second conductive material over the substrate to create one or more rows, each row coupled to a different trace, the rows forming part of a plurality of sensors on the touch sensor panel.\n",
      "\n",
      " \n",
      "\n",
      "{'': [], 'different trace': [a different trace], 'plurality of sensors': [a plurality of sensors], 'touch sensor panel': [a touch sensor panel, the touch sensor panel], 'layer': [a layer], 'substrate': [a substrate, the substrate], 'row': [each row], 'rows forming part': [the rows forming part], 'full width': [a full width], 'traces': [the traces], 'separation areas': [any separation areas], 'portion': [that portion], 'stackup': [a stackup], 'second conductive material': [a second conductive material], 'method of forming conductive traces': [A method of forming conductive traces], 'first conductive material': [a first conductive material], 'border area': [the border area, the border area], 'particular portion': [any particular portion]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A method for a storage system, the storage system including a first controller, a second controller and a plurality of storage devices, each of the first and second controllers communicatively coupled to each one of the storage devices, the method comprising:\n",
      "starting a timer that expires after a first time period; and\n",
      "subsequent to starting the timer, transmitting a first message from the first controller to a memory element shared by the first and second controllers, the first message capable of notifying the second controller of an imminent failure of the first controller, wherein subsequent to transmitting the first message to the shared memory element and before or when the timer expires, the first controller becomes unavailable to facilitate access to the storage devices.\n",
      "\n",
      " \n",
      "\n",
      "{'': [, , , ], 'first time period': [a first time period], 'method': [A method, the method], 'first message': [a first message, the first message, the first message], 'shared memory element': [the shared memory element], 'second controller': [a second controller, the second controller], 'plurality of storage devices': [a plurality of storage devices], 'first controller': [a first controller, the first controller, the first controller], 'first controller becomes unavailable to facilitate access': [the first controller becomes unavailable to facilitate access], 'storage system': [a storage system, the storage system], 'storage devices': [the storage devices], 'timer': [a timer, the timer, the timer], 'imminent failure': [an imminent failure], 'memory element': [a memory element]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A method comprising:\n",
      "initiating a first interaction of a payment cardholder computer system executing a browser application with an item of content associated with a merchant Website;\n",
      "before completion of the first interaction, initiating a proxy interaction of the payment cardholder computer system executing a browser application with an item of content associated with the merchant Website;\n",
      "communicating with a payment card network connected to the merchant Website; and\n",
      "using the proxy interaction to determine whether the merchant Website is registered with the payment card network.\n",
      "\n",
      " \n",
      "\n",
      "{'merchant website': [a merchant Website, the merchant Website, the merchant Website], 'method': [A method], 'browser application': [a browser application, a browser application], 'payment card network': [a payment card network], 'merchant': [the merchant], 'proxy interaction': [a proxy interaction, the proxy interaction], 'first interaction': [a first interaction, the first interaction], 'item of content': [an item of content, an item of content], 'payment cardholder computer system': [a payment cardholder computer system, the payment cardholder computer system]} \n",
      "------\n",
      "\n",
      "\n",
      "1. A power-saving method for a mobile terminal, comprising the steps of:\n",
      "monitoring, by the mobile terminal, data services in accordance with a monitoring strategy for a data service power-saving state;\n",
      "triggering a data service power-saving operation when a triggering condition set in the monitoring strategy is met; and\n",
      "performing, by the mobile terminal, the data service power-saving operation.\n",
      "\n",
      " \n",
      "\n",
      "{'data service power-saving state': [a data service power-saving state], 'mobile terminal': [a mobile terminal, the mobile terminal], 'triggering condition': [a triggering condition], 'mobile terminal, data services in accordance': [the mobile terminal, data services in accordance], 'monitoring strategy': [a monitoring strategy, the monitoring strategy], 'power-saving method': [A power-saving method], 'data service power-saving operation': [a data service power-saving operation], 'steps of:\\nmonitoring': [the steps of:\n",
      "monitoring]} \n",
      "------\n",
      "\n",
      "\n",
      "1-10. (canceled)\n",
      " \n",
      "\n",
      "{} \n",
      "------\n",
      "\n",
      "\n",
      "1. A system for automatically replacing problematic media files comprising:\n",
      "a first media store configured to store a plurality of digitally encoded local media files;\n",
      "a second media store configured to store a plurality of digitally encoded source media files;\n",
      "a media diagnostic engine configured to identify whether problems exist within one of the media files located in the first media store, wherein when a problem is identified, the associated file having the problem is able to be referred to as a problematic file, wherein the media diagnostic engine is configured to identify whether problems exist based upon at least one of a user input and a software based error detection algorithm determination; and\n",
      "a media replacement engine configured to replace a problematic file in the first media store with a copy of a corresponding media file from the second media store.\n",
      "\n",
      " \n",
      "\n",
      "{'problematic file': [a problematic file, a problematic file], 'media diagnostic engine is configured to identify whether problems': [the media diagnostic engine is configured to identify whether problems], 'plurality of digitally encoded source media files': [a plurality of digitally encoded source media files], 'associated file': [the associated file], 'problem': [a problem, the problem], 'second media store': [a second media store], 'media files': [the media files], 'system for automatically replacing problematic media files': [A system for automatically replacing problematic media files], 'first media store': [a first media store, the first media store, the first media store], 'copy': [a copy], 'plurality of digitally encoded local media files': [a plurality of digitally encoded local media files], 'media diagnostic engine configured to identify whether problems': [a media diagnostic engine configured to identify whether problems], 'user input': [a user input], 'media replacement engine': [a media replacement engine], 'corresponding media file': [a corresponding media file], 'software based error detection algorithm determination': [a software based error detection algorithm determination]} \n",
      "------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d, e in zip(nlp_docs, ent_from_claims):\n",
    "    print(d, \"\\n\")\n",
    "    print(e, \"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* Matching occurrences of \"the X\" with other entries looks generally useful (e.g. is needed across multiple claims). Phrases such as \"the given X\" or \"the selected X\" also appear.\n",
    "* There are some long sections that appear not to meet the simple parse.\n",
    "* Some have a blank entity?\n",
    "* We could use the noun_chunks as a second test and merge for greater accuracy?\n",
    "* Doesn't work so well on some method claims.\n",
    "* Need to stop on punctuation as well, i.e. \",\" or \";\"\n",
    "* \"said\" needs to be a DET.\n",
    "* Plurals cause an issue, e.g. \"multimedia data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " [A game system, an interactive apparatus, a communication system, an interactive application, the interactive application, the interactive apparatus, an offline, an online experience, the interactive application, interactive apparatus, its operation, the experience] \n",
      "\n",
      "['interactive apparatus', 'game system', 'communication system', 'offline', 'interactive application', 'experience', 'online experience']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A server, affiliate store information, a receiver, transaction count information, a number, transactions, each affiliate store, search condition information, a mobile communication terminal, a database, affiliate store information, the collected transaction count information, a predetermined criteria, the affiliate store information, at least one affiliate store, the database, the search condition information, a search result, the extracted affiliate store information, a transmitter, the search result, the mobile communication terminal] \n",
      "\n",
      "['predetermined criteria', 'mobile communication terminal', 'receiver for collecting transaction count information', 'generator', 'affiliate store', 'server for providing affiliate store information', 'collected transaction count information', 'database for storing affiliate store information', 'extracted affiliate store information', 'transmitter', 'search result', 'number of transactions', 'database', 'affiliate store information of at least one affiliate store', 'search condition information']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A cover, a mobile device, at least a portion, a mobile device, at least a portion, a rear surface, the mobile device, the side surfaces, the side surfaces, the rear surface form, at least a portion, the mobile device, a port, the mobile device, the connector, a communication module, transactions, contactless devices, the mobile device, the circuit, the connector] \n",
      "\n",
      "['portion', 'rear surface', 'cover', 'port', 'opening', 'circuit', 'rear surface form', 'communication module', 'mobile device, comprising:\\nside surfaces', 'communication module configured to execute transactions with contactless devices', 'mobile device', 'side surfaces', 'connector']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [\n",
      "1. A dynamic voltage scaling, scheduling method, resource-sharing, hard real-time tasks, scheduling tasks, a delayed task, a property, a task, the following steps, the task, the delayed task, the task, the task collection, a waiting time, a period, the task, one task, the delayed task, a working voltage, the task, the task, the delayed task, the step, the property, the task, one task, the delayed task, sharing resources, the working voltage, the task, a current working voltage, a larger one, least upper bounds, all tasks, sharing resources, the step, the property, the task, the delayed task, set, the waiting time, the task, the period, the task, the working voltage, the task, the task, the delayed task, the step, the property, the task] \n",
      "\n",
      "['delayed task set requires for sharing resources', 'waiting time', 'larger one in least upper bounds', 'step', 'dynamic voltage scaling scheduling method for resource-sharing', 'task', 'delayed task', 'property', 'working voltage', 'delayed task set', 'current working voltage', 'task collection', 'following steps', 'tasks requiring for sharing resources', 'period']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A method, conductive traces, a touch sensor panel, a stackup, a first conductive material, a substrate, one or more border areas, the touch sensor panel, one or more traces, widths, that the one or more traces, any separation areas, the traces, any particular portion, the border area, a full width, that portion, the border area, a layer, a second conductive material, the substrate, one or more rows, a different trace, part, a plurality, sensors, the touch sensor panel] \n",
      "\n",
      "['', 'different trace', 'plurality of sensors', 'touch sensor panel', 'layer', 'substrate', 'row', 'rows forming part', 'full width', 'traces', 'separation areas', 'portion', 'stackup', 'second conductive material', 'method of forming conductive traces', 'first conductive material', 'border area', 'particular portion']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A method, a storage system, the storage system, a first controller, a second controller, a plurality, storage devices, the first and second controllers, the storage devices, the method, a timer, a first time period, the timer, a first message, the first controller, a memory element, the first and second controllers, the second controller, an imminent failure, the first controller, the first message, the shared memory element, the timer, the first controller, access, the storage devices] \n",
      "\n",
      "['', 'first time period', 'method', 'first message', 'shared memory element', 'second controller', 'plurality of storage devices', 'first controller', 'first controller becomes unavailable to facilitate access', 'storage system', 'storage devices', 'timer', 'imminent failure', 'memory element']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A method, a first interaction, a payment cardholder computer system, a browser application, an item, content, a merchant Website, completion, the first interaction, a proxy interaction, the payment cardholder computer system, a browser application, an item, content, the merchant Website, a payment card network, the merchant Website, the proxy interaction, the merchant, the payment card network] \n",
      "\n",
      "['merchant website', 'method', 'browser application', 'payment card network', 'merchant', 'proxy interaction', 'first interaction', 'item of content', 'payment cardholder computer system']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A power-saving method, a mobile terminal, the steps, the mobile terminal, accordance, a monitoring strategy, a data service power-saving state, a data service power-saving operation, a triggering condition, the monitoring strategy, the mobile terminal] \n",
      "\n",
      "['data service power-saving state', 'mobile terminal', 'triggering condition', 'mobile terminal, data services in accordance', 'monitoring strategy', 'power-saving method', 'data service power-saving operation', 'steps of:\\nmonitoring']\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [] \n",
      "\n",
      "[]\n",
      "\n",
      "-----\n",
      "\n",
      "----\n",
      " [A system, problematic media files, a plurality, digitally encoded local media files, a second media store, a plurality, digitally encoded source media files, a media diagnostic engine, problems, the media files, the first media store, a problem, the problem, a problematic file, the media diagnostic engine, problems, a user input, a software based error detection algorithm determination, a media replacement engine, a problematic file, the first media store, a copy, a corresponding media file, the second media store] \n",
      "\n",
      "['problematic file', 'media diagnostic engine is configured to identify whether problems', 'plurality of digitally encoded source media files', 'associated file', 'problem', 'second media store', 'media files', 'system for automatically replacing problematic media files', 'first media store', 'copy', 'plurality of digitally encoded local media files', 'media diagnostic engine configured to identify whether problems', 'user input', 'media replacement engine', 'corresponding media file', 'software based error detection algorithm determination']\n",
      "\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d, e in zip(nlp_docs, ent_from_claims):\n",
    "    print(\"----\\n\", list(d.noun_chunks), \"\\n\")\n",
    "    print(list(e.keys()))\n",
    "    print(\"\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I define the problem using probabilities?  \n",
    "\n",
    "Entities are latent variables of which the words are the visible / observable data.  \n",
    "\n",
    "Problem is aligning groups of tokens with entities. Classification in a case where we don't know what the classes are or how many classes there are.  \n",
    "\n",
    "P(entity | words)\n",
    "\n",
    "What do we know for certain:\n",
    "* It will have a form of DET ... NOUN or no DET but noun phrase ending in NNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def annotated_entity_extraction(doc):\n",
    "    entity_list = list()\n",
    "    record = False\n",
    "    # Generate a list of tokens so we can iterate backwards through it\n",
    "    enum_doc_list = list(enumerate(doc))\n",
    "    last_end = 0\n",
    "    # Add indices\n",
    "    for i, word in enum_doc_list:\n",
    "        print(i, word, record)\n",
    "        if word.pos == DET and not record:\n",
    "            # Start recording and record start index\n",
    "            record = True\n",
    "            start_index = i\n",
    "            print(\"Starting to record at {0}-{1}\".format(i, word))\n",
    "        else:        \n",
    "            if (word.pos == DET or word.pos == CCONJ or word.lemma_ == \";\" or word.lemma_ == '.') and record:\n",
    "                print(\"Stepping back at {0}-{1}\".format(i, word))\n",
    "                # Step back until last noun is found\n",
    "                added = False\n",
    "                for j, bword in reversed(enum_doc_list[last_end:i]):\n",
    "                    print(j, bword, last_end)\n",
    "                    if bword.pos == NOUN:\n",
    "                        # Add np_chunk to buffer\n",
    "                        print(\"-----> Adding from {0}-{1} = {2}\".format(j, i, doc[start_index:j+1]))\n",
    "                        entity_list.append(doc[start_index:j+1])\n",
    "                        last_end = j+1\n",
    "                        added = True\n",
    "                        break\n",
    "                # Here if nothing has been added, e.g. no noun found, we need to keep recording\n",
    "                if word.pos == DET:\n",
    "                    # Set new start index\n",
    "                    record = True\n",
    "                    start_index = i\n",
    "                    print(\"Starting to record again at {0}-{1}\".format(i, word))\n",
    "                else:\n",
    "                    if (word.pos == CCONJ and not added):\n",
    "                        record = True\n",
    "                    else:\n",
    "                        record = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " False\n",
      "1 1 False\n",
      "2 . False\n",
      "3 A False\n",
      "Starting to record at 3-A\n",
      "4 game True\n",
      "5 system True\n",
      "6 comprising True\n",
      "7 : True\n",
      "8 \n",
      " True\n",
      "9 an True\n",
      "Stepping back at 9-an\n",
      "8 \n",
      " 0\n",
      "7 : 0\n",
      "6 comprising 0\n",
      "5 system 0\n",
      "-----> Adding from 5-9 = A game system\n",
      "Starting to record again at 9-an\n",
      "10 interactive True\n",
      "11 apparatus True\n",
      "12 having True\n",
      "13 a True\n",
      "Stepping back at 13-a\n",
      "12 having 6\n",
      "11 apparatus 6\n",
      "-----> Adding from 11-13 = an interactive apparatus\n",
      "Starting to record again at 13-a\n",
      "14 communication True\n",
      "15 system True\n",
      "16 ; True\n",
      "Stepping back at 16-;\n",
      "15 system 12\n",
      "-----> Adding from 15-16 = a communication system\n",
      "17 and False\n",
      "18 \n",
      " False\n",
      "19 an False\n",
      "Starting to record at 19-an\n",
      "20 interactive True\n",
      "21 application True\n",
      "22 , True\n",
      "23 the True\n",
      "Stepping back at 23-the\n",
      "22 , 16\n",
      "21 application 16\n",
      "-----> Adding from 21-23 = an interactive application\n",
      "Starting to record again at 23-the\n",
      "24 interactive True\n",
      "25 application True\n",
      "26 and True\n",
      "Stepping back at 26-and\n",
      "25 application 22\n",
      "-----> Adding from 25-26 = the interactive application\n",
      "27 the False\n",
      "Starting to record at 27-the\n",
      "28 interactive True\n",
      "29 apparatus True\n",
      "30 are True\n",
      "31 independently True\n",
      "32 operable True\n",
      "33 to True\n",
      "34 provide True\n",
      "35 an True\n",
      "Stepping back at 35-an\n",
      "34 provide 26\n",
      "33 to 26\n",
      "32 operable 26\n",
      "31 independently 26\n",
      "30 are 26\n",
      "29 apparatus 26\n",
      "-----> Adding from 29-35 = the interactive apparatus\n",
      "Starting to record again at 35-an\n",
      "36 offline True\n",
      "37 and True\n",
      "Stepping back at 37-and\n",
      "36 offline 30\n",
      "-----> Adding from 36-37 = an offline\n",
      "38 an False\n",
      "Starting to record at 38-an\n",
      "39 online True\n",
      "40 experience True\n",
      "41 , True\n",
      "42 wherein True\n",
      "43 at True\n",
      "44 least True\n",
      "45 one True\n",
      "46 of True\n",
      "47 the True\n",
      "Stepping back at 47-the\n",
      "46 of 37\n",
      "45 one 37\n",
      "44 least 37\n",
      "43 at 37\n",
      "42 wherein 37\n",
      "41 , 37\n",
      "40 experience 37\n",
      "-----> Adding from 40-47 = an online experience\n",
      "Starting to record again at 47-the\n",
      "48 interactive True\n",
      "49 application True\n",
      "50 and True\n",
      "Stepping back at 50-and\n",
      "49 application 41\n",
      "-----> Adding from 49-50 = the interactive application\n",
      "51 interactive False\n",
      "52 apparatus False\n",
      "53 is False\n",
      "54 configured False\n",
      "55 modify False\n",
      "56 its False\n",
      "57 operation False\n",
      "58 based False\n",
      "59 on False\n",
      "60 the False\n",
      "Starting to record at 60-the\n",
      "61 experience True\n",
      "62 of True\n",
      "63 the True\n",
      "Stepping back at 63-the\n",
      "62 of 50\n",
      "61 experience 50\n",
      "-----> Adding from 61-63 = the experience\n",
      "Starting to record again at 63-the\n",
      "64 other True\n",
      "65 . True\n",
      "Stepping back at 65-.\n",
      "64 other 62\n",
      "63 the 62\n",
      "62 of 62\n",
      "66 \n",
      "\n",
      " False\n"
     ]
    }
   ],
   "source": [
    "annotated_entity_extraction(nlp_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " False\n",
      "1 1 False\n",
      "2 . False\n",
      "3 A False\n",
      "Starting to record at 3-A\n",
      "4 server True\n",
      "5 for True\n",
      "6 providing True\n",
      "7 affiliate True\n",
      "8 store True\n",
      "9 information True\n",
      "10 , True\n",
      "11 comprising True\n",
      "12 : True\n",
      "13 \n",
      " True\n",
      "14 a True\n",
      "Stepping back at 14-a\n",
      "13 \n",
      " 0\n",
      "12 : 0\n",
      "11 comprising 0\n",
      "10 , 0\n",
      "9 information 0\n",
      "-----> Adding from 9-14 = A server for providing affiliate store information\n",
      "Starting to record again at 14-a\n",
      "15 receiver True\n",
      "16 for True\n",
      "17 collecting True\n",
      "18 transaction True\n",
      "19 count True\n",
      "20 information True\n",
      "21 relating True\n",
      "22 to True\n",
      "23 a True\n",
      "Stepping back at 23-a\n",
      "22 to 10\n",
      "21 relating 10\n",
      "20 information 10\n",
      "-----> Adding from 20-23 = a receiver for collecting transaction count information\n",
      "Starting to record again at 23-a\n",
      "24 number True\n",
      "25 of True\n",
      "26 transactions True\n",
      "27 made True\n",
      "28 in True\n",
      "29 each True\n",
      "Stepping back at 29-each\n",
      "28 in 21\n",
      "27 made 21\n",
      "26 transactions 21\n",
      "-----> Adding from 26-29 = a number of transactions\n",
      "Starting to record again at 29-each\n",
      "30 affiliate True\n",
      "31 store True\n",
      "32 and True\n",
      "Stepping back at 32-and\n",
      "31 store 27\n",
      "-----> Adding from 31-32 = each affiliate store\n",
      "33 receiving False\n",
      "34 search False\n",
      "35 condition False\n",
      "36 information False\n",
      "37 from False\n",
      "38 a False\n",
      "Starting to record at 38-a\n",
      "39 mobile True\n",
      "40 communication True\n",
      "41 terminal True\n",
      "42 ; True\n",
      "Stepping back at 42-;\n",
      "41 terminal 32\n",
      "-----> Adding from 41-42 = a mobile communication terminal\n",
      "43 \n",
      " False\n",
      "44 a False\n",
      "Starting to record at 44-a\n",
      "45 database True\n",
      "46 for True\n",
      "47 storing True\n",
      "48 affiliate True\n",
      "49 store True\n",
      "50 information True\n",
      "51 generated True\n",
      "52 based True\n",
      "53 on True\n",
      "54 the True\n",
      "Stepping back at 54-the\n",
      "53 on 42\n",
      "52 based 42\n",
      "51 generated 42\n",
      "50 information 42\n",
      "-----> Adding from 50-54 = a database for storing affiliate store information\n",
      "Starting to record again at 54-the\n",
      "55 collected True\n",
      "56 transaction True\n",
      "57 count True\n",
      "58 information True\n",
      "59 according True\n",
      "60 to True\n",
      "61 a True\n",
      "Stepping back at 61-a\n",
      "60 to 51\n",
      "59 according 51\n",
      "58 information 51\n",
      "-----> Adding from 58-61 = the collected transaction count information\n",
      "Starting to record again at 61-a\n",
      "62 predetermined True\n",
      "63 criteria True\n",
      "64 ; True\n",
      "Stepping back at 64-;\n",
      "63 criteria 59\n",
      "-----> Adding from 63-64 = a predetermined criteria\n",
      "65 \n",
      " False\n",
      "66 a False\n",
      "Starting to record at 66-a\n",
      "67 generator True\n",
      "68 for True\n",
      "69 extracting True\n",
      "70 the True\n",
      "Stepping back at 70-the\n",
      "69 extracting 64\n",
      "68 for 64\n",
      "67 generator 64\n",
      "-----> Adding from 67-70 = a generator\n",
      "Starting to record again at 70-the\n",
      "71 affiliate True\n",
      "72 store True\n",
      "73 information True\n",
      "74 of True\n",
      "75 at True\n",
      "76 least True\n",
      "77 one True\n",
      "78 affiliate True\n",
      "79 store True\n",
      "80 from True\n",
      "81 the True\n",
      "Stepping back at 81-the\n",
      "80 from 68\n",
      "79 store 68\n",
      "-----> Adding from 79-81 = the affiliate store information of at least one affiliate store\n",
      "Starting to record again at 81-the\n",
      "82 database True\n",
      "83 according True\n",
      "84 to True\n",
      "85 the True\n",
      "Stepping back at 85-the\n",
      "84 to 80\n",
      "83 according 80\n",
      "82 database 80\n",
      "-----> Adding from 82-85 = the database\n",
      "Starting to record again at 85-the\n",
      "86 search True\n",
      "87 condition True\n",
      "88 information True\n",
      "89 and True\n",
      "Stepping back at 89-and\n",
      "88 information 83\n",
      "-----> Adding from 88-89 = the search condition information\n",
      "90 generating False\n",
      "91 a False\n",
      "Starting to record at 91-a\n",
      "92 search True\n",
      "93 result True\n",
      "94 using True\n",
      "95 the True\n",
      "Stepping back at 95-the\n",
      "94 using 89\n",
      "93 result 89\n",
      "-----> Adding from 93-95 = a search result\n",
      "Starting to record again at 95-the\n",
      "96 extracted True\n",
      "97 affiliate True\n",
      "98 store True\n",
      "99 information True\n",
      "100 ; True\n",
      "Stepping back at 100-;\n",
      "99 information 94\n",
      "-----> Adding from 99-100 = the extracted affiliate store information\n",
      "101 and False\n",
      "102 \n",
      " False\n",
      "103 a False\n",
      "Starting to record at 103-a\n",
      "104 transmitter True\n",
      "105 for True\n",
      "106 transmitting True\n",
      "107 the True\n",
      "Stepping back at 107-the\n",
      "106 transmitting 100\n",
      "105 for 100\n",
      "104 transmitter 100\n",
      "-----> Adding from 104-107 = a transmitter\n",
      "Starting to record again at 107-the\n",
      "108 search True\n",
      "109 result True\n",
      "110 to True\n",
      "111 the True\n",
      "Stepping back at 111-the\n",
      "110 to 105\n",
      "109 result 105\n",
      "-----> Adding from 109-111 = the search result\n",
      "Starting to record again at 111-the\n",
      "112 mobile True\n",
      "113 communication True\n",
      "114 terminal True\n",
      "115 . True\n",
      "Stepping back at 115-.\n",
      "114 terminal 110\n",
      "-----> Adding from 114-115 = the mobile communication terminal\n",
      "116 \n",
      "\n",
      " False\n"
     ]
    }
   ],
   "source": [
    "annotated_entity_extraction(nlp_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_entity_finder(doc):\n",
    "    \"\"\" Find entities using noun phrases/chunks.\"\"\"\n",
    "    entity_dict = dict()\n",
    "    for entity in doc.noun_chunks:\n",
    "        np_start = entity.start\n",
    "        # Ignore the determinant \n",
    "        if doc[np_start].pos == DET:\n",
    "            np_start += 1\n",
    "        # Generate a string representation excluding the determinant\n",
    "        np_string = doc[np_start:entity.end].text.lower()\n",
    "                                \n",
    "        if np_string not in entity_dict.keys():\n",
    "            entity_dict[np_string] = list()          \n",
    "        entity_dict[np_string].append(entity)\n",
    "        \n",
    "    return entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'affiliate store': [each affiliate store],\n",
       " 'affiliate store information': [affiliate store information,\n",
       "  affiliate store information,\n",
       "  the affiliate store information],\n",
       " 'at least one affiliate store': [at least one affiliate store],\n",
       " 'collected transaction count information': [the collected transaction count information],\n",
       " 'database': [a database, the database],\n",
       " 'extracted affiliate store information': [the extracted affiliate store information],\n",
       " 'mobile communication terminal': [a mobile communication terminal,\n",
       "  the mobile communication terminal],\n",
       " 'number': [a number],\n",
       " 'predetermined criteria': [a predetermined criteria],\n",
       " 'receiver': [a receiver],\n",
       " 'search condition information': [search condition information,\n",
       "  the search condition information],\n",
       " 'search result': [a search result, the search result],\n",
       " 'server': [A server],\n",
       " 'transaction count information': [transaction count information],\n",
       " 'transactions': [transactions],\n",
       " 'transmitter': [a transmitter]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_entity_finder(nlp_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([A server for providing affiliate store information,\n",
       "  a receiver for collecting transaction count information,\n",
       "  a number of transactions,\n",
       "  each affiliate store,\n",
       "  a mobile communication terminal,\n",
       "  a database for storing affiliate store information,\n",
       "  the collected transaction count information,\n",
       "  a predetermined criteria,\n",
       "  a generator,\n",
       "  the affiliate store information of at least one affiliate store,\n",
       "  the database,\n",
       "  the search condition information,\n",
       "  a search result,\n",
       "  the extracted affiliate store information,\n",
       "  a transmitter,\n",
       "  the search result],\n",
       " {'affiliate store': [each affiliate store],\n",
       "  'affiliate store information of at least one affiliate store': [the affiliate store information of at least one affiliate store],\n",
       "  'collected transaction count information': [the collected transaction count information],\n",
       "  'database': [the database],\n",
       "  'database for storing affiliate store information': [a database for storing affiliate store information],\n",
       "  'extracted affiliate store information': [the extracted affiliate store information],\n",
       "  'generator': [a generator],\n",
       "  'mobile communication terminal': [a mobile communication terminal],\n",
       "  'number of transactions': [a number of transactions],\n",
       "  'predetermined criteria': [a predetermined criteria],\n",
       "  'receiver for collecting transaction count information': [a receiver for collecting transaction count information],\n",
       "  'search condition information': [the search condition information],\n",
       "  'search result': [a search result, the search result],\n",
       "  'server for providing affiliate store information': [A server for providing affiliate store information],\n",
       "  'transmitter': [a transmitter]})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_spacy_entity_finder(nlp_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'at least a portion': [at least a portion,\n",
       "  at least a portion,\n",
       "  at least a portion],\n",
       " 'circuit': [the circuit],\n",
       " 'communication module': [a communication module],\n",
       " 'connector': [the connector, the connector],\n",
       " 'contactless devices': [contactless devices],\n",
       " 'cover': [A cover],\n",
       " 'mobile device': [a mobile device,\n",
       "  a mobile device,\n",
       "  the mobile device,\n",
       "  the mobile device,\n",
       "  the mobile device,\n",
       "  the mobile device],\n",
       " 'port': [a port],\n",
       " 'rear surface': [a rear surface],\n",
       " 'rear surface form': [the rear surface form],\n",
       " 'side surfaces': [the side surfaces, the side surfaces],\n",
       " 'transactions': [transactions]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_entity_finder(nlp_docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([A cover, a mobile device, comprising:\n",
       "  side surfaces, a portion, a mobile device, a rear surface, a portion, a rear surface, the mobile device, the side surfaces, the side surfaces, the rear surface form, an opening, a portion, the mobile device, a connector, a port, the mobile device, a circuit, the connector, a communication module, the communication module configured to execute transactions with contactless devices, the mobile device, the circuit],\n",
       " {'circuit': [a circuit, the circuit],\n",
       "  'communication module': [a communication module],\n",
       "  'communication module configured to execute transactions with contactless devices': [the communication module configured to execute transactions with contactless devices],\n",
       "  'connector': [a connector, the connector],\n",
       "  'cover': [A cover],\n",
       "  'mobile device': [a mobile device,\n",
       "   the mobile device,\n",
       "   the mobile device,\n",
       "   the mobile device,\n",
       "   the mobile device],\n",
       "  'mobile device, comprising:\\nside surfaces': [a mobile device, comprising:\n",
       "   side surfaces],\n",
       "  'opening': [an opening],\n",
       "  'port': [a port],\n",
       "  'portion': [a portion, a portion, a portion],\n",
       "  'rear surface': [a rear surface, a rear surface],\n",
       "  'rear surface form': [the rear surface form],\n",
       "  'side surfaces': [the side surfaces, the side surfaces]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_spacy_entity_finder(nlp_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Algorithm\n",
    "\n",
    "What do we know:\n",
    "* A DET or a NOUN will always form part of an entity.\n",
    "* A plural noun may not start with a DET.\n",
    "* An entity will consist of consecutive tokens.\n",
    "* The world following a DET will be part of the entity.\n",
    "* Each determinant can only be linked to one of the nouns in front of it before the next determinant or [\";\", \":\", \".\"] (and possibly \",\").\n",
    "* Entities with a \"the\" determinant should have occurred before.\n",
    "* There are no overlaps.\n",
    "* We can be more confident if a phrase is repeated.\n",
    "* We can be more confident still if the phrase is repeated that initially starts with \"a\" and the next occurrence starts with \"the\" or \"said\".\n",
    "* \"said\" should be taken as a DET.\n",
    "* There will be between 1 and number of NOUNS entities.\n",
    "* The boundary of an entity will be marked by NOUN NOTNOUN - however this pattern can also occur as part of the noun phrase for the entity.\n",
    "* Entity text sequences will not cross a \":\" or \";\".\n",
    "* Occurrences of an entity will have matching text including at least a matching noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definite constraints for a well-formed claim:\n",
    "* A NOUN will always form part of an entity;\n",
    "* A singular noun will have a determinant;\n",
    "* An entity will consist of consecutive tokens.\n",
    "* There are no overlaps in occurrences - a word can only be linked to a single entity.\n",
    "* There will be between 1 and number of NOUNS entities.\n",
    "* Entity text sequences will not cross a \":\" or \";\" or \".\" (and possibly a \",\").\n",
    "* The boundary of an entity will be marked by NOUN NOTNOUN - however this pattern can also occur as part of the noun phrase for the entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the probability of a set of entities, $ \\boldsymbol E $, given a claim as a sequence of words, $ \\boldsymbol W $: $$ P(\\boldsymbol E | \\boldsymbol W) $$   \n",
    "\n",
    "In fact we want to calculate: $$ \\underset{\\boldsymbol E}{\\operatorname{argmax}} P(\\boldsymbol E | \\boldsymbol W) $$\n",
    "\n",
    "Our claim has a length $ N $:$$\\boldsymbol W = (\\boldsymbol w_0, \\boldsymbol w_1, ..., \\boldsymbol w_{N})$$\n",
    "\n",
    "$N$ may be calculated as the length of the claim in tokens.\n",
    "\n",
    "Each word $\\boldsymbol w_i$ has:\n",
    "* text - $t_i$;\n",
    "* a simple POS tag - $pos_i$;\n",
    "* a more detailed POS tag - $posplus_i$;\n",
    "* a lemma (i.e. a normalised word form) - $lemma_i$; and\n",
    "* dependeny tree information - $dep_i$.\n",
    "\n",
    "I.e. $$ \\boldsymbol w_i = (t_i, pos_i, posplus_i, lemma_i, dep_i) $$\n",
    "\n",
    "We have $ M $ entities: $$\\boldsymbol E = (e_0, e_1, ..., e_{M})$$ \n",
    "\n",
    "where $\\boldsymbol e_0 $ indicates \"no related entity\" or a \"null\" token. $M$ is not known but will be greater than 2 and less than a number of nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An occurrence is a set of consecutive tokens: $$ \\boldsymbol o_k = [\\boldsymbol w_i, \\boldsymbol w_{i+1}, ..., \\boldsymbol w_{i+L_{k}}] $$ where $L_k$ is the length of occurrence $k$ which begins at word index $i$.\n",
    "\n",
    "$$ \\boldsymbol W = [o_1, o_2, ..., o_K] $$ where there are $K$ total occurrences in the claim. However, we don't know $K$ for sure. \n",
    "\n",
    "We do know the number of nouns $N_{noun}$. And we know $1 \\leqslant K \\leqslant N_{noun}$. Also $M \\leqslant K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An entity can have:\n",
    "* a set of one or more occurrences;\n",
    "* a string representation - possibly equal to common text across the set of occurrences;\n",
    "* a number (e.g. be singular or plural).\n",
    "\n",
    "An entity may be though of as a class label that is applied to a word: $$ \\sum_{i=0}^M p(e_i | w) = 1 $$\n",
    "\n",
    "We know that $ p(e_0 | pos = {DET}) = p(e_0 | pos = {NOUN}) = 0 $, i.e. that determinants and nouns will be assigned to some entity. We also know $ p(e_0 | t = \";\") = p(e_0 | t = \":\") = p(e_0 | t = \".\") = 1$.\n",
    "\n",
    "Entities are primarily just groupings of word spans, wherein the grouping creates a discrete entity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{i=0}^M P(\\boldsymbol o_k | e_i) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decomposing using Bayes' Rule: \n",
    "\n",
    "$$ \\underset{\\boldsymbol e}{\\operatorname{argmax}} P(\\boldsymbol e | \\boldsymbol w) = {P(\\boldsymbol w | \\boldsymbol e) P(\\boldsymbol e)}/ P(\\boldsymbol w)$$ \n",
    "\n",
    "where we can ignore the denominator as we are looking for argmax: $$ \\underset{\\boldsymbol e}{\\operatorname{argmax}} P(\\boldsymbol e | \\boldsymbol w) = {P(\\boldsymbol w | \\boldsymbol e) P(\\boldsymbol e)}$$\n",
    "\n",
    "In other models $P(\\boldsymbol w | \\boldsymbol e)$ and $P(\\boldsymbol e)$ may be approximated by a product of transitions (e.g. as per a hidden markov model). However, we have dependencies across sets of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each determinant can only be linked to one of the nouns in front of it before the next determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by setting each noun as a separate entity? And marking the tokens that are not an entity? Or look at confident selections e.g. DET NOUN [:;.,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can maybe start with a binary classification: $\\boldsymbol e = [0,1]$? No, we can confidently apply a positive determination but our negative determination is unknown, i.e. a word that is not positively marked may still form part of an entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate $M$ by counting the number of \"a\"/\"an\" determinants + the number of multiple nouns.  \n",
    "\n",
    "Issue multiple nouns are often introduced by \"a X of Ys\".  \n",
    "\n",
    "Also we have \"at least one X\" and \"one or more Ys\" - these may not be introduced by \"a\" or \"an\" and \"at least one\" may be referred to again as \"the at least one\".  \n",
    "\n",
    "Can we use an estimate of number of determinants as a lower bound?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works fairly well for a lower bound / initial estimate.  \n",
    "\n",
    "We can cross check later for missing plural nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we model a sequential constraint? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word $w_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pass\n",
      "\n",
      " [{}]\n",
      "1 [{}]\n",
      ". [{0: 1}]\n",
      "A [{0: 0}]\n",
      "method [{0: 0}]\n",
      "of [{}]\n",
      "forming [{}]\n",
      "conductive [{}]\n",
      "traces [{0: 0}]\n",
      "on [{}]\n",
      "a [{0: 0}]\n",
      "touch [{0: 0}]\n",
      "sensor [{0: 0}]\n",
      "panel [{0: 0}]\n",
      ", [{0: 1}]\n",
      "comprising [{}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{}]\n",
      "forming [{}]\n",
      "and [{}]\n",
      "patterning [{}]\n",
      "a [{0: 0}]\n",
      "stackup [{0: 0}]\n",
      "of [{}]\n",
      "a [{0: 0}]\n",
      "first [{}]\n",
      "conductive [{}]\n",
      "material [{0: 0}]\n",
      "over [{}]\n",
      "a [{0: 0}]\n",
      "substrate [{0: 0}]\n",
      "in [{}]\n",
      "one [{0: 0}]\n",
      "or [{}]\n",
      "more [{}]\n",
      "border [{0: 0}]\n",
      "areas [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "touch [{0: 0}]\n",
      "sensor [{0: 0}]\n",
      "panel [{0: 0}]\n",
      "to [{}]\n",
      "create [{}]\n",
      "one [{0: 0}]\n",
      "or [{}]\n",
      "more [{}]\n",
      "traces [{0: 0}]\n",
      "having [{}]\n",
      "widths [{0: 0}]\n",
      "maximized [{}]\n",
      "so [{}]\n",
      "that [{}]\n",
      "the [{0: 0}]\n",
      "one [{}]\n",
      "or [{}]\n",
      "more [{}]\n",
      "traces [{0: 0}]\n",
      "and [{}]\n",
      "any [{0: 0}]\n",
      "separation [{0: 0}]\n",
      "areas [{0: 0}]\n",
      "between [{}]\n",
      "the [{0: 0}]\n",
      "traces [{0: 0}]\n",
      "in [{}]\n",
      "any [{0: 0}]\n",
      "particular [{}]\n",
      "portion [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "border [{0: 0}]\n",
      "area [{0: 0}]\n",
      "occupy [{}]\n",
      "a [{0: 0}]\n",
      "full [{}]\n",
      "width [{0: 0}]\n",
      "of [{}]\n",
      "that [{0: 0}]\n",
      "portion [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "border [{0: 0}]\n",
      "area [{0: 0}]\n",
      "; [{0: 1}]\n",
      "and [{}]\n",
      "\n",
      " [{}]\n",
      "forming [{}]\n",
      "and [{}]\n",
      "patterning [{}]\n",
      "a [{0: 0}]\n",
      "layer [{0: 0}]\n",
      "of [{}]\n",
      "a [{0: 0}]\n",
      "second [{}]\n",
      "conductive [{}]\n",
      "material [{0: 0}]\n",
      "over [{}]\n",
      "the [{0: 0}]\n",
      "substrate [{0: 0}]\n",
      "to [{}]\n",
      "create [{}]\n",
      "one [{0: 0}]\n",
      "or [{}]\n",
      "more [{}]\n",
      "rows [{0: 0}]\n",
      ", [{0: 1}]\n",
      "each [{0: 0}]\n",
      "row [{0: 0}]\n",
      "coupled [{}]\n",
      "to [{}]\n",
      "a [{0: 0}]\n",
      "different [{}]\n",
      "trace [{0: 0}]\n",
      ", [{0: 1}]\n",
      "the [{0: 0}]\n",
      "rows [{0: 0}]\n",
      "forming [{}]\n",
      "part [{0: 0}]\n",
      "of [{}]\n",
      "a [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{}]\n",
      "sensors [{0: 0}]\n",
      "on [{}]\n",
      "the [{0: 0}]\n",
      "touch [{0: 0}]\n",
      "sensor [{0: 0}]\n",
      "panel [{0: 0}]\n",
      ". [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "Second pass\n",
      "panel is e_0=0\n",
      "Next - sensor is e_0=0\n",
      "area is e_0=0\n",
      "Next - border is e_0=0\n",
      "rows is e_0=0\n",
      "Next - one is e_0=0\n",
      "Found - one or more rows\n",
      "trace is e_0=0\n",
      "Next - a is e_0=0\n",
      "Found - a different trace\n",
      "panel is e_0=0\n",
      "Next - sensor is e_0=0\n",
      "\n",
      " [{}]\n",
      "1 [{}]\n",
      ". [{0: 1}]\n",
      "A [{0: 0}]\n",
      "method [{0: 0}]\n",
      "of [{}]\n",
      "forming [{}]\n",
      "conductive [{}]\n",
      "traces [{0: 0}]\n",
      "on [{}]\n",
      "a [{0: 0}]\n",
      "touch [{0: 0}]\n",
      "sensor [{0: 0}]\n",
      "panel [{0: 0}]\n",
      ", [{0: 1}]\n",
      "comprising [{}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{}]\n",
      "forming [{}]\n",
      "and [{}]\n",
      "patterning [{}]\n",
      "a [{0: 0}]\n",
      "stackup [{0: 0}]\n",
      "of [{}]\n",
      "a [{0: 0}]\n",
      "first [{}]\n",
      "conductive [{}]\n",
      "material [{0: 0}]\n",
      "over [{}]\n",
      "a [{0: 0}]\n",
      "substrate [{0: 0}]\n",
      "in [{}]\n",
      "one [{0: 0}]\n",
      "or [{}]\n",
      "more [{}]\n",
      "border [{0: 0}]\n",
      "areas [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "touch [{0: 0}]\n",
      "sensor [{0: 0}]\n",
      "panel [{0: 0}]\n",
      "to [{}]\n",
      "create [{}]\n",
      "one [{0: 0}]\n",
      "or [{}]\n",
      "more [{}]\n",
      "traces [{0: 0}]\n",
      "having [{}]\n",
      "widths [{0: 0}]\n",
      "maximized [{}]\n",
      "so [{}]\n",
      "that [{}]\n",
      "the [{0: 0}]\n",
      "one [{}]\n",
      "or [{}]\n",
      "more [{}]\n",
      "traces [{0: 0}]\n",
      "and [{}]\n",
      "any [{0: 0}]\n",
      "separation [{0: 0}]\n",
      "areas [{0: 0}]\n",
      "between [{}]\n",
      "the [{0: 0}]\n",
      "traces [{0: 0}]\n",
      "in [{}]\n",
      "any [{0: 0}]\n",
      "particular [{}]\n",
      "portion [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "border [{0: 0}]\n",
      "area [{0: 0}]\n",
      "occupy [{}]\n",
      "a [{0: 0}]\n",
      "full [{}]\n",
      "width [{0: 0}]\n",
      "of [{}]\n",
      "that [{0: 0}]\n",
      "portion [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "border [{0: 0}]\n",
      "area [{0: 0}]\n",
      "; [{0: 1}]\n",
      "and [{}]\n",
      "\n",
      " [{}]\n",
      "forming [{}]\n",
      "and [{}]\n",
      "patterning [{}]\n",
      "a [{0: 0}]\n",
      "layer [{0: 0}]\n",
      "of [{}]\n",
      "a [{0: 0}]\n",
      "second [{}]\n",
      "conductive [{}]\n",
      "material [{0: 0}]\n",
      "over [{}]\n",
      "the [{0: 0}]\n",
      "substrate [{0: 0}]\n",
      "to [{}]\n",
      "create [{}]\n",
      "one [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "rows [{0: 0}]\n",
      ", [{0: 1}]\n",
      "each [{0: 0}]\n",
      "row [{0: 0}]\n",
      "coupled [{}]\n",
      "to [{}]\n",
      "a [{0: 0}]\n",
      "different [{0: 0}]\n",
      "trace [{0: 0}]\n",
      ", [{0: 1}]\n",
      "the [{0: 0}]\n",
      "rows [{0: 0}]\n",
      "forming [{}]\n",
      "part [{0: 0}]\n",
      "of [{}]\n",
      "a [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{}]\n",
      "sensors [{0: 0}]\n",
      "on [{}]\n",
      "the [{0: 0}]\n",
      "touch [{0: 0}]\n",
      "sensor [{0: 0}]\n",
      "panel [{0: 0}]\n",
      ". [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "Third pass\n",
      ". is e_0=1 - looking back\n",
      ", is e_0=1 - looking back\n",
      "Next - panel is e_0=0 and noun\n",
      "Next - a is e_0=0 and DET\n",
      "Last break set to 14\n",
      ": is e_0=1 - looking back\n",
      "; is e_0=1 - looking back\n",
      "Next - area is e_0=0 and noun\n",
      "Next - the is e_0=0 and DET\n",
      "Last break set to 84\n",
      ", is e_0=1 - looking back\n",
      "Next - rows is e_0=0 and noun\n",
      ", is e_0=1 - looking back\n",
      "Next - trace is e_0=0 and noun\n",
      "Next - a is e_0=0 and DET\n",
      "Last break set to 114\n",
      ". is e_0=1 - looking back\n",
      "Next - panel is e_0=0 and noun\n",
      "Next - the is e_0=0 and DET\n",
      "Last break set to 129\n",
      "\n",
      "--------\n",
      "\n",
      "Looking for matches for 'a touch sensor panel'\n",
      "Looking for matches for 'the border area'\n",
      "Looking for matches for 'a different trace'\n",
      "Looking for matches for 'the touch sensor panel'\n",
      "Unique entities include ['touch sensor panel', 'border area', 'different trace']\n",
      "[a touch sensor panel, the touch sensor panel]\n",
      "[the border area]\n",
      "[a different trace]\n",
      "\n",
      " [{}]\n",
      "1 [{}]\n",
      ". [{0: 1}]\n",
      "A [{0: 0}]\n",
      "method [{0: 0}]\n",
      "of [{}]\n",
      "forming [{}]\n",
      "conductive [{}]\n",
      "traces [{0: 0}]\n",
      "on [{}]\n",
      "a [{0: 0, 1: 1}]\n",
      "touch [{0: 0, 1: 1}]\n",
      "sensor [{0: 0, 1: 1}]\n",
      "panel [{0: 0, 1: 1}]\n",
      ", [{0: 1}]\n",
      "comprising [{}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{}]\n",
      "forming [{}]\n",
      "and [{}]\n",
      "patterning [{}]\n",
      "a [{0: 0}]\n",
      "stackup [{0: 0}]\n",
      "of [{}]\n",
      "a [{0: 0}]\n",
      "first [{}]\n",
      "conductive [{}]\n",
      "material [{0: 0}]\n",
      "over [{}]\n",
      "a [{0: 0}]\n",
      "substrate [{0: 0}]\n",
      "in [{}]\n",
      "one [{0: 0}]\n",
      "or [{}]\n",
      "more [{}]\n",
      "border [{0: 0}]\n",
      "areas [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "touch [{0: 0}]\n",
      "sensor [{0: 0}]\n",
      "panel [{0: 0}]\n",
      "to [{}]\n",
      "create [{}]\n",
      "one [{0: 0}]\n",
      "or [{}]\n",
      "more [{}]\n",
      "traces [{0: 0}]\n",
      "having [{}]\n",
      "widths [{0: 0}]\n",
      "maximized [{}]\n",
      "so [{}]\n",
      "that [{}]\n",
      "the [{0: 0}]\n",
      "one [{}]\n",
      "or [{}]\n",
      "more [{}]\n",
      "traces [{0: 0}]\n",
      "and [{}]\n",
      "any [{0: 0}]\n",
      "separation [{0: 0}]\n",
      "areas [{0: 0}]\n",
      "between [{}]\n",
      "the [{0: 0}]\n",
      "traces [{0: 0}]\n",
      "in [{}]\n",
      "any [{0: 0}]\n",
      "particular [{}]\n",
      "portion [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "border [{0: 0}]\n",
      "area [{0: 0}]\n",
      "occupy [{}]\n",
      "a [{0: 0}]\n",
      "full [{}]\n",
      "width [{0: 0}]\n",
      "of [{}]\n",
      "that [{0: 0}]\n",
      "portion [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0, 2: 1}]\n",
      "border [{0: 0, 2: 1}]\n",
      "area [{0: 0, 2: 1}]\n",
      "; [{0: 1}]\n",
      "and [{}]\n",
      "\n",
      " [{}]\n",
      "forming [{}]\n",
      "and [{}]\n",
      "patterning [{}]\n",
      "a [{0: 0}]\n",
      "layer [{0: 0}]\n",
      "of [{}]\n",
      "a [{0: 0}]\n",
      "second [{}]\n",
      "conductive [{}]\n",
      "material [{0: 0}]\n",
      "over [{}]\n",
      "the [{0: 0}]\n",
      "substrate [{0: 0}]\n",
      "to [{}]\n",
      "create [{}]\n",
      "one [{0: 0}]\n",
      "or [{0: 0}]\n",
      "more [{0: 0}]\n",
      "rows [{0: 0}]\n",
      ", [{0: 1}]\n",
      "each [{0: 0}]\n",
      "row [{0: 0}]\n",
      "coupled [{}]\n",
      "to [{}]\n",
      "a [{0: 0, 3: 1}]\n",
      "different [{0: 0, 3: 1}]\n",
      "trace [{0: 0, 3: 1}]\n",
      ", [{0: 1}]\n",
      "the [{0: 0}]\n",
      "rows [{0: 0}]\n",
      "forming [{}]\n",
      "part [{0: 0}]\n",
      "of [{}]\n",
      "a [{0: 0}]\n",
      "plurality [{0: 0}]\n",
      "of [{}]\n",
      "sensors [{0: 0}]\n",
      "on [{}]\n",
      "the [{0: 0, 1: 1}]\n",
      "touch [{0: 0, 1: 1}]\n",
      "sensor [{0: 0, 1: 1}]\n",
      "panel [{0: 0, 1: 1}]\n",
      ". [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "{'that the one or more traces': [that the one or more traces], 'one or more rows': [one or more rows], 'layer': [a layer], 'traces': [the traces], 'separation areas': [any separation areas], 'border area': [the border area, the border area], 'second conductive material': [a second conductive material], 'touch sensor panel': [a touch sensor panel, the touch sensor panel, the touch sensor panel], 'plurality': [a plurality], 'particular portion': [any particular portion], 'method': [A method], 'portion': [that portion], 'part': [part], 'conductive traces': [conductive traces], 'substrate': [a substrate, the substrate], 'sensors': [sensors], 'one or more traces': [one or more traces], 'different trace': [a different trace], 'stackup': [a stackup], 'one or more border areas': [one or more border areas], 'widths': [widths], 'full width': [a full width], 'first conductive material': [a first conductive material]}\n",
      "([A method of forming conductive traces, a touch sensor panel, a stackup, a first conductive material, a substrate, the touch sensor panel, , any separation areas, the traces, any particular portion, the border area, a full width, that portion, the border area, a layer, a second conductive material, the substrate, each row, a different trace, the rows forming part, a plurality of sensors], {'': [], 'different trace': [a different trace], 'plurality of sensors': [a plurality of sensors], 'touch sensor panel': [a touch sensor panel, the touch sensor panel], 'layer': [a layer], 'substrate': [a substrate, the substrate], 'row': [each row], 'rows forming part': [the rows forming part], 'full width': [a full width], 'traces': [the traces], 'separation areas': [any separation areas], 'portion': [that portion], 'stackup': [a stackup], 'second conductive material': [a second conductive material], 'method of forming conductive traces': [A method of forming conductive traces], 'first conductive material': [a first conductive material], 'border area': [the border area, the border area], 'particular portion': [any particular portion]})\n"
     ]
    }
   ],
   "source": [
    "# This is our good algorithm\n",
    "\n",
    "# Start with all words relate to no entities\n",
    "p_all_e_word = dict()\n",
    "\n",
    "def check_start_phrase(token, doc):\n",
    "    \"\"\" Check for start of phrases 'at least one' and 'one or more' as determinant.\n",
    "    \n",
    "    Return true if located.\"\"\"\n",
    "    i = token.i\n",
    "    condition = (\n",
    "        doc[i:i+3].text.lower() == \"at least one\" or\n",
    "        doc[i:i+3].text.lower() == \"one or more\"\n",
    "    )\n",
    "    condition = condition and (doc[i-1].text.lower() != \"the\")\n",
    "    return condition\n",
    "\n",
    "def is_det(token, doc):\n",
    "    \"\"\" Wrapper function for determinant check.\"\"\"\n",
    "    # Add 'said' as custom determination\n",
    "    condition = (token.pos == DET or token.text == \"said\")\n",
    "    # Alternatively we can have the start phrases as above\n",
    "    condition = (condition or check_start_phrase(token, doc))\n",
    "    # Add check for 'a)' and 'a.' - this is not a det\n",
    "    condition = condition and (doc[token.i:token.i+2].text.lower() not in ['a)', 'a.'])\n",
    "    return condition\n",
    "\n",
    "# 6 is a good test claim\n",
    "doc = nlp_docs[4]\n",
    "noun_count = list()\n",
    "\n",
    "# Initialise probabilities\n",
    "for token in doc:\n",
    "    p_all_e_word[token] = dict()\n",
    "\n",
    "# First parse of tokens\n",
    "for token in doc:\n",
    "    p_all_e_word[token] = dict()\n",
    "    if is_det(token, doc):\n",
    "        p_all_e_word[token][0] = 0\n",
    "        # Also set next word as an entity\n",
    "        p_all_e_word[doc[token.i+1]][0] = 0\n",
    "    elif token.pos == NOUN:\n",
    "        p_all_e_word[token][0] = 0\n",
    "        noun_count.append(token)\n",
    "    if token.text in [\":\",\";\",\".\", \",\"]:\n",
    "        p_all_e_word[token][0] = 1\n",
    "\n",
    "print(\"First pass\")\n",
    "for token in doc:\n",
    "    print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n') \n",
    "#print(\"Number of nouns = {0}\".format(len(noun_count)))\n",
    "\n",
    "print(\"Second pass\")\n",
    "# Second parse to fill in probabilities given hard end points\n",
    "last_break = 0\n",
    "for token in doc:\n",
    "    # Look for hard end points\n",
    "    if p_all_e_word[token].get(0, None) == 1:\n",
    "        # Look at previous token\n",
    "        previous_word = doc[token.i - 1]\n",
    "        # If it is set as an entity (i.e. e_0=0)\n",
    "        if p_all_e_word[previous_word].get(0, None) == 0:\n",
    "            print(\"{0} is e_0=0\".format(previous_word))\n",
    "            # Go back to next e_0=0 entry\n",
    "            for j in range(token.i-2, last_break, -1):\n",
    "                if p_all_e_word[doc[j]].get(0, None) == 0:\n",
    "                    # If last e_0=0 entry is a determinant\n",
    "                    print(\"Next - {0} is e_0=0\".format(doc[j]))\n",
    "                    if is_det(doc[j], doc):\n",
    "                        print(\"Found - {0}\".format(doc[j:token.i]))\n",
    "                        # Set in between tokens as e_0=0\n",
    "                        for k in range(j+1, token.i):\n",
    "                            p_all_e_word[doc[k]][0] = 0 \n",
    "                    # Finish when hitting previous e_0 token\n",
    "                    break\n",
    "\n",
    "                    \n",
    "for token in doc:\n",
    "    print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n') \n",
    "#print(\"Number of nouns = {0}\".format(len(noun_count)))\n",
    "\n",
    "print(\"Third pass\") \n",
    "# Third parse - take any DET ... NOUN <boundary> portions\n",
    "last_break = 0\n",
    "spans_to_match = list()\n",
    "for token in doc:\n",
    "    # Look for hard end points\n",
    "    if p_all_e_word[token].get(0, None) == 1:\n",
    "        print(\"{0} is e_0=1 - looking back\".format(token))\n",
    "        # See if there is a continuous set of e_0 = 0 ending with a noun and starting with a DET\n",
    "        for j in range(token.i-1, last_break, -1):\n",
    "            # Look for e_0=0 and noun (do we need to limit to singular noun)\n",
    "            if p_all_e_word[doc[j]].get(0, None) == 0 and doc[j].pos == NOUN:\n",
    "                print(\"Next - {0} is e_0=0 and noun\".format(doc[j]))\n",
    "                # Look back for DET\n",
    "                for k in range(j, last_break, -1):\n",
    "                    if p_all_e_word[doc[k]].get(0, None) != 0:\n",
    "                        # Exit if don't meet a e_0 token\n",
    "                        break\n",
    "                    elif doc[k].pos == DET:\n",
    "                        print(\"Next - {0} is e_0=0 and DET\".format(doc[k]))\n",
    "                        print(\"Last break set to {0}\".format(token.i))\n",
    "                        spans_to_match.append((k,j+1))\n",
    "                        last_break = token.i\n",
    "                        break\n",
    "                break\n",
    "                \n",
    "print(\"\\n--------\\n\")\n",
    "\n",
    "entity_dict = dict()\n",
    "for stm in spans_to_match:\n",
    "    print(\"Looking for matches for '{0}'\".format(doc[stm[0]:stm[1]]))\n",
    "    non_det_string = doc[stm[0]+1:stm[1]].text\n",
    "    if non_det_string not in entity_dict.keys():\n",
    "        entity_dict[non_det_string] = list()\n",
    "    entity_dict[non_det_string].append(doc[stm[0]:stm[1]])\n",
    "    \n",
    "print(\"Unique entities include {0}\".format(list(entity_dict.keys()))) \n",
    "\n",
    "#Add entity values\n",
    "entity_count = 0\n",
    "for entity_string in entity_dict.keys():\n",
    "    entity_occurrences = entity_dict[entity_string]\n",
    "    entity_count += 1\n",
    "    print(entity_dict[entity_string])\n",
    "    for occurrence in entity_occurrences:\n",
    "        for token in occurrence:\n",
    "            p_all_e_word[token][entity_count] = 1\n",
    "        \n",
    "for token in doc:\n",
    "    print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n') \n",
    "\n",
    "# Compare with existing methods \n",
    "print(np_entity_finder(doc))\n",
    "print(simple_spacy_entity_finder(doc))\n",
    "\n",
    "# Here look for matches that are found with both methods that have an [\"a ...\",\"the ...\", ...] pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristics:\n",
    "* \"for\" marks a non-entity [e_0=1]\n",
    "* \"DET X of ...\" [e_0=0]\n",
    "* \"in X with\" [e_0=1]\n",
    "* \"at least one\" / \"one or more\" [e_0=0]\n",
    "* lemma = \\[\"comprise\", \"have\", \"be\", \"include\"\\] [e_0=1]\n",
    "* \"where\" in token.text [e_0=1] (e.g. \"where or wherein\")\n",
    "* \"associated with\" [e_0=1]\n",
    "* \"configured/adapted to\" [e_0=0]\n",
    "\n",
    "Also watch out for \"each of the plurality of X\" or \"at least one of the plurality of X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import NUM\n",
    "\n",
    "# Could we change this to slice on a key? Probably\n",
    "def sliceodict(d, i):\n",
    "    \"\"\"Slice an ordered dict based on a passed index.\n",
    "    list[:i] for an ordered dict\n",
    "    \"\"\"\n",
    "    temp_dict = {k:v for j, (k,v) in enumerate(d.items()) if j < i}\n",
    "    return OrderedDict(sorted(temp_dict.items(), key=lambda t: t[1][0][0].i))\n",
    "\n",
    "# We want to set these if they are not already set\n",
    "def set_probability(token, p_all_e_word, entity, new_value):\n",
    "    \"\"\" Set probability value if not set already\"\"\"\n",
    "    if entity not in p_all_e_word[token].keys():\n",
    "        if sum([v for k, v in p_all_e_word[token]] + new_value) <= 1: \n",
    "            p_all_e_word[token][entity] = new_value\n",
    "    return p_all_e_word\n",
    "            \n",
    "\n",
    "def heuristics(token, doc, p_all_e_word):\n",
    "    \"\"\" Apply heuristics to mark entity probabilities\"\"\"\n",
    "    entity_stop_chars = [\"\\n\",\":\",\";\",\".\", \",\"]\n",
    "    # Set stop characters as non-entity\n",
    "    if token.text in entity_stop_chars:\n",
    "        p_all_e_word[token][0] = 1\n",
    "    \n",
    "    # Set noun as entity\n",
    "    if token.pos == NOUN and p_all_e_word[token].get(0, None) != 1:\n",
    "        p_all_e_word[token][0] = 0\n",
    "    \n",
    "    # 'for' is an entity boundary\n",
    "    if token.lemma_ == \"for\":\n",
    "        p_all_e_word[token][0] = 1\n",
    "    \n",
    "    # \"comprise\", \"have\", \"be\", \"include\" do not relate to an entity\n",
    "    if token.lemma_ in [\"comprise\", \"have\", \"be\", \"include\"]:\n",
    "        p_all_e_word[token][0] = 1\n",
    "    \n",
    "    # \"where\" and \"wherein\" do not relate to an entity\n",
    "    if \"where\" in token.lemma_:\n",
    "         p_all_e_word[token][0] = 1\n",
    "    \n",
    "    # Look ahead - check not at end\n",
    "    if token.i < (len(doc)-1):\n",
    "        \n",
    "        # \"configured/adapted to\" do not relate to an entity\n",
    "        if doc[token.i+1].lemma_ == \"to\" and token.lemma_ in [\"configure\", \"adapt\"]:\n",
    "            p_all_e_word[token][0] = 1\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 1\n",
    "    \n",
    "    if token.i < (len(doc)-2):\n",
    "        # Set DETs as entity\n",
    "        if (\n",
    "            token.pos == DET or token.text == \"said\"\n",
    "        ) and (\n",
    "            doc[token.i:token.i+2].text.lower() not in ['a)', 'a.']\n",
    "        ):\n",
    "            p_all_e_word[token][0] = 0\n",
    "            p_all_e_word[doc[token.i+1]][0] = 0\n",
    "            \n",
    "        # DET X of .. relates to an entity\n",
    "        if token.pos == DET and doc[token.i+2].lemma_ == \"of\":\n",
    "            p_all_e_word[token][0] = 0\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 0\n",
    "            # Set of\n",
    "            p_all_e_word[doc[token.i + 2]][0] = 0\n",
    "            # Set term after off\n",
    "            p_all_e_word[doc[token.i + 3]][0] = 0\n",
    "            \n",
    "        # \"in X with\" does not relate to an entity\n",
    "        if token.lemma_ == \"in\" and doc[token.i+2].lemma_ == \"with\":\n",
    "            p_all_e_word[token][0] = 1\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 1\n",
    "            p_all_e_word[doc[token.i + 2]][0] = 1\n",
    "            \n",
    "        # Associated with does not relate to an entity\n",
    "        if doc[token.i:token.i+2].text.lower() == \"associated with\":\n",
    "            p_all_e_word[token][0] = 1\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 1\n",
    "    \n",
    "    if token.i < (len(doc)-3):\n",
    "        # \"at least NUM\" / \"NUM or more\" relates to an entity\n",
    "        if doc[token.i:token.i + 2].text.lower() == \"at least\" and doc[token.i + 2].pos == NUM:\n",
    "            p_all_e_word[token][0] = 0\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 0\n",
    "            p_all_e_word[doc[token.i + 2]][0] = 0\n",
    "        if doc[token.i+1:token.i + 3].text.lower() == \"or more\" and token.pos == NUM:\n",
    "            p_all_e_word[token][0] = 0\n",
    "            p_all_e_word[doc[token.i + 1]][0] = 0\n",
    "            p_all_e_word[doc[token.i + 2]][0] = 0\n",
    "    \n",
    "    return p_all_e_word\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm generally is:\n",
    "* Mark as entity or not based on rules;\n",
    "* Look back from DET or punct break [':',';',',','.'] - set as non-entity until noun is found;\n",
    "* Look at noun phrase chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pass - entity label heuristics\n",
      "\n",
      " [{0: 1}]\n",
      "1 [{}]\n",
      ". [{0: 1}]\n",
      "A [{0: 0}]\n",
      "game [{0: 0}]\n",
      "system [{0: 0}]\n",
      "comprising [{0: 1}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "an [{0: 0}]\n",
      "interactive [{0: 0}]\n",
      "apparatus [{0: 0}]\n",
      "having [{0: 1}]\n",
      "a [{0: 0}]\n",
      "communication [{0: 0}]\n",
      "system [{0: 0}]\n",
      "; [{0: 1}]\n",
      "and [{}]\n",
      "\n",
      " [{0: 1}]\n",
      "an [{0: 0}]\n",
      "interactive [{0: 0}]\n",
      "application [{0: 0}]\n",
      ", [{0: 1}]\n",
      "the [{0: 0}]\n",
      "interactive [{0: 0}]\n",
      "application [{0: 0}]\n",
      "and [{}]\n",
      "the [{0: 0}]\n",
      "interactive [{0: 0}]\n",
      "apparatus [{0: 0}]\n",
      "are [{0: 1}]\n",
      "independently [{}]\n",
      "operable [{}]\n",
      "to [{}]\n",
      "provide [{}]\n",
      "an [{0: 0}]\n",
      "offline [{0: 0}]\n",
      "and [{}]\n",
      "an [{0: 0}]\n",
      "online [{0: 0}]\n",
      "experience [{0: 0}]\n",
      ", [{0: 1}]\n",
      "wherein [{0: 1}]\n",
      "at [{0: 0}]\n",
      "least [{0: 0}]\n",
      "one [{0: 0}]\n",
      "of [{}]\n",
      "the [{0: 0}]\n",
      "interactive [{0: 0}]\n",
      "application [{0: 0}]\n",
      "and [{}]\n",
      "interactive [{}]\n",
      "apparatus [{0: 0}]\n",
      "is [{0: 1}]\n",
      "configured [{}]\n",
      "modify [{}]\n",
      "its [{}]\n",
      "operation [{0: 0}]\n",
      "based [{}]\n",
      "on [{}]\n",
      "the [{0: 0}]\n",
      "experience [{0: 0}]\n",
      "of [{0: 0}]\n",
      "the [{0: 0}]\n",
      "other [{0: 0}]\n",
      ". [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "Second pass - look for DET ... NOUN groupings\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      ". is e_0=1 or DET - looking back\n",
      "Step back token - 1 with pos - 95\n",
      "Setting non-Noun\n",
      "A is e_0=1 or DET - looking back\n",
      "Step back token - . with pos - 95\n",
      "Setting non-Noun\n",
      "Step back token - 1 with pos - 95\n",
      "Setting non-Noun\n",
      "comprising is e_0=1 or DET - looking back\n",
      "Step back token - system with pos - 90\n",
      ": is e_0=1 or DET - looking back\n",
      "Step back token - comprising with pos - 98\n",
      "Setting non-Noun\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      "Step back token - : with pos - 95\n",
      "Setting non-Noun\n",
      "Step back token - comprising with pos - 98\n",
      "Setting non-Noun\n",
      "an is e_0=1 or DET - looking back\n",
      "Step back token - \n",
      " with pos - 101\n",
      "Setting non-Noun\n",
      "Step back token - : with pos - 95\n",
      "Setting non-Noun\n",
      "Step back token - comprising with pos - 98\n",
      "Setting non-Noun\n",
      "having is e_0=1 or DET - looking back\n",
      "Step back token - apparatus with pos - 90\n",
      "a is e_0=1 or DET - looking back\n",
      "Step back token - having with pos - 98\n",
      "Setting non-Noun\n",
      "; is e_0=1 or DET - looking back\n",
      "Step back token - system with pos - 90\n",
      "\n",
      " is e_0=1 or DET - looking back\n",
      "Step back token - and with pos - 87\n",
      "Setting non-Noun\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      "an is e_0=1 or DET - looking back\n",
      "Step back token - \n",
      " with pos - 101\n",
      "Setting non-Noun\n",
      "Step back token - and with pos - 87\n",
      "Setting non-Noun\n",
      "Step back token - ; with pos - 95\n",
      "Setting non-Noun\n",
      ", is e_0=1 or DET - looking back\n",
      "Step back token - application with pos - 90\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - , with pos - 95\n",
      "Setting non-Noun\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - and with pos - 87\n",
      "Setting non-Noun\n",
      "Step back token - application with pos - 90\n",
      "are is e_0=1 or DET - looking back\n",
      "Step back token - apparatus with pos - 90\n",
      "an is e_0=1 or DET - looking back\n",
      "Step back token - provide with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - to with pos - 92\n",
      "Setting non-Noun\n",
      "Step back token - operable with pos - 82\n",
      "Setting non-Noun\n",
      "Step back token - independently with pos - 84\n",
      "Setting non-Noun\n",
      "Step back token - are with pos - 98\n",
      "Setting non-Noun\n",
      "an is e_0=1 or DET - looking back\n",
      "Step back token - and with pos - 87\n",
      "Setting non-Noun\n",
      "Step back token - offline with pos - 90\n",
      ", is e_0=1 or DET - looking back\n",
      "Step back token - experience with pos - 90\n",
      "wherein is e_0=1 or DET - looking back\n",
      "Step back token - , with pos - 95\n",
      "Setting non-Noun\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - of with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - one with pos - 91\n",
      "Setting non-Noun\n",
      "Step back token - least with pos - 84\n",
      "Setting non-Noun\n",
      "Step back token - at with pos - 84\n",
      "Setting non-Noun\n",
      "Step back token - wherein with pos - 84\n",
      "Setting non-Noun\n",
      "Step back token - , with pos - 95\n",
      "Setting non-Noun\n",
      "is is e_0=1 or DET - looking back\n",
      "Step back token - apparatus with pos - 90\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - on with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - based with pos - 98\n",
      "Setting non-Noun\n",
      "Step back token - operation with pos - 90\n",
      "the is e_0=1 or DET - looking back\n",
      "Step back token - of with pos - 83\n",
      "Setting non-Noun\n",
      "Step back token - experience with pos - 90\n",
      ". is e_0=1 or DET - looking back\n",
      "Step back token - other with pos - 82\n",
      "Setting non-Noun\n",
      "Step back token - the with pos - 88\n",
      "Setting non-Noun\n",
      "Step back token - of with pos - 83\n",
      "Setting non-Noun\n",
      "\n",
      " [{0: 1}]\n",
      "1 [{0: 1}]\n",
      ". [{0: 1}]\n",
      "A [{0: 0}]\n",
      "game [{0: 0}]\n",
      "system [{0: 0}]\n",
      "comprising [{0: 1}]\n",
      ": [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "an [{0: 0}]\n",
      "interactive [{0: 0}]\n",
      "apparatus [{0: 0}]\n",
      "having [{0: 1}]\n",
      "a [{0: 0}]\n",
      "communication [{0: 0}]\n",
      "system [{0: 0}]\n",
      "; [{0: 1}]\n",
      "and [{0: 1}]\n",
      "\n",
      " [{0: 1}]\n",
      "an [{0: 0}]\n",
      "interactive [{0: 0}]\n",
      "application [{0: 0}]\n",
      ", [{0: 1}]\n",
      "the [{0: 0}]\n",
      "interactive [{0: 0}]\n",
      "application [{0: 0}]\n",
      "and [{0: 1}]\n",
      "the [{0: 0}]\n",
      "interactive [{0: 0}]\n",
      "apparatus [{0: 0}]\n",
      "are [{0: 1}]\n",
      "independently [{0: 1}]\n",
      "operable [{0: 1}]\n",
      "to [{0: 1}]\n",
      "provide [{0: 1}]\n",
      "an [{0: 0}]\n",
      "offline [{0: 0}]\n",
      "and [{0: 1}]\n",
      "an [{0: 0}]\n",
      "online [{0: 0}]\n",
      "experience [{0: 0}]\n",
      ", [{0: 1}]\n",
      "wherein [{0: 1}]\n",
      "at [{0: 1}]\n",
      "least [{0: 1}]\n",
      "one [{0: 1}]\n",
      "of [{0: 1}]\n",
      "the [{0: 0}]\n",
      "interactive [{0: 0}]\n",
      "application [{0: 0}]\n",
      "and [{}]\n",
      "interactive [{}]\n",
      "apparatus [{0: 0}]\n",
      "is [{0: 1}]\n",
      "configured [{}]\n",
      "modify [{}]\n",
      "its [{}]\n",
      "operation [{0: 0}]\n",
      "based [{0: 1}]\n",
      "on [{0: 1}]\n",
      "the [{0: 0}]\n",
      "experience [{0: 0}]\n",
      "of [{0: 1}]\n",
      "the [{0: 1}]\n",
      "other [{0: 1}]\n",
      ". [{0: 1}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "and [{}]\n",
      "interactive [{}]\n",
      "configured [{}]\n",
      "modify [{}]\n",
      "its [{}]\n",
      "\n",
      "\n",
      " [{}]\n",
      "Extracted possible occurrences:\n",
      "\n",
      "[A game system, an interactive apparatus, a communication system, an interactive application, the interactive application, the interactive apparatus, an offline, an online experience, the interactive application, , apparatus, , , , operation, the experience, ]\n",
      "\n",
      "1. A game system comprising:\n",
      "an interactive apparatus having a communication system; and\n",
      "an interactive application, the interactive application and the interactive apparatus are independently operable to provide an offline and an online experience, wherein at least one of the interactive application and interactive apparatus is configured modify its operation based on the experience of the other.\n",
      "\n",
      "\n",
      "OrderedDict([('game system', [A game system]), ('interactive apparatus', [an interactive apparatus, the interactive apparatus]), ('communication system', [a communication system]), ('interactive application', [an interactive application, the interactive application, the interactive application]), ('offline', [an offline]), ('online experience', [an online experience]), ('apparatus', [apparatus]), ('operation', [operation]), ('experience', [the experience])])\n",
      "Found entity 'the experience' with incorrect antecedence\n",
      "Found match with online experience\n",
      "OrderedDict([('game system', [A game system]), ('interactive apparatus', [an interactive apparatus, the interactive apparatus]), ('communication system', [a communication system]), ('interactive application', [an interactive application, the interactive application, the interactive application]), ('offline', [an offline]), ('online experience', [an online experience, the experience]), ('apparatus', [apparatus]), ('operation', [operation])])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "# Try out with adding heuristics\n",
    "\n",
    "# Start with all words relate to no entities\n",
    "p_all_e_word = dict()\n",
    "\n",
    "# Initialise probabilities\n",
    "for token in doc:\n",
    "    p_all_e_word[token] = dict()\n",
    "\n",
    "# Is the order of our labelling important? Probably as we overwrite following probs    \n",
    "    \n",
    "# This can be combined with first pass easily - similar checks\n",
    "print(\"First pass - entity label heuristics\")\n",
    "for token in doc:\n",
    "    p_all_e_word = heuristics(token, doc, p_all_e_word)\n",
    "    print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n')\n",
    "   \n",
    "print(\"Second pass - look for DET ... NOUN groupings\") \n",
    "# Second parse - take any DET ... NOUN <boundary> portions\n",
    "last_break = 0\n",
    "spans_to_match = list()\n",
    "for token in doc:\n",
    "    # Look for hard end points or DET\n",
    "    if (p_all_e_word[token].get(0, None) == 1) or (token.pos == DET):\n",
    "        print(\"{0} is e_0=1 or DET - looking back\".format(token))\n",
    "        # Step back marking as e_0=1 until first NOUN      \n",
    "        for j in range(token.i-1, last_break, -1):\n",
    "            print(\"Step back token - {0} with pos - {1}\".format(doc[j], doc[j].pos))\n",
    "            if doc[j].pos != NOUN:\n",
    "                print(\"Setting non-Noun\")\n",
    "                p_all_e_word[doc[j]][0] = 1\n",
    "            else:\n",
    "                last_break = j\n",
    "                break\n",
    "    # Look at grouping from DET\n",
    "    if is_det(token, doc):\n",
    "        # Tweak for \"at least X\" and \"X or more\"\n",
    "        if (\n",
    "            doc[token.i:token.i + 2].text.lower() == \"at least\" and doc[token.i + 2].pos == NUM\n",
    "        ) or (\n",
    "            doc[token.i+1:token.i + 3].text.lower() == \"or more\" and token.pos == NUM\n",
    "        ):\n",
    "            #print(\"Head index set to {0}\".format())\n",
    "            head_index = doc[token.i+2].head.i\n",
    "        else: \n",
    "            head_index = token.head.i\n",
    "        possible_entity = True\n",
    "        # Step through intermediate tokens between current and head\n",
    "        for j in range(token.i, head_index):\n",
    "            # If head is outside of DET ... end_NOUN sequence\n",
    "            if doc[j].head.i < token.i and doc[j].head.i > head_index:\n",
    "                # Check for nested portions\n",
    "                possible_entity = False\n",
    "        if possible_entity:\n",
    "            for k in range(token.i, head_index + 1):\n",
    "                p_all_e_word[doc[k]][0] = 0 \n",
    "    # Need to adapt the above for at least one ... X and one or more ... Xs - \"at\" > head > \"least\" > \"one\" > X\n",
    "    # Look at plural nouns\n",
    "    if token.tag_ == \"NNS\":\n",
    "        print(\"Located plural noun: {0}\".format(token))\n",
    "        #Step back and mark as e_0=0 any preceding word that has the token as a head\n",
    "        for j in range(token.i-1, 0, -1):\n",
    "            print(doc[j], doc[j].head.i, p_all_e_word[doc[j]])\n",
    "            if p_all_e_word[doc[j]]:\n",
    "                break\n",
    "            elif (\n",
    "                doc[j].head.i == token.i\n",
    "            ):\n",
    "                print(\"Setting {0} as e_0=0\".format(doc[j]))\n",
    "                p_all_e_word[doc[j]][0] = 0\n",
    "    \n",
    "for token in doc:\n",
    "    print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n') \n",
    "    \n",
    "for token in doc:\n",
    "    if not p_all_e_word[token]:\n",
    "        print(token.text, \"[{0}]\".format(p_all_e_word[token]), end = '\\n') \n",
    "    \n",
    "print(\"Extracted possible occurrences:\\n\")\n",
    "poss_occ = list()\n",
    "for token in doc[1:]:\n",
    "    # If transition\n",
    "    if p_all_e_word[token].get(0, 0) == 0 and p_all_e_word[doc[token.i-1]].get(0, 1) == 1:\n",
    "        # Add consecutive e_0=0\n",
    "        for j in range(token.i, len(doc)+1):\n",
    "            if p_all_e_word[doc[j]].get(0, 1) != 0:\n",
    "                poss_occ.append(doc[token.i:j])\n",
    "                break\n",
    "\n",
    "print(poss_occ)\n",
    "\n",
    "# Matching occurrences\n",
    "entity_dict = dict()\n",
    "# Now group by unique\n",
    "for entity in poss_occ:\n",
    "    np_start = entity.start\n",
    "    # Ignore the determinant \n",
    "    if doc[np_start].pos == DET:\n",
    "        np_start += 1\n",
    "    # Generate a string representation excluding the determinant\n",
    "    np_string = doc[np_start:entity.end].text.lower()                        \n",
    "    if np_string:\n",
    "        if np_string not in entity_dict.keys():\n",
    "            entity_dict[np_string] = list()          \n",
    "        entity_dict[np_string].append(entity)\n",
    "\n",
    "print(doc)\n",
    "# print(entity_dict)\n",
    "\n",
    "# Quick function to sort entities by occurrence\n",
    "# Need to sort the keys by the index of the first word in the first entry\n",
    "ordered_entities = OrderedDict(sorted(entity_dict.items(), key=lambda t: t[1][0][0].i))\n",
    "\n",
    "print(ordered_entities)\n",
    "\n",
    "# Look for duplict entities and merge\n",
    "new_o_e = ordered_entities.copy()\n",
    "for i, (entity_string, occurrences) in enumerate(ordered_entities.items()):\n",
    "    # Check if first entry in occurrences begins with the\n",
    "    current_occurrence = occurrences[0]\n",
    "    if current_occurrence[0].lemma_ == \"the\":\n",
    "        print(\"Found entity '{0}' with incorrect antecedence\".format(current_occurrence))\n",
    "        for previous_entity_string, previous_occurrences in sliceodict(ordered_entities, i).items():\n",
    "            first_entry = previous_occurrences[0]\n",
    "            # Check to see if head of occurrence with \"the\" agrees with head of previous occurrence\n",
    "            # print(first_entry[-1].text.lower(), first_entry[-1].tag_, \n",
    "                  # current_occurrence[-1].text.lower(), current_occurrence[-1].tag_)\n",
    "            if (\n",
    "                first_entry[-1].text.lower() == current_occurrence[-1].text.lower()\n",
    "            ) and (\n",
    "                first_entry[-1].tag == current_occurrence[-1].tag\n",
    "            ):\n",
    "                # print(first_entry[0].head, first_entry[-1], first_entry[-1].tag_)\n",
    "                print(\"Found match with {0}\". format(previous_entity_string))\n",
    "                # Merge entries in copy of dict\n",
    "                new_o_e[previous_entity_string] += occurrences\n",
    "                new_o_e.pop(entity_string)\n",
    "                \n",
    "print(new_o_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('game system', [A game system]),\n",
       "             ('interactive apparatus',\n",
       "              [an interactive apparatus, the interactive apparatus]),\n",
       "             ('communication system', [a communication system])])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliceodict(ordered_entities, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 the experience\n",
      "experience experience NN\n",
      "online experience\n",
      "OrderedDict([('game system', [A game system]), ('interactive apparatus', [an interactive apparatus, the interactive apparatus]), ('communication system', [a communication system]), ('interactive application', [an interactive application, the interactive application, the interactive application]), ('offline', [an offline]), ('online experience', [an online experience, the experience]), ('apparatus', [apparatus]), ('operation', [operation]), ('experience', [the experience])])\n",
      "OrderedDict([('game system', [A game system]), ('interactive apparatus', [an interactive apparatus, the interactive apparatus]), ('communication system', [a communication system]), ('interactive application', [an interactive application, the interactive application, the interactive application]), ('offline', [an offline]), ('online experience', [an online experience, the experience]), ('apparatus', [apparatus]), ('operation', [operation])])\n"
     ]
    }
   ],
   "source": [
    "new_o_e = ordered_entities.copy()\n",
    "for i, (entity_string, occurrences) in enumerate(ordered_entities.items()):\n",
    "    # Check if first entry in occurrences begins with the\n",
    "    current_occurrence = occurrences[0]\n",
    "    if current_occurrence[0].lemma_ == \"the\":\n",
    "        print(i, current_occurrence)\n",
    "        for previous_entity_string, previous_occurrences in sliceodict(ordered_entities, i).items():\n",
    "            first_entry = previous_occurrences[0]\n",
    "            # Check to see if head of occurrence with \"the\" agrees with head of previous occurrence\n",
    "            # print(first_entry[-1].text.lower(), first_entry[-1].tag_, \n",
    "                  # current_occurrence[-1].text.lower(), current_occurrence[-1].tag_)\n",
    "            if (\n",
    "                first_entry[-1].text.lower() == current_occurrence[-1].text.lower()\n",
    "            ) and (\n",
    "                first_entry[-1].tag == current_occurrence[-1].tag\n",
    "            ):\n",
    "                print(first_entry[0].head, first_entry[-1], first_entry[-1].tag_)\n",
    "                print(previous_entity_string)\n",
    "                # Merge entries in copy of dict\n",
    "                new_o_e[previous_entity_string] += occurrences\n",
    "                new_o_e.pop(entity_string)\n",
    "                \n",
    "print(ordered_entities)\n",
    "print(new_o_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "system system\n",
    "apparatus apparatus\n",
    "system system\n",
    "application application\n",
    "offline offline\n",
    "experience experience\n",
    "application apparatus\n",
    "modify operation\n",
    "```\n",
    "A possible entity cross-check - first_entry[0].head = first_entry[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper - http://cogprints.org/5025/1/nrc-48727.pdf - suggests a two-phase process:\n",
    "* Generate a \"gazetteer\" (a list of named entities) - similar to our first stage of simple_entity_extraction method;\n",
    "* Disambiguate names in \"gazetteer\" (this is similar to our second stage of simple_entity_extraction method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do:\n",
    "* Need to look for entities with different names to merge based on number agreement and head agreement and presence before use of the in claim. (e.g. \"An elongate container....the container\" or \"a plurality of notches....the respective notches\"\n",
    "* Also look for unassigned words between det and noun - mark as e_0=1 look for head = noun (two image storage regions).\n",
    "* Look an phrases such as \"an offline and an online experience\" - currently split as \"an offline\" and \"an online experience\" - need to merge to \"an offline experience\" and \"an online experience\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for spans between e_0=1 - these must contain an occurrence. If there is only one DET-NOUN (check NP using head) or X NNS (check again using NP head) - that must be an entity. (This is the second parse?)\n",
    "\n",
    "Can we look backwards from DET? Anything that is not a NOUN is e_0=1?\n",
    "\n",
    "Plurals need looking at:\n",
    "```\n",
    "user [{0: 0}]\n",
    "defined [{}]\n",
    "rules [{0: 0}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 \n",
      " SPACE 1 1 SP\n",
      "1 1 1 NUM 1 1 CD\n",
      "- 2 - SYM 10 3 SYM\n",
      "10 3 10 NUM 1 1 CD\n",
      ". 4 . PUNCT 1 1 .\n",
      "( 5 ( PUNCT 1 1 -LRB-\n",
      "canceled 6 cancel VERB 1 1 VBN\n",
      ") 7 ) PUNCT canceled 6 -RRB-\n",
      "\n",
      " 8 \n",
      " SPACE ) 7 SP\n"
     ]
    }
   ],
   "source": [
    "# Look at POS and head for each token\n",
    "for token in doc:\n",
    "    print(token.text, token.i, token.lemma_, token.pos_, token.head.text, token.head.i, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When matching what to do with 'the time scale' and 'the time scale display information' or 'a project' and:\n",
    "```\n",
    "A [{0: 0}]\n",
    "project [{}]\n",
    "information [{0: 0}]\n",
    "display [{0: 0}]\n",
    "device [{0: 0}]\n",
    ", [{0: 1}]\n",
    "comprising [{}]\n",
    ": [{0: 1}]\n",
    "```\n",
    "Only look for e_0 stretches of same number with matching pos and text? (Are we now getting to look at transitions?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are issues with \"(a)\" and \"a.\"\n",
    "\n",
    "Also \"response\" from \"in response\".\n",
    "\n",
    "Check det is not working for \"at least one\"\n",
    "\n",
    "We can iterate back from where e_0 = 1 - tokens between a last noun and determinant will be part of an entity. We can then match those across the claim. This is the simple entity finder but stepping back at [:;,.] as well as DET.  \n",
    "Pattern is:\n",
    "* If next step back is e_0=0;\n",
    "* If next e_0=0 is a check_det=True;\n",
    "* Fill in inbetween as e_0=0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another pattern is \"DET X FOR [phrase]\" - this is one entity? But contains references to other entities\n",
    "```\n",
    "a [{0: 0}]\n",
    "system [{0: 0}]\n",
    "for [{}]\n",
    "providing [{}]\n",
    "a [{0: 0}]\n",
    "plurality [{0: 0}]\n",
    "of [{}]\n",
    "football [{0: 0}]\n",
    "player [{0: 0}]\n",
    "types [{0: 0}]\n",
    "from [{}]\n",
    "which [{}]\n",
    "a [{0: 0}]\n",
    "football [{0: 0}]\n",
    "player [{0: 0}]\n",
    "type [{0: 0}]\n",
    "is [{}]\n",
    "selected \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a for clause as the whole entity string - e.g. \"A method for modeling electrical characteristics of cells having given circuit elements\" and \"a layout of cells having at least one cell\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Claims - Extracting Steps\n",
    "\n",
    "Let's try something similar for method steps. This may give us some clues for \"comprising\" X, Y, Z structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive algorithm:\n",
    "* Look for a comprising relating to a method.\n",
    "* Look for VERBs following at least one of ['\\n', ';', ','] after the comprising colon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Extracted Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May not be able to use dependency information from spaCy for looking at structure - link between \"comprising\" and subsequent features appear lost over long claim text.  \n",
    "\n",
    "Head of verb will give you the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity system has relationship comprising\n",
      "Entity apparatus has relationship having\n"
     ]
    }
   ],
   "source": [
    "# Look for [\"comprise\", \"have\", \"be\", \"include\"]\n",
    "for token in doc:\n",
    "    if token.lemma_ in [\"comprise\", \"have\", \"include\"]:\n",
    "        print(\"Entity {0} has relationship {1}\".format(token.head.text, token.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
