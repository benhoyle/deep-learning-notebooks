{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Claims - Keras Tokeniser TFIDF + FFNN\n",
    "\n",
    "In this post we will see if we can build some classifiers to predict a first level patent classification from the claim text.\n",
    "\n",
    "In particular, here we will look at applying a standard feed forward neural network on a TFIDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"raw_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n1. A detector for atrial fibrillation or flutter (AF) comprising: \\nan impedance measuring unit comprising a measuring input, to which an atrial electrode line having an electrode for a unipolar measurement of an impedance in an atrium is connected and is implemented to generate an atrial impedance signal, obtained in a unipolar manner, in such a way that an impedance signal for each atrial cycle, comprising an atrial contraction and a following relaxation of said atrium, comprises multiple impedance values detected at different instants within a particular atrial cycle; \\nsaid impedance measuring unit comprising a signal input, via which a ventricle signal is to be supplied to said detector, which reflects instants of ventricular contractions in chronological assignment to said impedance signal; \\nan analysis unit configured to average multiple sequential impedance signal sections of a unipolar atrial impedance signal, which are each delimited by two sequential ventricular contractions, with one another, and to determine a maximum amplitude of an averaged unipolar atrial impedance signal section, \\nsaid analysis unit configured to compare said maximum amplitude to a comparison value, and if said maximum amplitude of said averaged unipolar atrial impedance signal is less than said comparison value, generate an AF suspicion signal. \\n\\n',\n",
       " 'A')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a play with the Keras text tokenizer (as per here - https://keras.io/preprocessing/text/#tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11238, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "docs = [d[0] for d in data]\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer(num_words=10000)\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "encoded_claims = t.texts_to_matrix(docs, mode='tfidf')\n",
    "\n",
    "encoded_claims.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the texts_to_matrix function we need to apply a feed-forward neural network rather than a RNN, as we have for each claim a set of word counts scaled by document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much faster than my old methods! But hey, I learnt some stuff about tokenisation.  \n",
    "\n",
    "We can use the \"num_words\" parameter as passed into the Tokenizer to restrict to the top n words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our classes are now a matrix of (11238, 8)\n",
      "Original label: A; Converted label: [ 1.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "Y_class = [d[1] for d in data]\n",
    "\n",
    "# encode class values as integers\n",
    "label_e = LabelEncoder()\n",
    "label_e.fit(Y_class)\n",
    "encoded_Y = label_e.transform(Y_class)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "Y_data = to_categorical(encoded_Y)\n",
    "print(\"Our classes are now a matrix of {0}\".format(Y_data.shape))\n",
    "print(\"Original label: {0}; Converted label: {1}\".format(Y_class[0], Y_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into training (80%) and testing (20%)\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(encoded_claims, Y_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear some memory\n",
    "del data, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input dimension for our claim representation is 10000\n"
     ]
    }
   ],
   "source": [
    "input_dim = encoded_claims.shape[1]\n",
    "print(\"Our input dimension for our claim representation is {0}\".format(input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our output dimension is 8\n"
     ]
    }
   ],
   "source": [
    "no_classes = Y_data.shape[1]\n",
    "print(\"Our output dimension is {0}\".format(no_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.2, input_shape=(input_dim,)))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8990 samples, validate on 2248 samples\n",
      "Epoch 1/5\n",
      "8990/8990 [==============================] - 436s - loss: 0.0635 - acc: 0.9802 - val_loss: 1.5020 - val_acc: 0.6303\n",
      "Epoch 2/5\n",
      "8990/8990 [==============================] - 386s - loss: 0.0476 - acc: 0.9853 - val_loss: 1.5905 - val_acc: 0.6312\n",
      "Epoch 3/5\n",
      "8990/8990 [==============================] - 671s - loss: 0.0436 - acc: 0.9868 - val_loss: 1.6466 - val_acc: 0.6357\n",
      "Epoch 4/5\n",
      "8990/8990 [==============================] - 235s - loss: 0.0431 - acc: 0.9861 - val_loss: 1.6857 - val_acc: 0.6299\n",
      "Epoch 5/5\n",
      "8990/8990 [==============================] - 158s - loss: 0.0363 - acc: 0.9890 - val_loss: 1.7460 - val_acc: 0.6268\n",
      "Accuracy: 62.68%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is overfitting on our training set. We should try adding dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"2017-10-28 dense_dropout.hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that this approach doesn't seem to work. Likely because there are limited patterns at the word level to be detected by a CNN.  \n",
    "\n",
    "This would likely work better at the character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code for building a confusion matrix\n",
    "\n",
    "def get_confusion_matrix_one_hot(model_results, truth):\n",
    "    '''model_results and truth should be for one-hot format, i.e, have >= 2 columns,\n",
    "    where truth is 0/1, and max along each row of model_results is model result\n",
    "    '''\n",
    "    assert model_results.shape == truth.shape\n",
    "    num_outputs = truth.shape[1]\n",
    "    confusion_matrix = np.zeros((num_outputs, num_outputs), dtype=np.int32)\n",
    "    predictions = np.argmax(model_results,axis=1)\n",
    "    assert len(predictions)==truth.shape[0]\n",
    "\n",
    "    for actual_class in range(num_outputs):\n",
    "        idx_examples_this_class = truth[:,actual_class]==1\n",
    "        prediction_for_this_class = predictions[idx_examples_this_class]\n",
    "        for predicted_class in range(num_outputs):\n",
    "            count = np.sum(prediction_for_this_class==predicted_class)\n",
    "            confusion_matrix[actual_class, predicted_class] = count\n",
    "    assert np.sum(confusion_matrix)==len(truth)\n",
    "    assert np.sum(confusion_matrix)==np.sum(truth)\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'A': 1777,\n",
       "         'B': 1449,\n",
       "         'C': 865,\n",
       "         'D': 54,\n",
       "         'E': 269,\n",
       "         'F': 735,\n",
       "         'G': 3335,\n",
       "         'H': 2754})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Let's check how our data is distributed across the classes\n",
    "class_count = Counter(Y_class)\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = get_confusion_matrix_one_hot(predict, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[212,  37,  30,   0,   3,   9,  30,  15],\n",
       "       [ 15, 161,   9,   0,   5,  20,  46,  31],\n",
       "       [ 29,  29,  90,   0,   0,   2,  16,  12],\n",
       "       [  1,   6,   0,   2,   0,   0,   1,   2],\n",
       "       [  6,  19,   0,   0,  21,   3,   8,   3],\n",
       "       [  8,  25,   2,   0,   3,  62,  11,  17],\n",
       "       [ 15,  28,   6,   0,   1,   9, 512, 129],\n",
       "       [  9,  22,   7,   0,   2,  14, 123, 370]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could we reduce the classification to a series of binary classifications?\n",
    "\n",
    "We also really ought to test against a naive Bayes classifier to use as a baseline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8990, 26142) (8990, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8990,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_labelint = np.argmax(Y_train, axis=1)\n",
    "Y_train_labelint.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For sklearn we need the integer Y labels\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train, Y_train_labelint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60186832740213525"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = clf.predict(X_test)\n",
    "np.mean(predicted == np.argmax(Y_test, axis=1))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the naive Bayes classifier is 60%. \n",
    "\n",
    "So I only gain 3% by using a complicated neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# To use the confusion matrix I need to convert predicted into one hot\n",
    "predicted_one_hot = np.zeros(\n",
    "    (len(predicted), Y_test.shape[1]),\n",
    "    dtype='float32')\n",
    "for i, p in enumerate(predicted):\n",
    "    predicted_one_hot[i, p] = 1\n",
    "print(predicted_one_hot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm_NB = get_confusion_matrix_one_hot(predicted_one_hot, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[123,  59,  69,  12,  17,  15,  35,   6],\n",
       "       [  2, 174,  20,   3,  16,  42,  18,  12],\n",
       "       [  9,  17, 131,   5,   1,   5,   3,   7],\n",
       "       [  0,   5,   2,   3,   1,   0,   0,   1],\n",
       "       [  0,  14,   2,   0,  31,   9,   4,   0],\n",
       "       [  0,  25,   3,   0,   7,  86,   2,   5],\n",
       "       [  4,  37,  18,   4,  12,  21, 490, 114],\n",
       "       [  4,  48,  14,   0,   8,  35, 123, 315]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for NB:\n",
    "```\n",
    "       [123,  59,  69,  12,  17,  15,  35,   6],\n",
    "       [  2, 174,  20,   3,  16,  42,  18,  12],\n",
    "       [  9,  17, 131,   5,   1,   5,   3,   7],\n",
    "       [  0,   5,   2,   3,   1,   0,   0,   1],\n",
    "       [  0,  14,   2,   0,  31,   9,   4,   0],\n",
    "       [  0,  25,   3,   0,   7,  86,   2,   5],\n",
    "       [  4,  37,  18,   4,  12,  21, 490, 114],\n",
    "       [  4,  48,  14,   0,   8,  35, 123, 315]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_1 (Dropout)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2000)              20002000  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               1000500   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 4008      \n",
      "=================================================================\n",
      "Total params: 21,006,508\n",
      "Trainable params: 21,006,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create a deeper model\n",
    "deep_model = Sequential()\n",
    "deep_model.add(Dropout(0.2, input_shape=(input_dim,)))\n",
    "deep_model.add(Dense(2000, activation='relu'))\n",
    "deep_model.add(Dropout(0.5))\n",
    "deep_model.add(Dense(500, activation='relu'))\n",
    "deep_model.add(Dense(no_classes, activation='softmax'))\n",
    "deep_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(deep_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8990 samples, validate on 2248 samples\n",
      "Epoch 1/20\n",
      "8990/8990 [==============================] - 259s - loss: 0.1062 - acc: 0.9640 - val_loss: 1.6931 - val_acc: 0.6366\n",
      "Epoch 2/20\n",
      "8990/8990 [==============================] - 265s - loss: 0.1047 - acc: 0.9676 - val_loss: 1.7964 - val_acc: 0.6170\n",
      "Epoch 3/20\n",
      "8990/8990 [==============================] - 265s - loss: 0.0828 - acc: 0.9744 - val_loss: 1.7712 - val_acc: 0.6370\n",
      "Epoch 4/20\n",
      "8990/8990 [==============================] - 251s - loss: 0.0784 - acc: 0.9765 - val_loss: 1.8022 - val_acc: 0.6321\n",
      "Epoch 5/20\n",
      "8990/8990 [==============================] - 238s - loss: 0.0782 - acc: 0.9778 - val_loss: 1.9791 - val_acc: 0.6223\n",
      "Epoch 6/20\n",
      "8990/8990 [==============================] - 237s - loss: 0.0688 - acc: 0.9781 - val_loss: 1.9709 - val_acc: 0.6241\n",
      "Epoch 7/20\n",
      "8990/8990 [==============================] - 235s - loss: 0.0812 - acc: 0.9778 - val_loss: 1.9101 - val_acc: 0.6388\n",
      "Epoch 8/20\n",
      "8990/8990 [==============================] - 262s - loss: 0.0730 - acc: 0.9795 - val_loss: 2.0165 - val_acc: 0.6339\n",
      "Epoch 9/20\n",
      "8990/8990 [==============================] - 259s - loss: 0.0744 - acc: 0.9808 - val_loss: 1.9885 - val_acc: 0.6379\n",
      "Epoch 10/20\n",
      "8990/8990 [==============================] - 241s - loss: 0.0669 - acc: 0.9802 - val_loss: 1.8990 - val_acc: 0.6259\n",
      "Epoch 11/20\n",
      "8990/8990 [==============================] - 268s - loss: 0.0553 - acc: 0.9848 - val_loss: 2.0387 - val_acc: 0.6210\n",
      "Epoch 12/20\n",
      "8990/8990 [==============================] - 237s - loss: 0.0713 - acc: 0.9793 - val_loss: 2.0629 - val_acc: 0.6210\n",
      "Epoch 13/20\n",
      "8990/8990 [==============================] - 242s - loss: 0.0596 - acc: 0.9851 - val_loss: 2.1261 - val_acc: 0.6254\n",
      "Epoch 14/20\n",
      "8990/8990 [==============================] - 239s - loss: 0.0567 - acc: 0.9862 - val_loss: 2.1996 - val_acc: 0.6246\n",
      "Epoch 15/20\n",
      "8990/8990 [==============================] - 239s - loss: 0.0585 - acc: 0.9849 - val_loss: 2.1448 - val_acc: 0.6210\n",
      "Epoch 16/20\n",
      "8990/8990 [==============================] - 239s - loss: 0.0515 - acc: 0.9859 - val_loss: 2.1265 - val_acc: 0.6134\n",
      "Epoch 17/20\n",
      "8990/8990 [==============================] - 239s - loss: 0.0553 - acc: 0.9840 - val_loss: 2.1661 - val_acc: 0.6299\n",
      "Epoch 18/20\n",
      "8990/8990 [==============================] - 239s - loss: 0.0492 - acc: 0.9854 - val_loss: 2.1442 - val_acc: 0.6281\n",
      "Epoch 19/20\n",
      "8990/8990 [==============================] - 239s - loss: 0.0494 - acc: 0.9859 - val_loss: 2.1292 - val_acc: 0.6335\n",
      "Epoch 20/20\n",
      "8990/8990 [==============================] - 240s - loss: 0.0446 - acc: 0.9871 - val_loss: 2.1069 - val_acc: 0.6268\n",
      "Accuracy: 62.68%\n"
     ]
    }
   ],
   "source": [
    "deep_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = deep_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_model.save(\"2017-10-28 deep_dense_dropout.hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
