{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Encoding Using LSTMs\n",
    "\n",
    "Here we will see if we can generate a word encoding using LSTMs.\n",
    "\n",
    "The idea is that we take tokenised words of the form: ```<W>the</W>``` and feed into an auto-encoder at a character-level. We then take the output of the trained encoder as a word encoder to output a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# We'll start with some claim 1 data\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import pickle\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# We've built a character encoding tool in patentdata we can use to help us\n",
    "from patentdata.models.chardict import CharDict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: \n",
    "\n",
    "We can probably truncate / split out words at length 15 or 20 and only affect a few words\n",
    "\n",
    "Use this post to help remind us of the next bit: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build our own truncation routine - we want to split sequences over 20 into several sequences\n",
    "def limit_length(sequences, limit):\n",
    "    \"\"\" Limits sequences to limit by splitting sequences over limit into sub-sequences.\"\"\"\n",
    "    new_seqs = list()\n",
    "    for seq in sequences:\n",
    "        if len(seq) > limit:\n",
    "            cut_seq = seq\n",
    "            while cut_seq:\n",
    "                new_seqs.append(cut_seq[:limit])\n",
    "                cut_seq = cut_seq[limit:]\n",
    "        else:\n",
    "            new_seqs.append(seq)\n",
    "    return new_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our longest sequence is 124 integers long\n",
      "Our longest sequence is 20 integers long\n",
      "The maximum integer value (i.e. char index) is: 82\n",
      "X saved\n"
     ]
    }
   ],
   "source": [
    "x_filename = \"X_pre_pad.pkl\"\n",
    "int_word_filename = \"int_words.pkl\"\n",
    "\n",
    "cd = CharDict()\n",
    "\n",
    "if os.path.isfile(x_filename):\n",
    "    # If we have saved our X vector, load it\n",
    "    with open(x_filename, \"rb\") as f:\n",
    "        X = pickle.load(f)\n",
    "else:\n",
    "    if os.path.isfile(int_word_filename):\n",
    "         with open(int_word_filename, \"rb\") as f:\n",
    "            int_words = pickle.load(f)\n",
    "    else:\n",
    "        # Load data from scratch-ish\n",
    "        with open(\"raw_data.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        print(data[0])\n",
    "\n",
    "        # We'll use spacy for tokenisation etc\n",
    "        import spacy\n",
    "        nlp = spacy.load('en')\n",
    "\n",
    "        # Convert each claim into a spacy 'doc' object\n",
    "        spacy_data = [nlp(d[0]) for d in data]\n",
    "        del data\n",
    "        # First let's look at some statistics regarding our words\n",
    "        word_lengths = list()\n",
    "        for doc in spacy_data:\n",
    "            word_lengths = word_lengths + [len(word) for word in doc]\n",
    "\n",
    "        bins = np.linspace(0, 20, 20)\n",
    "        plt.hist(word_lengths, bins)\n",
    "        plt.ylabel('Frequency');\n",
    "\n",
    "        # Let's test our character converter\n",
    "        print(\"Here's 'test' in integers - {0}\".format(cd.text2int(\"test\")))\n",
    "\n",
    "        # Let's test our character converter\n",
    "        print(\"Here's 'Test' in integers - {0}\".format(cd.text2int(\"Test\")))\n",
    "\n",
    "        # Let's generate a data set consisting of words in our claim\n",
    "        words = [word for doc in spacy_data for word in doc]\n",
    "        del spacy_data\n",
    "        print(\"There are {0} words\".format(len(words)))\n",
    "        print(\"Here is a sample: {0}\".format(words[50:60]))\n",
    "\n",
    "        print(\n",
    "            \"Here is a sample of a word as an integer sequence: {0}\".format(\n",
    "                [cd.startwordint] + cd.text2int(words[0].text) + [cd.endwordint]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "\n",
    "        # We need to convert each of our words to character indices and add <W> and </W> indices to the beginning and end\n",
    "        int_words = [[cd.startwordint] + cd.text2int(word.text) + [cd.endwordint] for word in words]\n",
    "        del words\n",
    "        # Let's save our int_words for future use\n",
    "        with open(\"int_words.pkl\", \"wb\") as f:\n",
    "            pickle.dump(int_words, f)\n",
    "            \n",
    "    # What is the maximum length of our int_words?\n",
    "    max_len = max([len(iw) for iw in int_words])\n",
    "    print(\"Our longest sequence is {0} integers long\".format(max_len))\n",
    "\n",
    "    int_words_limited = limit_length(int_words, 20)\n",
    "\n",
    "    # What is the maximum length of our int_words?\n",
    "    max_len_limited = max([len(iw) for iw in int_words_limited])\n",
    "    print(\"Our longest sequence is {0} integers long\".format(max_len_limited))\n",
    "\n",
    "    # Find out what our maximum integer index is\n",
    "    max_index = max([max(iw) for iw in int_words_limited])\n",
    "\n",
    "    print(\"The maximum integer value (i.e. char index) is: {0}\".format(max_index))\n",
    "\n",
    "    X = sequence.pad_sequences(int_words_limited)\n",
    "\n",
    "    with open(x_filename, \"wb\") as f:\n",
    "        pickle.dump(X, f)\n",
    "        print(\"X saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "       79, 71, 80], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1687527 word samples\n"
     ]
    }
   ],
   "source": [
    "print(\"We have {0} word samples\".format(len(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was not possible because our X is too big!\n",
    "```\n",
    "X_one_hot = np.eye(max_index+1)[X]\n",
    "```\n",
    "However, I don't think we need to convert X into a one-hot vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just have a think about dimensionality.  \n",
    "\n",
    "Our RNN output at each time stamp will be a vector of length \"num_of_characters\", i.e. 82 above.  \n",
    "\n",
    "Our hidden dimension we can set at 300. (We can experiment with changing this.)  \n",
    "\n",
    "Our sequence length is fixed to 20. (Again we can experiment with this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay - now let's have a think about our data for the model.  \n",
    "\n",
    "We have 1687527 \"word\" sequences. \n",
    "\n",
    "Isn't our input and output data the same?\n",
    "\n",
    "This is what I need to read: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html.\n",
    "\n",
    "Our input data is a a set of sequences where each entry in the sequence is a one-hot array based on the number of characters we are using (83).\n",
    "\n",
    "The character vocabulary size is fixed for the encoder and decoder. It is independent of the text. \n",
    "\n",
    "What are our decoder inputs and outputs?\n",
    "* decoder input = encoder input (but minus ```</W>```)\n",
    "* decoder output = decoder input shifted 1 later in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some hyperparameters - start with those used in this example -\n",
    "# https://github.com/fchollet/keras/blob/master/examples/lstm_seq2seq.py\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "latent_dim = 64  # Latent dimensionality of the encoding space.\n",
    "num_samples = 30000  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The number of encoder tokens = number of different characters in our CharDict\n",
    "num_encoder_tokens = 83 # cd.vocabulary_size #= 83\n",
    "# Our decoder character space equals are encoder character space\n",
    "num_decoder_tokens = num_encoder_tokens\n",
    "\n",
    "# Our words are all set to the same max length (20)\n",
    "max_encoder_seq_length = 20\n",
    "max_decoder_seq_length = max_encoder_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (num_samples, max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (num_samples, max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (num_samples, max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, num_samples):\n",
    "    for t in range(0, max_encoder_seq_length):\n",
    "        encoder_input_data[i, t, X[i][t]] = 1\n",
    "        if t < (max_encoder_seq_length - 1):\n",
    "            # decoder input = encoder input for this autoencoder case but skipping last char\n",
    "            decoder_input_data[i, t, X[i][t]] = 1\n",
    "        if t > 0:\n",
    "            # Shift decoder target get so it is one ahead\n",
    "            decoder_target_data[i, t-1, X[i][t]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, None, 83)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, None, 83)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    [(None, 64), (None, 6 37888       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    [(None, None, 64), (N 37888       input_2[0][0]                    \n",
      "                                                                   lstm_1[0][1]                     \n",
      "                                                                   lstm_1[0][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, None, 83)      5395        lstm_2[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 81,171\n",
      "Trainable params: 81,171\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 506s - loss: 0.1721 - acc: 0.9005 - val_loss: 0.1387 - val_acc: 0.9125\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 774s - loss: 0.1474 - acc: 0.9080 - val_loss: 0.1183 - val_acc: 0.9174\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 387s - loss: 0.1250 - acc: 0.9149 - val_loss: 0.1006 - val_acc: 0.9240\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 349s - loss: 0.1071 - acc: 0.9205 - val_loss: 0.1131 - val_acc: 0.9182\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 359s - loss: 0.0930 - acc: 0.9246 - val_loss: 0.2101 - val_acc: 0.8903\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 339s - loss: 0.0819 - acc: 0.9280 - val_loss: 0.0718 - val_acc: 0.9320\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 346s - loss: 0.0706 - acc: 0.9312 - val_loss: 0.0574 - val_acc: 0.9362\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 340s - loss: 0.0636 - acc: 0.9330 - val_loss: 0.0547 - val_acc: 0.9355\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 289s - loss: 0.0557 - acc: 0.9353 - val_loss: 0.0504 - val_acc: 0.9371\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 167s - loss: 0.0515 - acc: 0.9368 - val_loss: 0.0522 - val_acc: 0.9358\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2\n",
    "          )\n",
    "# Save model\n",
    "model.save('s2s_64.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can actually test the model on words in X that are beyond index 20,000 (we had to limit for training to avoid memory errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful if we could look at the confusion matrix to determine where misclassifications were occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dimensionality of 64 in our hidden layer we have an accuracy of 93% and a loss of ~ 0.05:\n",
    "```loss: 0.0515 - acc: 0.9368 - val_loss: 0.0522 - val_acc: 0.9358```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can tweak this function to use our character dictionary\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, cd.startwordint] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = cd.int2char(sampled_token_index)\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_token_index == cd.endwordint or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><W>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n",
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(2000,2200):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    # print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opps - setting hidden dimensionality to 64 just sets everything to ```<PAD>```!\n",
    "\n",
    "```Decoded sentence: <PAD><PAD><PAD><PAD><PAD>```\n",
    "\n",
    "Can we adjust our training data batches? E.g. have x of length 3, y of length 5, z of length 10 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model.save(\"encoder_64.h5\")\n",
    "decoder_model.save(\"decoder_64.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 20, 5, 19, 20, 80]\n",
      "[79, 20, 5, 19, 20, 80, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "word_text = \"test\"\n",
    "integer_values = [cd.startwordint] + cd.text2int(word_text) + [cd.endwordint]\n",
    "print(integer_values)\n",
    "integer_values.extend([0] * (max_encoder_seq_length - len(integer_values)))\n",
    "print(integer_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2onehot(word_text):\n",
    "    \"\"\" Convert a word to a one hot encoding of the character sequence.\"\"\"\n",
    "    integer_values = [cd.startwordint] + cd.text2int(word_text) + [cd.endwordint]\n",
    "    integer_values.extend([0] * (max_encoder_seq_length - len(integer_values)))\n",
    "    # Create a single sample zero array\n",
    "    one_hot = np.zeros((1, max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
    "    for t in range(0, max_encoder_seq_length):\n",
    "        one_hot[0, t, integer_values[t]] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.]]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2onehot(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "input_seq = word2onehot(\"match\")\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('-')\n",
    "print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Decoded sentence: <PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "input_seq = word2onehot(\"A\")\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('-')\n",
    "print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"s2s.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extensions\n",
    "* Bi-directional LSTM for encoding and decoding (have an additional layer to resolve differences on decoder output).\n",
    "* CNN on input as additional vector portion to capture structure that is spatially invariant across the 1D character stream.\n",
    "* Use character embeddings rather than one hot vectors.\n",
    "* See if any faster without loss of accuracy with GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I have a trained encoder and decoder I can use these to generate a word vector and to decode that word vector.  \n",
    "\n",
    "I can then start looking at sequences of word vectors, e.g. sentences.  \n",
    "\n",
    "Some thoughts:\n",
    "* From spacy I also have lots of other cool labels, such as POS and dependency tree information. I want to jointly optimise to predict not only the next word vector in a sequence but the next POS etc label.\n",
    "* I can then generate sentence embeddings with shorter sequences, i.e. I can repeat the mthods above to generate a claim encoding. I can then try this on my classification model and my claim-to-title models.\n",
    "* On the claim-to-title models I can predict sequences of word vectors and then decode these to provide an output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
