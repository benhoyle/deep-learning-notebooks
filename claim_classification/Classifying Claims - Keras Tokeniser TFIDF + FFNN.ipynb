{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Claims - Keras Tokeniser TFIDF + FFNN\n",
    "\n",
    "In this post we will see if we can build some classifiers to predict a first level patent classification from the claim text.\n",
    "\n",
    "In particular, here we will look at applying a standard feed forward neural network on a TFIDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"raw_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n1. A detector for atrial fibrillation or flutter (AF) comprising: \\nan impedance measuring unit comprising a measuring input, to which an atrial electrode line having an electrode for a unipolar measurement of an impedance in an atrium is connected and is implemented to generate an atrial impedance signal, obtained in a unipolar manner, in such a way that an impedance signal for each atrial cycle, comprising an atrial contraction and a following relaxation of said atrium, comprises multiple impedance values detected at different instants within a particular atrial cycle; \\nsaid impedance measuring unit comprising a signal input, via which a ventricle signal is to be supplied to said detector, which reflects instants of ventricular contractions in chronological assignment to said impedance signal; \\nan analysis unit configured to average multiple sequential impedance signal sections of a unipolar atrial impedance signal, which are each delimited by two sequential ventricular contractions, with one another, and to determine a maximum amplitude of an averaged unipolar atrial impedance signal section, \\nsaid analysis unit configured to compare said maximum amplitude to a comparison value, and if said maximum amplitude of said averaged unipolar atrial impedance signal is less than said comparison value, generate an AF suspicion signal. \\n\\n',\n",
       " 'A')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a play with the Keras text tokenizer (as per here - https://keras.io/preprocessing/text/#tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "docs = [d[0] for d in data]\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_claims = t.texts_to_matrix(docs, mode='tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the texts_to_matrix function we need to apply a feed-forward neural network rather than a RNN, as we have for each claim a set of word counts scaled by document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11238, 26142)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_claims.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much faster than my old methods! But hey, I learnt some stuff about tokenisation.  \n",
    "\n",
    "We can use the \"num_words\" parameter as passed into the Tokenizer to restrict to the top n words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our classes are now a matrix of (11238, 8)\n",
      "Original label: A; Converted label: [ 1.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "Y_class = [d[1] for d in data]\n",
    "\n",
    "# encode class values as integers\n",
    "label_e = LabelEncoder()\n",
    "label_e.fit(Y_class)\n",
    "encoded_Y = label_e.transform(Y_class)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "Y_data = to_categorical(encoded_Y)\n",
    "print(\"Our classes are now a matrix of {0}\".format(Y_data.shape))\n",
    "print(\"Original label: {0}; Converted label: {1}\".format(Y_class[0], Y_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into training (80%) and testing (20%)\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(encoded_claims, Y_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear some memory\n",
    "del data, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input dimension for our claim representation is 26142\n"
     ]
    }
   ],
   "source": [
    "input_dim = encoded_claims.shape[1]\n",
    "print(\"Our input dimension for our claim representation is {0}\".format(input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our output dimension is 8\n"
     ]
    }
   ],
   "source": [
    "no_classes = Y_data.shape[1]\n",
    "print(\"Our output dimension is {0}\".format(no_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 500)               13071500  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 4008      \n",
      "=================================================================\n",
      "Total params: 13,075,508\n",
      "Trainable params: 13,075,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8990 samples, validate on 2248 samples\n",
      "Epoch 1/5\n",
      "8990/8990 [==============================] - 359s - loss: 1.1837 - acc: 0.5830 - val_loss: 0.9802 - val_acc: 0.6664\n",
      "Epoch 2/5\n",
      "8990/8990 [==============================] - 286s - loss: 0.3369 - acc: 0.8958 - val_loss: 1.1043 - val_acc: 0.6486\n",
      "Epoch 3/5\n",
      "8990/8990 [==============================] - 216s - loss: 0.0961 - acc: 0.9764 - val_loss: 1.2447 - val_acc: 0.6383\n",
      "Epoch 4/5\n",
      "8990/8990 [==============================] - 210s - loss: 0.0292 - acc: 0.9956 - val_loss: 1.3897 - val_acc: 0.6455\n",
      "Epoch 5/5\n",
      "8990/8990 [==============================] - 193s - loss: 0.0155 - acc: 0.9982 - val_loss: 1.4976 - val_acc: 0.6361\n",
      "Accuracy: 63.61%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is overfitting on our training set. We should try adding dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b24ec54a2c6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv_lstm.hd5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"conv_lstm.hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that this approach doesn't seem to work. Likely because there are limited patterns at the word level to be detected by a CNN.  \n",
    "\n",
    "This would likely work better at the character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'A': 1777,\n",
       "         'B': 1449,\n",
       "         'C': 865,\n",
       "         'D': 54,\n",
       "         'E': 269,\n",
       "         'F': 735,\n",
       "         'G': 3335,\n",
       "         'H': 2754})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Let's check how our data is distributed across the classes\n",
    "class_count = Counter(Y_class)\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code for building a confusion matrix\n",
    "\n",
    "def get_confusion_matrix_one_hot(model_results, truth):\n",
    "    '''model_results and truth should be for one-hot format, i.e, have >= 2 columns,\n",
    "    where truth is 0/1, and max along each row of model_results is model result\n",
    "    '''\n",
    "    assert model_results.shape == truth.shape\n",
    "    num_outputs = truth.shape[1]\n",
    "    confusion_matrix = np.zeros((num_outputs, num_outputs), dtype=np.int32)\n",
    "    predictions = np.argmax(model_results,axis=1)\n",
    "    assert len(predictions)==truth.shape[0]\n",
    "\n",
    "    for actual_class in range(num_outputs):\n",
    "        idx_examples_this_class = truth[:,actual_class]==1\n",
    "        prediction_for_this_class = predictions[idx_examples_this_class]\n",
    "        for predicted_class in range(num_outputs):\n",
    "            count = np.sum(prediction_for_this_class==predicted_class)\n",
    "            confusion_matrix[actual_class, predicted_class] = count\n",
    "    assert np.sum(confusion_matrix)==len(truth)\n",
    "    assert np.sum(confusion_matrix)==np.sum(truth)\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = get_confusion_matrix_one_hot(predict, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[212,  37,  30,   0,   3,   9,  30,  15],\n",
       "       [ 15, 161,   9,   0,   5,  20,  46,  31],\n",
       "       [ 29,  29,  90,   0,   0,   2,  16,  12],\n",
       "       [  1,   6,   0,   2,   0,   0,   1,   2],\n",
       "       [  6,  19,   0,   0,  21,   3,   8,   3],\n",
       "       [  8,  25,   2,   0,   3,  62,  11,  17],\n",
       "       [ 15,  28,   6,   0,   1,   9, 512, 129],\n",
       "       [  9,  22,   7,   0,   2,  14, 123, 370]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
