{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Claims - Keras Tokeniser TFIDF + FFNN - Binary Classification\n",
    "\n",
    "In this post we will see if we can build some classifiers to predict a first level patent classification from the claim text.\n",
    "\n",
    "In particular, here we will look at applying a standard feed forward neural network on a TFIDF matrix.\n",
    "\n",
    "We will try to decompose the classification as a binary classification: G or not G and see what results we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(\"raw_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n1. A detector for atrial fibrillation or flutter (AF) comprising: \\nan impedance measuring unit comprising a measuring input, to which an atrial electrode line having an electrode for a unipolar measurement of an impedance in an atrium is connected and is implemented to generate an atrial impedance signal, obtained in a unipolar manner, in such a way that an impedance signal for each atrial cycle, comprising an atrial contraction and a following relaxation of said atrium, comprises multiple impedance values detected at different instants within a particular atrial cycle; \\nsaid impedance measuring unit comprising a signal input, via which a ventricle signal is to be supplied to said detector, which reflects instants of ventricular contractions in chronological assignment to said impedance signal; \\nan analysis unit configured to average multiple sequential impedance signal sections of a unipolar atrial impedance signal, which are each delimited by two sequential ventricular contractions, with one another, and to determine a maximum amplitude of an averaged unipolar atrial impedance signal section, \\nsaid analysis unit configured to compare said maximum amplitude to a comparison value, and if said maximum amplitude of said averaged unipolar atrial impedance signal is less than said comparison value, generate an AF suspicion signal. \\n\\n',\n",
       " 'A')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a play with the Keras text tokenizer (as per here - https://keras.io/preprocessing/text/#tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11238, 10000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "docs = [d[0] for d in data]\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer(num_words=10000)\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "encoded_claims = t.texts_to_matrix(docs, mode='count')\n",
    "\n",
    "encoded_claims.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the texts_to_matrix function we need to apply a feed-forward neural network rather than a RNN, as we have for each claim a set of word counts scaled by document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_class = [d[1] for d in data]\n",
    "\n",
    "Y_data = [1 if c == 'G' else 0 for c in Y_class ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'H', 'H', 'C', 'A', 'B', 'G', 'A', 'H', 'G', 'C', 'F', 'H', 'G', 'G', 'G', 'E', 'G', 'A']\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_class[0:20])\n",
    "print(Y_data[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into training (80%) and testing (20%)\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(encoded_claims, Y_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input dimension for our claim representation is 10000\n"
     ]
    }
   ],
   "source": [
    "input_dim = encoded_claims.shape[1]\n",
    "print(\"Our input dimension for our claim representation is {0}\".format(input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_5 (Dropout)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2000)              20002000  \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 500)               1000500   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 21,003,001\n",
      "Trainable params: 21,003,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create a deeper model\n",
    "deep_model = Sequential()\n",
    "deep_model.add(Dropout(0.2, input_shape=(input_dim,)))\n",
    "deep_model.add(Dense(2000, activation='relu'))\n",
    "deep_model.add(Dropout(0.5))\n",
    "deep_model.add(Dense(500, activation='relu'))\n",
    "deep_model.add(Dense(1, activation='sigmoid'))\n",
    "deep_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(deep_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8990 samples, validate on 2248 samples\n",
      "Epoch 1/20\n",
      "8990/8990 [==============================] - 320s - loss: 0.5131 - acc: 0.7711 - val_loss: 0.4256 - val_acc: 0.8296\n",
      "Epoch 2/20\n",
      "8990/8990 [==============================] - 290s - loss: 0.3497 - acc: 0.8503 - val_loss: 0.4067 - val_acc: 0.8390\n",
      "Epoch 3/20\n",
      "8990/8990 [==============================] - 273s - loss: 0.2564 - acc: 0.8932 - val_loss: 0.4307 - val_acc: 0.8345\n",
      "Epoch 4/20\n",
      "8990/8990 [==============================] - 266s - loss: 0.1918 - acc: 0.9179 - val_loss: 0.5301 - val_acc: 0.8292\n",
      "Epoch 5/20\n",
      "8990/8990 [==============================] - 261s - loss: 0.1252 - acc: 0.9509 - val_loss: 0.6776 - val_acc: 0.8323\n",
      "Epoch 6/20\n",
      "8990/8990 [==============================] - 288s - loss: 0.0956 - acc: 0.9667 - val_loss: 0.7173 - val_acc: 0.8350\n",
      "Epoch 7/20\n",
      "8990/8990 [==============================] - 275s - loss: 0.0804 - acc: 0.9710 - val_loss: 0.7861 - val_acc: 0.8270\n",
      "Epoch 8/20\n",
      "8990/8990 [==============================] - 263s - loss: 0.0565 - acc: 0.9793 - val_loss: 0.8205 - val_acc: 0.8319\n",
      "Epoch 9/20\n",
      "8990/8990 [==============================] - 263s - loss: 0.0532 - acc: 0.9815 - val_loss: 0.7943 - val_acc: 0.8256\n",
      "Epoch 10/20\n",
      "8990/8990 [==============================] - 264s - loss: 0.0479 - acc: 0.9840 - val_loss: 0.9314 - val_acc: 0.8261\n",
      "Epoch 11/20\n",
      "8990/8990 [==============================] - 263s - loss: 0.0518 - acc: 0.9823 - val_loss: 0.8473 - val_acc: 0.8301\n",
      "Epoch 12/20\n",
      "8990/8990 [==============================] - 263s - loss: 0.0557 - acc: 0.9832 - val_loss: 0.7381 - val_acc: 0.8105\n",
      "Epoch 13/20\n",
      "8990/8990 [==============================] - 263s - loss: 0.0462 - acc: 0.9841 - val_loss: 0.7959 - val_acc: 0.8310\n",
      "Epoch 14/20\n",
      "8990/8990 [==============================] - 265s - loss: 0.0377 - acc: 0.9890 - val_loss: 0.8915 - val_acc: 0.8341\n",
      "Epoch 15/20\n",
      "8990/8990 [==============================] - 274s - loss: 0.0304 - acc: 0.9902 - val_loss: 0.8411 - val_acc: 0.8274\n",
      "Epoch 16/20\n",
      "8990/8990 [==============================] - 324s - loss: 0.0320 - acc: 0.9912 - val_loss: 0.8008 - val_acc: 0.8238\n",
      "Epoch 17/20\n",
      "8990/8990 [==============================] - 309s - loss: 0.0312 - acc: 0.9900 - val_loss: 0.7894 - val_acc: 0.8243\n",
      "Epoch 18/20\n",
      "8990/8990 [==============================] - 279s - loss: 0.0324 - acc: 0.9891 - val_loss: 0.8807 - val_acc: 0.8310\n",
      "Epoch 19/20\n",
      "8990/8990 [==============================] - 271s - loss: 0.0253 - acc: 0.9920 - val_loss: 0.9096 - val_acc: 0.8274\n",
      "Epoch 20/20\n",
      "8990/8990 [==============================] - 291s - loss: 0.0247 - acc: 0.9939 - val_loss: 0.9626 - val_acc: 0.8305\n",
      "Accuracy: 83.05%\n"
     ]
    }
   ],
   "source": [
    "deep_model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = deep_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 10000 word vocab and TDIDF we have:\n",
    "```Epoch 20/20\n",
    "8990/8990 [==============================] - 233s - loss: 0.0190 - acc: 0.9950 - val_loss: 1.0357 - val_acc: 0.8123\n",
    "Accuracy: 81.23%\n",
    "```\n",
    "\n",
    "With 10000 words and count we have:\n",
    "```8990/8990 [==============================] - 291s - loss: 0.0247 - acc: 0.9939 - val_loss: 0.9626 - val_acc: 0.8305\n",
    "Accuracy: 83.05%```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_model.save(\"2017-10-29 deep_dense_dropout_binary.hd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count seems to work as well as or better than TFIDF. Overfitting is still a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further Investigations\n",
    "\n",
    "Have more layers with more dropout?\n",
    "\n",
    "Compare with bidirectional LSTM approach?\n",
    "\n",
    "Experiment with regularisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_7 (Dropout)          (None, 30000)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2000)              60002000  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 500)               1000500   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 61,003,001\n",
      "Trainable params: 61,003,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8990 samples, validate on 2248 samples\n",
      "Epoch 1/20\n",
      "8990/8990 [==============================] - 1100s - loss: 0.4895 - acc: 0.7863 - val_loss: 0.4217 - val_acc: 0.8265\n",
      "Epoch 2/20\n",
      "8990/8990 [==============================] - 858s - loss: 0.3456 - acc: 0.8562 - val_loss: 0.4288 - val_acc: 0.8038\n",
      "Epoch 3/20\n",
      "8990/8990 [==============================] - 880s - loss: 0.2468 - acc: 0.8951 - val_loss: 0.5070 - val_acc: 0.8292\n",
      "Epoch 4/20\n",
      "8990/8990 [==============================] - 856s - loss: 0.1735 - acc: 0.9344 - val_loss: 0.5748 - val_acc: 0.8136\n",
      "Epoch 5/20\n",
      "8990/8990 [==============================] - 878s - loss: 0.1281 - acc: 0.9525 - val_loss: 0.7221 - val_acc: 0.8203\n",
      "Epoch 6/20\n",
      "8990/8990 [==============================] - 855s - loss: 0.0825 - acc: 0.9721 - val_loss: 0.7763 - val_acc: 0.8181\n",
      "Epoch 7/20\n",
      "8990/8990 [==============================] - 902s - loss: 0.0672 - acc: 0.9770 - val_loss: 0.9032 - val_acc: 0.8270\n",
      "Epoch 8/20\n",
      "8990/8990 [==============================] - 882s - loss: 0.0573 - acc: 0.9819 - val_loss: 0.8998 - val_acc: 0.8198\n",
      "Epoch 9/20\n",
      "8990/8990 [==============================] - 925s - loss: 0.0576 - acc: 0.9831 - val_loss: 0.8975 - val_acc: 0.8310\n",
      "Epoch 10/20\n",
      "8990/8990 [==============================] - 867s - loss: 0.0556 - acc: 0.9841 - val_loss: 0.9008 - val_acc: 0.8305\n",
      "Epoch 11/20\n",
      "8990/8990 [==============================] - 879s - loss: 0.0409 - acc: 0.9892 - val_loss: 1.0058 - val_acc: 0.8256\n",
      "Epoch 12/20\n",
      "8990/8990 [==============================] - 896s - loss: 0.0449 - acc: 0.9868 - val_loss: 1.1209 - val_acc: 0.8261\n",
      "Epoch 13/20\n",
      "8990/8990 [==============================] - 901s - loss: 0.0410 - acc: 0.9882 - val_loss: 0.7739 - val_acc: 0.8203\n",
      "Epoch 14/20\n",
      "8990/8990 [==============================] - 887s - loss: 0.0311 - acc: 0.9934 - val_loss: 0.9328 - val_acc: 0.8194\n",
      "Epoch 15/20\n",
      "8990/8990 [==============================] - 866s - loss: 0.0293 - acc: 0.9924 - val_loss: 0.9732 - val_acc: 0.8252\n",
      "Epoch 16/20\n",
      "8990/8990 [==============================] - 866s - loss: 0.0292 - acc: 0.9928 - val_loss: 1.0014 - val_acc: 0.8158\n",
      "Epoch 17/20\n",
      "8990/8990 [==============================] - 876s - loss: 0.0285 - acc: 0.9914 - val_loss: 1.0246 - val_acc: 0.8136\n",
      "Epoch 18/20\n",
      "8990/8990 [==============================] - 896s - loss: 0.0377 - acc: 0.9907 - val_loss: 1.0245 - val_acc: 0.8167\n",
      "Epoch 19/20\n"
     ]
    }
   ],
   "source": [
    "# create the tokenizer\n",
    "t = Tokenizer(num_words=30000)\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "encoded_claims = t.texts_to_matrix(docs, mode='count')\n",
    "\n",
    "# split the data into training (80%) and testing (20%)\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(encoded_claims, Y_data, test_size=0.2)\n",
    "\n",
    "# create a deeper model\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.2, input_shape=(encoded_claims.shape[1],)))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"2017-10-29 30k_word_deep_dense_dropout_binary.hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
